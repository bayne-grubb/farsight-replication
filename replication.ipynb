{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSfVs2qCXJX_",
    "outputId": "ca2d9285-5ea8-4d43-8c3d-95b9a5518d3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nose\n",
      "  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n",
      "Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "Installing collected packages: nose\n",
      "Successfully installed nose-1.3.7\n",
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /home/bayne/miniconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: setuptools in /home/bayne/miniconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/bayne/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.1 triton-3.1.0 typing-extensions-4.12.2\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /home/bayne/miniconda3/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.0.5 wrapt-1.16.0\n",
      "Collecting bayesian-optimization\n",
      "  Downloading bayesian_optimization-2.0.0-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting colorama<0.5.0,>=0.4.6 (from bayesian-optimization)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.25 in /home/bayne/miniconda3/lib/python3.12/site-packages (from bayesian-optimization) (1.26.4)\n",
      "Collecting scikit-learn<2.0.0,>=1.0.0 (from bayesian-optimization)\n",
      "  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in /home/bayne/miniconda3/lib/python3.12/site-packages (from bayesian-optimization) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/bayne/miniconda3/lib/python3.12/site-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading bayesian_optimization-2.0.0-py3-none-any.whl (30 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, colorama, scikit-learn, bayesian-optimization\n",
      "Successfully installed bayesian-optimization-2.0.0 colorama-0.4.6 scikit-learn-1.5.2 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nose\n",
    "!pip install torch\n",
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/bayne/miniconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/bayne/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.55.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (164 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bayne/miniconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/bayne/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, pandas, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.0 kiwisolver-1.4.7 matplotlib-3.9.2 pandas-2.2.3 pillow-11.0.0 pyparsing-3.2.0 pytz-2024.2 tzdata-2024.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "noIEKXN0rwn6",
    "outputId": "94261928-8c70-4474-da7a-cdecb6378053"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "!pip install pyspark\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lWVufNm7Xbhd",
    "outputId": "edb33c1f-23f7-4b08-a3ee-84872cf3d046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /content\n",
      "Mounted at /content/gdrive\n",
      "Working Directory: /content/gdrive/.shortcut-targets-by-id/1qzyxUJShWKZIUmnpVBgOGLUoKeLf2ZdF/CSE6250 Final Project\n",
      "\n",
      "Listdir of working dir: ['FarSight_Long-Term_Disease_Prediction_Using_Unstructured_Clinical_Nursing_Notes.pdf', 'PATIENTS.csv', 'ADMISSIONS.csv', 'DIAGNOSES_ICD.csv', 'NOTEEVENTS.csv', 'nursing_notes.csv', 'filtered_cleaned_raw_nursing_notes_processed.csv', 'doc2vec.model.dv.vectors.npy', 'doc2vec.model', 'doc2vec_X.csv', '15yo_min_patients.csv', 'first_admission_records.csv', 'filtered_cleaned_raw_nursing_notes.csv', 'y_labels_multigroup_icd9.pkl', 'doc2vec_dataset_lite.pkl', 'doc2vec_dataset.pkl', 'doc2vec_dataset_full.pkl', 'CS6250 Final Project.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# View and modify the working path\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# View current working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# Change working directory to your file position\n",
    "path = \"/content/gdrive/My Drive/CSE6250 Final Project\"\n",
    "#path = \"/content/gdrive/Shared With Me/CSE6250 Final Project\" # Bayne path\n",
    "os.chdir(path)\n",
    "\n",
    "# Confirm the change\n",
    "print(\"Working Directory:\", os.getcwd())\n",
    "\n",
    "# Quick view listdir of working dir\n",
    "print(\"\\nListdir of working dir:\",os.listdir(os.getcwd()+\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXL9Nw7_3Y8F"
   },
   "outputs": [],
   "source": [
    "#os.kill(os.getpid(), 9) #run if pandas break..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1CPOpGmXm7i"
   },
   "source": [
    "*Data* Load, preview. Pre-ETL. Can skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_CX2RR2XqjT",
    "outputId": "ee085f0a-b9c8-4c6f-b8f4-94acdc7e6087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Row count: 46520\n",
      "  Columns: ['ROW_ID', 'SUBJECT_ID', 'GENDER', 'DOB', 'DOD', 'DOD_HOSP', 'DOD_SSN', 'EXPIRE_FLAG']\n",
      "\n",
      "\n",
      "  Row count: 58976\n",
      "  Columns: ['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'ADMISSION_TYPE', 'ADMISSION_LOCATION', 'DISCHARGE_LOCATION', 'INSURANCE', 'LANGUAGE', 'RELIGION', 'MARITAL_STATUS', 'ETHNICITY', 'EDREGTIME', 'EDOUTTIME', 'DIAGNOSIS', 'HOSPITAL_EXPIRE_FLAG', 'HAS_CHARTEVENTS_DATA']\n",
      "\n",
      "\n",
      "  Row count: 651047\n",
      "  Columns: ['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE']\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_raw_data(verbose = 0):\n",
    "    file_name = [\"PATIENTS.csv\",\\\n",
    "                \"ADMISSIONS.csv\", \\\n",
    "                \"DIAGNOSES_ICD.csv\", \\\n",
    "                #  \"NOTEEVENTS.csv\"\n",
    "                 ]\n",
    "    #srx https://physionet.org/content/mimiciii/1.4/\n",
    "    #mounted from personal drive, shared,\n",
    "    #if needed: https://drive.google.com/drive/folders/1qzyxUJShWKZIUmnpVBgOGLUoKeLf2ZdF?usp=share_link\n",
    "    csv_files = [os.getcwd() +\"/\"+ x for x in file_name]\n",
    "    df_dict = {}\n",
    "\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)  # Load CSV into a Pandas DataFrame\n",
    "        if verbose:\n",
    "          # print(f\"First 5 rows of {file.split('/')[-1]}:\")\n",
    "          # display(df.head())\n",
    "          print(f\"  Row count: {df.shape[0]}\")  # Access row count using shape[0]\n",
    "          print(f\"  Columns: {df.columns.tolist()}\")  # Access columns and convert to list\n",
    "          print(\"\\n\")  # Add a newline for better readability\n",
    "        df_dict[file.split('/')[-1]] = df  # Store DataFrame in dictionary with filename as key\n",
    "        del df\n",
    "    return df_dict\n",
    "\n",
    "import pandas as pd\n",
    "import gc\n",
    "df_dict = load_raw_data(1)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3RU21d6F9Vi"
   },
   "source": [
    "## ETL | Filter for >15yo and 1st admit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kqbRqju6xKn",
    "outputId": "997011af-0bd7-4932-f1ac-b059b717d9a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient count in PATIENTS.csv: 46520\n",
      "Patient count in ADMISSIONS.csv: 46520\n",
      "Number of null rows of admit time:  0\n",
      "Number of null rows of dob:  0\n",
      "Count of >15yo patients by diff(dob, admit time)>=timedelta(days=365.25*15):\n",
      " 38645\n",
      "\n",
      "Succesfully saved to  /content/gdrive/.shortcut-targets-by-id/1qzyxUJShWKZIUmnpVBgOGLUoKeLf2ZdF/CSE6250 Final Project/15yo_min_patients.csv\n",
      "Number of null HADM_IDs in raw data: 0\n",
      "Data type of HADM_ID column in raw data: int64\n",
      "Initial unique SUBJECT_ID count: 46520\n",
      "Initial unique HADM_ID count: 58976\n",
      "Filtered unique SUBJECT_ID count: 46520\n",
      "Filtered unique HADM_ID count: 46520\n",
      "Succesfully saved to  /content/gdrive/.shortcut-targets-by-id/1qzyxUJShWKZIUmnpVBgOGLUoKeLf2ZdF/CSE6250 Final Project/first_admission_records.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict.keys()\n",
    "from datetime import timedelta\n",
    "def get_non_neonates(df_dict, output_dir, verbose):\n",
    "    \"\"\"\n",
    "    Identifies non-neonatal patients: (Admit_time - DOB > 15 years old)\n",
    "    and save their SUBJECT_ID to a CSV file.\n",
    "    Args:\n",
    "      df_dict: A dictionary containing the 'PATIENTS.csv' and 'ADMISSIONS.csv' DataFrames.\n",
    "      output_dir: The directory to save the output CSV file.\n",
    "      verbose: A boolean indicating whether to print progress messages.\n",
    "    \"\"\"\n",
    "    merged_df = pd.merge(df_dict['PATIENTS.csv'][['SUBJECT_ID', 'DOB']], df_dict['ADMISSIONS.csv'][['SUBJECT_ID', 'ADMITTIME']], on='SUBJECT_ID')\n",
    "    merged_df['ADMITTIME'] = pd.to_datetime(merged_df['ADMITTIME']).dt.date\n",
    "    merged_df['DOB'] = pd.to_datetime(merged_df['DOB']).dt.date\n",
    "    merged_df['AgeAtAdmission'] = (merged_df['ADMITTIME'] - merged_df['DOB'])\n",
    "    non_neonates_filtered_df = merged_df[merged_df['AgeAtAdmission'] >= timedelta(days=365.25*15)]\n",
    "    non_neonates_filtered_df = pd.DataFrame(non_neonates_filtered_df['SUBJECT_ID'].unique(), columns=['SUBJECT_ID'])\n",
    "    if verbose:\n",
    "      print(f\"Patient count in PATIENTS.csv: {df_dict['PATIENTS.csv']['SUBJECT_ID'].nunique()}\")\n",
    "      print(f\"Patient count in ADMISSIONS.csv: {df_dict['ADMISSIONS.csv']['SUBJECT_ID'].nunique()}\")\n",
    "      print(\"Number of null rows of admit time: \", merged_df['ADMITTIME'].isna().sum())\n",
    "      print(\"Number of null rows of dob: \",merged_df['DOB'].isna().sum())\n",
    "      print(f\"Count of >15yo patients by diff(dob, admit time)>=timedelta(days=365.25*15):\\n {non_neonates_filtered_df.shape[0]}\\n\")\n",
    "    non_neonates_filtered_df.to_csv(output_dir, index=False)\n",
    "    if verbose: print(\"Succesfully saved to \",output_dir)\n",
    "    del non_neonates_filtered_df\n",
    "    del merged_df\n",
    "\n",
    "def get_first_admissions(df_dict, output_first_dir, verbose):\n",
    "    \"\"\"\n",
    "    Get only admission rows of first admit date and save to a CSV file.\n",
    "    Args:\n",
    "      df_dict: A dictionary containing the 'ADMISSIONS.csv' DataFrames.\n",
    "      output_dir: The directory to save the output CSV file.\n",
    "      verbose: A boolean indicating whether to print progress messages.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "      print(f\"Number of null HADM_IDs in raw data: {df_dict['ADMISSIONS.csv']['HADM_ID'].isnull().sum()}\")\n",
    "      print(f\"Data type of HADM_ID column in raw data: {df_dict['ADMISSIONS.csv']['HADM_ID'].dtype}\")\n",
    "      print(f\"Initial unique SUBJECT_ID count: {df_dict['ADMISSIONS.csv']['SUBJECT_ID'].nunique()}\")\n",
    "      print(f\"Initial unique HADM_ID count: {df_dict['ADMISSIONS.csv']['HADM_ID'].nunique()}\")\n",
    "    first_admit_ids = df_dict['ADMISSIONS.csv'].loc[df_dict['ADMISSIONS.csv'].groupby('SUBJECT_ID')['HADM_ID'].idxmin()][['SUBJECT_ID', 'HADM_ID']]\n",
    "\n",
    "    if verbose:\n",
    "      print(f\"Filtered unique SUBJECT_ID count: {first_admit_ids['SUBJECT_ID'].nunique()}\")\n",
    "      print(f\"Filtered unique HADM_ID count: {first_admit_ids['HADM_ID'].nunique()}\")\n",
    "    first_admit_ids.to_csv(output_first_dir, index=False)\n",
    "    if verbose:\n",
    "      print(\"Succesfully saved to \",output_first_dir)\n",
    "    del first_admit_ids\n",
    "\n",
    "\n",
    "NON_NEONATE_CSV_DIR = os.getcwd() +\"/15yo_min_patients.csv\"\n",
    "FIRST_ADMIT_CSV_DIR = os.getcwd() +\"/first_admission_records.csv\"\n",
    "############# If uncomment to generate filtered csv again\n",
    "get_non_neonates(df_dict, NON_NEONATE_CSV_DIR, verbose = 1)\n",
    "get_first_admissions(df_dict, FIRST_ADMIT_CSV_DIR, verbose=1)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARuZ672qsv_y"
   },
   "source": [
    "# ETL | Filter for Nursing category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TA-eCc2ctOC5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_cat_filtered_notes(input_dir, category, output_dir, verbose):\n",
    "    \"\"\"\n",
    "    Get only notes of specified category (NURSING)\n",
    "    Args:\n",
    "      input_dir: A dictionary containing the 'ADMISSIONS.csv' DataFrames.\n",
    "      category: The category of notes to filter for.\n",
    "      output_dir: The directory to save the filtered CSV file.\n",
    "      verbose: A boolean indicating whether to print progress messages.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_dir)\n",
    "    if verbose:\n",
    "        ax = df['CATEGORY'].value_counts().plot(kind='bar', title='Category Counts')\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "        plt.show()\n",
    "        print(f\"Unique SUBJECT_ID count: {df['SUBJECT_ID'].nunique()}\")\n",
    "        print(f\"Empty TEXT count: {df['TEXT'].isnull().sum()}\")\n",
    "    nursing_notes_df = df[df['CATEGORY'].isin(category)]\n",
    "    del df #rm big df from RAM\n",
    "    gc.collect()\n",
    "    if verbose:\n",
    "        print(f\"After filtering for {category} notes:\")\n",
    "        print(f\"Row count: {nursing_notes_df.shape[0]}\")\n",
    "        print(f\"Unique SUBJECT_ID count: {nursing_notes_df['SUBJECT_ID'].nunique()}\")\n",
    "        print(f\"Empty TEXT count: {nursing_notes_df['TEXT'].isnull().sum()}\")\n",
    "    nursing_notes_df.to_csv(output_dir)\n",
    "    if verbose:\n",
    "        print(\"Succesfully saved to \",output_dir)\n",
    "    del nursing_notes_df\n",
    "\n",
    "########## Uncomment to get filtered csv\n",
    "noteevent_input_dir = os.getcwd() +\"/NOTEEVENTS.csv\"\n",
    "category = ['Nursing']\n",
    "NURSING_NOTES_CAT_FILTERED_CSV_DIR = os.getcwd() +\"/nursing_notes.csv\"\n",
    "# get_cat_filtered_notes(noteevent_input_dir, category, NURSING_NOTES_CAT_FILTERED_CSV_DIR, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TO6KZIF_tQa8"
   },
   "source": [
    "## ETL | Filter for all 3 criteria: >15yo, 1st admit, nursing notes category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f33SqqOLIAYj",
    "outputId": "8f6929a0-aadd-47dc-cd0b-5a3191beb251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nursing notes \n",
      " Columns:\n",
      "['Unnamed: 0', 'ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME', 'STORETIME', 'CATEGORY', 'DESCRIPTION', 'CGID', 'ISERROR', 'TEXT']\n",
      "First admit columns:\n",
      "['SUBJECT_ID', 'HADM_ID']\n",
      "##################\n",
      "\n",
      "Section III.A. Filtered noteevents of category NURSING\n",
      "Got patient count: 7704, and note count 223556\n",
      "correct patient count: 7704, and note count 223556\n",
      "\n",
      "Section III.B. Starting next filter, for >15yo\n",
      "got count: 7704\n",
      "Section III.B. starting next filter, for 1st admission notes only\n",
      "got count: 6733\n",
      "correct count: 7638\n",
      "\n",
      "Section III.C. Filter out for ISERROR coded notes\n",
      "initial count: 6733\n",
      "got count: 6732\n",
      "filter out for duplicates by subject_id and text, i.e. same notes verbatim for same patient\n",
      "Got patient count: 6732, and note count 142110\n",
      "correct patient count: 6532, and note count 140792\n",
      "\n",
      "Succesfully saved to  /content/gdrive/.shortcut-targets-by-id/1qzyxUJShWKZIUmnpVBgOGLUoKeLf2ZdF/CSE6250 Final Project/filtered_cleaned_raw_nursing_notes.csv\n"
     ]
    }
   ],
   "source": [
    "def get_raw_data(output_dir,verbose):\n",
    "    ############## Section III. A.\n",
    "    non_neonate_df = pd.read_csv(NON_NEONATE_CSV_DIR)\n",
    "    first_admit_df = pd.read_csv(FIRST_ADMIT_CSV_DIR)\n",
    "    nursing_notes_df = pd.read_csv(NURSING_NOTES_CAT_FILTERED_CSV_DIR)\n",
    "    if verbose:\n",
    "      print(\"Nursing notes \\n Columns:\")\n",
    "      print(nursing_notes_df.columns.tolist())\n",
    "      print(\"First admit columns:\")\n",
    "      print(first_admit_df.columns.tolist())\n",
    "      print(\"#\"*18)\n",
    "      print(\"\\nSection III.A. Filtered noteevents of category NURSING\")\n",
    "      print(f\"Got patient count: {nursing_notes_df['SUBJECT_ID'].nunique()}, and note count {nursing_notes_df.shape[0]}\")\n",
    "      print(\"correct patient count: 7704, and note count 223556\\n\")\n",
    "    ############### Section III. B. filter for both >15yo and 1st admit\n",
    "    if verbose:\n",
    "      print(f\"Section III.B. Starting next filter, for >15yo\")\n",
    "    nursing_notes_df = nursing_notes_df[nursing_notes_df['SUBJECT_ID'].isin(non_neonate_df['SUBJECT_ID'])]\n",
    "    if verbose:\n",
    "      print(f\"got count: {nursing_notes_df['SUBJECT_ID'].nunique()}\")\n",
    "      print(\"Section III.B. starting next filter, for 1st admission notes only\")\n",
    "\n",
    "    nursing_notes_df = pd.merge(nursing_notes_df, first_admit_df, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    if verbose:\n",
    "      print(f\"got count: {nursing_notes_df['SUBJECT_ID'].nunique()}\")\n",
    "      print(\"correct count: 7638\\n\")\n",
    "\n",
    "    ############### Section III. C. Filter out rows with ISERROR = True and Deduplicate based on patient record\n",
    "    if verbose:\n",
    "      print(\"Section III.C. Filter out for ISERROR coded notes\")\n",
    "      print(f\"initial count: {nursing_notes_df['SUBJECT_ID'].nunique()}\")\n",
    "    nursing_notes_df = nursing_notes_df[nursing_notes_df['ISERROR'].isna()]\n",
    "    if verbose:\n",
    "      print(f\"got count: {nursing_notes_df['SUBJECT_ID'].nunique()}\")\n",
    "      print(\"filter out for duplicates by subject_id and text, i.e. same notes verbatim for same patient\")\n",
    "    nursing_notes_df = nursing_notes_df.drop_duplicates(subset=['SUBJECT_ID', 'TEXT'], keep='first')\n",
    "    if verbose:\n",
    "      print(f\"Got patient count: {nursing_notes_df['SUBJECT_ID'].nunique()}, and note count {nursing_notes_df.shape[0]}\")\n",
    "      print(\"correct patient count: 6532, and note count 140792\\n\")\n",
    "\n",
    "    ######## save final to csv\n",
    "    nursing_notes_df.to_csv(output_dir)\n",
    "    if verbose:\n",
    "        print(\"Succesfully saved to \",output_dir)\n",
    "\n",
    "FILTERED_CLEANED_RAW_NOTES = os.getcwd() +\"/filtered_cleaned_raw_nursing_notes.csv\"\n",
    "get_raw_data(FILTERED_CLEANED_RAW_NOTES, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbVogQsuMAU4"
   },
   "source": [
    "# Farsight Aggregation (Section III. C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3tTqpyMMFAG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "FILTERED_CLEANED_RAW_NOTES = os.getcwd() +\"/filtered_cleaned_raw_nursing_notes.csv\"\n",
    "cleaned_nursing_notes = pd.read_csv(FILTERED_CLEANED_RAW_NOTES)\n",
    "#TODO FarSight stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5AcHsMCZv0BU",
    "outputId": "3f244ab2-60c9-408b-bea0-930ea4b544ac"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "cleaned_nursing_notes"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-e6e139ee-b127-43e4-ada3-ffa0e0d0f4b3\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>document_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>308707</td>\n",
       "      <td>316398</td>\n",
       "      <td>31608</td>\n",
       "      <td>152365</td>\n",
       "      <td>2133-01-19</td>\n",
       "      <td>2133-01-19 06:53:00</td>\n",
       "      <td>2133-01-19 06:53:16</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Nursing Progress Note</td>\n",
       "      <td>19650</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TITLE:\\n   Respiratory failure, acute (not ARD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>308711</td>\n",
       "      <td>316402</td>\n",
       "      <td>29076</td>\n",
       "      <td>193948</td>\n",
       "      <td>2151-02-08</td>\n",
       "      <td>2151-02-08 03:07:00</td>\n",
       "      <td>2151-02-08 07:19:55</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Nursing Progress Note</td>\n",
       "      <td>20063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respiratory failure, acute (not ARDS/[**Doctor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>308714</td>\n",
       "      <td>316480</td>\n",
       "      <td>31916</td>\n",
       "      <td>146431</td>\n",
       "      <td>2112-02-23</td>\n",
       "      <td>2112-02-23 03:51:00</td>\n",
       "      <td>2112-02-23 04:03:01</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Nursing Progress Note</td>\n",
       "      <td>14419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respiratory failure, acute (not ARDS/[**Doctor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>308715</td>\n",
       "      <td>317749</td>\n",
       "      <td>29959</td>\n",
       "      <td>167558</td>\n",
       "      <td>2117-02-02</td>\n",
       "      <td>2117-02-02 03:10:00</td>\n",
       "      <td>2117-02-02 03:10:38</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Nursing Progress Note</td>\n",
       "      <td>15526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thromboembolism, other\\n   Assessment:\\n   Act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>308716</td>\n",
       "      <td>317750</td>\n",
       "      <td>27866</td>\n",
       "      <td>109679</td>\n",
       "      <td>2143-04-09</td>\n",
       "      <td>2143-04-09 03:22:00</td>\n",
       "      <td>2143-04-09 03:22:26</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Nursing Progress Note</td>\n",
       "      <td>21297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ms. [**Known lastname 1170**] is an 83 yo F w/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6e139ee-b127-43e4-ada3-ffa0e0d0f4b3')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-e6e139ee-b127-43e4-ada3-ffa0e0d0f4b3 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-e6e139ee-b127-43e4-ada3-ffa0e0d0f4b3');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-43e616d8-c762-48e3-834f-170d16fb54de\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-43e616d8-c762-48e3-834f-170d16fb54de')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-43e616d8-c762-48e3-834f-170d16fb54de button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  ROW_ID  SUBJECT_ID  HADM_ID  \\\n",
       "0             0             0      308707  316398       31608   152365   \n",
       "1             1             1      308711  316402       29076   193948   \n",
       "2             2             2      308714  316480       31916   146431   \n",
       "3             3             3      308715  317749       29959   167558   \n",
       "4             4             4      308716  317750       27866   109679   \n",
       "\n",
       "    CHARTDATE            CHARTTIME            STORETIME CATEGORY  \\\n",
       "0  2133-01-19  2133-01-19 06:53:00  2133-01-19 06:53:16  Nursing   \n",
       "1  2151-02-08  2151-02-08 03:07:00  2151-02-08 07:19:55  Nursing   \n",
       "2  2112-02-23  2112-02-23 03:51:00  2112-02-23 04:03:01  Nursing   \n",
       "3  2117-02-02  2117-02-02 03:10:00  2117-02-02 03:10:38  Nursing   \n",
       "4  2143-04-09  2143-04-09 03:22:00  2143-04-09 03:22:26  Nursing   \n",
       "\n",
       "             DESCRIPTION   CGID  ISERROR  \\\n",
       "0  Nursing Progress Note  19650      NaN   \n",
       "1  Nursing Progress Note  20063      NaN   \n",
       "2  Nursing Progress Note  14419      NaN   \n",
       "3  Nursing Progress Note  15526      NaN   \n",
       "4  Nursing Progress Note  21297      NaN   \n",
       "\n",
       "                                       document_text  \n",
       "0  TITLE:\\n   Respiratory failure, acute (not ARD...  \n",
       "1  Respiratory failure, acute (not ARDS/[**Doctor...  \n",
       "2  Respiratory failure, acute (not ARDS/[**Doctor...  \n",
       "3  Thromboembolism, other\\n   Assessment:\\n   Act...  \n",
       "4  Ms. [**Known lastname 1170**] is an 83 yo F w/...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_nursing_notes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzGiPtqSXu5C"
   },
   "source": [
    "If we can save the processed/generated features in some format and load them to save on reprocessing time, that would be ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5D2UyMauYREy"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "cleaned_nursing_notes['processed_text'] = cleaned_nursing_notes['document_text'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "704TVxwz8Ye4"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_counts = defaultdict(int)\n",
    "for _, row in cleaned_nursing_notes.iterrows():\n",
    "  for word in row['processed_text']:\n",
    "    word_counts[word] += 1\n",
    "\n",
    "filtered_processed_text = []\n",
    "for _, row in cleaned_nursing_notes.iterrows():\n",
    "  new_processed_text = [word for word in row['processed_text'] if word_counts[word] >= 10]\n",
    "  filtered_processed_text.append(new_processed_text)\n",
    "\n",
    "cleaned_nursing_notes['processed_text'] = filtered_processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GedBGD9M8O2n"
   },
   "outputs": [],
   "source": [
    "cleaned_nursing_notes.to_csv('filtered_cleaned_raw_nursing_notes_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvTPsfGAivhP"
   },
   "source": [
    "############################################\n",
    "\n",
    "Target labels\n",
    "\n",
    "Section 4: Code Group Prediction, pg8-9\n",
    "\n",
    "for each nursing note of a patient p, target multi-labels are diagnostic code groups\n",
    "see Fig. 8 pg. 9\n",
    "\n",
    "SOURCE: https://www.tdrdata.com/IPD/ipd_SearchForICD9CodesAndDescriptions\n",
    "1. 001 - 139\tInfectious and Parasitic Diseases\n",
    "2. 140 - 239\tNeoplasms\n",
    "3. 240 - 279\tEndocrine, Nutritional, Metabolic, Immunity\n",
    "4. 280 - 289\tBlood and Blood-Forming Organs -->\n",
    "5. 290 - 319\tMental Disorders\n",
    "6. 320 - 389\tNervous System and Sense Organs\n",
    "7. 390 - 459\tCirculatory System\n",
    "8. 460 - 519\tRespiratory System\n",
    "9. 520 - 579\tDigestive System\n",
    "10. 580 - 629\tGenitourinary System\n",
    "11. 630 - 677\tPregnancy, Childbirth, and the Puerperium\n",
    "12. 680 - 709\tSkin and Subcutaneous Tissue\n",
    "13. 710 - 739\tMusculoskeletal System and Connective Tissue\n",
    "14. 740 - 759\tCongenital Anomalies\n",
    "<!-- 760 - 779\tConditions Originating in the Perinatal Period -->\n",
    "15. 780 - 789\tSymptoms\n",
    "16. 790 - 796\tNonspecific Abnormal Findings\n",
    "17. 797 - 799\tIll-defined and Unknown Causes of Morbidity and Mortality\n",
    "18. 800 - 999\tInjury and Poisoning\n",
    "19. V Codes\tSupplemental V-Codes, Ref Codes\tReference Codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Bfcsqh401tn"
   },
   "source": [
    "# ETL | Target Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5No0X3hQih2U"
   },
   "outputs": [],
   "source": [
    "target_label_code_groups = {\n",
    "    1: [1, 139, \"Infectious and Parasitic Diseases\"],\n",
    "    2: [140, 239, \"Neoplasms\"],\n",
    "    3: [240, 279, \"Endocrine, Nutritional, Metabolic, Immunity\"],\n",
    "    4: [280, 289, \"Blood and Blood-Forming Organs\"],\n",
    "    5: [290, 319, \"Mental Disorders\"],\n",
    "    6: [320, 389, \"Nervous System and Sense Organs\"],\n",
    "    7: [390, 459, \"Circulatory System\"],\n",
    "    8: [460, 519, \"Respiratory System\"],\n",
    "    9: [520, 579, \"Digestive System\"],\n",
    "    10: [580, 629, \"Genitourinary System\"],\n",
    "    11: [630, 677, \"Pregnancy, Childbirth, and the Puerperium\"],\n",
    "    12: [680, 709, \"Skin and Subcutaneous Tissue\"],\n",
    "    13: [710, 739, \"Musculoskeletal System and Connective Tissue\"],\n",
    "    14: [740, 759, \"Congenital Anomalies\"],\n",
    "    15: [780, 789, \"Symptoms\"],\n",
    "    16: [790, 796, \"Nonspecific Abnormal Findings\"],\n",
    "    17: [797, 799, \"Ill-defined and Unknown Causes of Morbidity and Mortality\"],\n",
    "    18: [800, 999, \"Injury and Poisoning\"],\n",
    "    # 19: [0, 0, \"Reference or Supplemental V-Codes\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwmeTTtPlKhd"
   },
   "outputs": [],
   "source": [
    "def get_multigroup_label(icd_code):\n",
    "    \"\"\"\n",
    "    return None for: cannot get first 3 numbers (null value) OR number does not match any groups (invalid code)\n",
    "    \"\"\"\n",
    "    # if letter, return group 19 as they start with letter\n",
    "    if isinstance(icd_code, str) and icd_code[0].isalpha():\n",
    "        return 19\n",
    "    try:\n",
    "        code_num = int(str(icd_code)[:3])  # Extract first 3 characters as an integer\n",
    "    except ValueError:\n",
    "        return None  # Return None if conversion fails\n",
    "\n",
    "    # Iterate through each group in target_label_code_groups to find the correct range\n",
    "    for label, (start, end, description) in target_label_code_groups.items():\n",
    "        if isinstance(start, int) and code_num in range(start, end + 1):  # +1 to include the end value\n",
    "            return label\n",
    "    return None  # Return None if no group is matched\n",
    "\n",
    "# Example application of function\n",
    "#diag_df['MULTIGROUP_LABEL_TARGET'] = diag_df['ICD9_CODE'].apply(get_multigroup_label).fillna(-1).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "T1EZy3Lgit-_",
    "outputId": "5ea1a977-f5bc-4d58-ac88-99d22b48def2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "diag_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-0a2dcaba-f4a6-4cbd-b015-3461e488b6f4\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>SEQ_NUM</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>MULTIGROUP_LABEL_TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1297</td>\n",
       "      <td>109</td>\n",
       "      <td>172335</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40301</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1298</td>\n",
       "      <td>109</td>\n",
       "      <td>172335</td>\n",
       "      <td>2.0</td>\n",
       "      <td>486</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1299</td>\n",
       "      <td>109</td>\n",
       "      <td>172335</td>\n",
       "      <td>3.0</td>\n",
       "      <td>58281</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1300</td>\n",
       "      <td>109</td>\n",
       "      <td>172335</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5855</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1301</td>\n",
       "      <td>109</td>\n",
       "      <td>172335</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4254</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a2dcaba-f4a6-4cbd-b015-3461e488b6f4')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-0a2dcaba-f4a6-4cbd-b015-3461e488b6f4 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-0a2dcaba-f4a6-4cbd-b015-3461e488b6f4');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-a27368cd-18d6-401d-829a-802fd63e3461\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a27368cd-18d6-401d-829a-802fd63e3461')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-a27368cd-18d6-401d-829a-802fd63e3461 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   ROW_ID  SUBJECT_ID  HADM_ID  SEQ_NUM ICD9_CODE  MULTIGROUP_LABEL_TARGET\n",
       "0    1297         109   172335      1.0     40301                        7\n",
       "1    1298         109   172335      2.0       486                        8\n",
       "2    1299         109   172335      3.0     58281                       10\n",
       "3    1300         109   172335      4.0      5855                       10\n",
       "4    1301         109   172335      5.0      4254                        7"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict['DIAGNOSES_ICD.csv'].head()\n",
    "diag_df = df_dict['DIAGNOSES_ICD.csv']\n",
    "# diag_df.shape\n",
    "diag_df['MULTIGROUP_LABEL_TARGET'] = diag_df['ICD9_CODE'].apply(get_multigroup_label).fillna(-1).astype(int)\n",
    "diag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AWsSHDiol8G",
    "outputId": "20c2db6c-69b4-48c8-985d-b4606a0c7632"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ROW_ID  SUBJECT_ID  HADM_ID  SEQ_NUM ICD9_CODE  MULTIGROUP_LABEL_TARGET  \\\n",
      "0    1297         109   172335      1.0     40301                        7   \n",
      "1    1298         109   172335      2.0       486                        8   \n",
      "2    1299         109   172335      3.0     58281                       10   \n",
      "3    1300         109   172335      4.0      5855                       10   \n",
      "4    1301         109   172335      5.0      4254                        7   \n",
      "\n",
      "   FIRST_VISIT_HADM_ID  \n",
      "0               102024  \n",
      "1               102024  \n",
      "2               102024  \n",
      "3               102024  \n",
      "4               102024  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-577f9ebf626c>:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  diag_df = diag_df.groupby('SUBJECT_ID', group_keys=False).apply(calculate_visit_columns)\n"
     ]
    }
   ],
   "source": [
    "def calculate_visit_columns(group):\n",
    "    hadm_ids = sorted(group['HADM_ID'].unique())\n",
    "\n",
    "    first_hadm = hadm_ids[0]\n",
    "\n",
    "    # All non-first HADM_IDs, excluding the first (lowest) one\n",
    "    all_non_first_hadm = hadm_ids[1:] if len(hadm_ids) > 1 else hadm_ids\n",
    "\n",
    "    # Add new columns to each row in the group\n",
    "    group['FIRST_VISIT_HADM_ID'] = first_hadm\n",
    "\n",
    "    return group\n",
    "\n",
    "# Step 2: Apply the function to each group of SUBJECT_ID\n",
    "diag_df = diag_df.groupby('SUBJECT_ID', group_keys=False).apply(calculate_visit_columns)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(diag_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIJp8mNk3us_",
    "outputId": "f635ca37-9e2c-454a-a082-06750361e8da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SUBJECT_ID DIAG_GROUPS_OF_FIRST_HADM_ONLY\n",
      "0           2                           [19]\n",
      "1           3          [1, 15, 10, 7, 12, 3]\n",
      "2           4          [1, 17, 3, 16, 9, 19]\n",
      "3           5                           [19]\n",
      "4           6              [7, 18, 3, 4, 19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-9aa7c7b6d738>:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  multi_group_label_df = diag_df.groupby('SUBJECT_ID', group_keys=False).apply(calculate_diag_groups).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "def calculate_diag_groups(group):\n",
    "    # Unique values of MULTIGROUP_LABEL_TARGET for the first_hadm_id\n",
    "    first_hadm_id = group['FIRST_VISIT_HADM_ID'].iloc[0]\n",
    "    diag_groups_first_hadm = group[group['HADM_ID'] == first_hadm_id]['MULTIGROUP_LABEL_TARGET'].unique().tolist()\n",
    "\n",
    "\n",
    "    # Return as a single row DataFrame\n",
    "    return pd.DataFrame({\n",
    "        'SUBJECT_ID': [group['SUBJECT_ID'].iloc[0]],\n",
    "        'DIAG_GROUPS_OF_FIRST_HADM_ONLY': [diag_groups_first_hadm],\n",
    "    })\n",
    "\n",
    "# Step 2: Apply the function to each SUBJECT_ID group to create multi_group_label_df\n",
    "multi_group_label_df = diag_df.groupby('SUBJECT_ID', group_keys=False).apply(calculate_diag_groups).reset_index(drop=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(multi_group_label_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxdIpqlY8wGx"
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCaGiDBcGCKn"
   },
   "outputs": [],
   "source": [
    "tagged_doc = [TaggedDocument(words=row['processed_text'], tags=[i]) for i, row in cleaned_nursing_notes.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PcW6FBBFhNz"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64htV-QRCgzV"
   },
   "outputs": [],
   "source": [
    "### this cell takes a few hours to run\n",
    "model = Doc2Vec(vector_size=500, min_count=2, epochs=25)\n",
    "model.build_vocab(tagged_doc)\n",
    "model.train(tagged_doc, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVq-dZ5YAwVH"
   },
   "outputs": [],
   "source": [
    "# run this instead\n",
    "model = Doc2Vec.load('doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdSSN9Uz-zWZ"
   },
   "outputs": [],
   "source": [
    "doc2vec_X = cleaned_nursing_notes['processed_text'].apply(model.infer_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3JNKrvCLY8R"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYWrZuLYLZUS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#doc2vec_X.to_csv('doc2vec_X.csv')\n",
    "aaa = pd.read_csv('doc2vec_X.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "ygeg3FY-kqN3",
    "outputId": "1dabdc66-9cea-425e-ce9e-a3d1b35713e0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "aaa"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-fe12ed7e-090d-4a3d-ad0a-78178f7047b1\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[ 1.49779171e-01 -6.70687318e-01 -5.50565541e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[-1.26888067e-01  9.07776952e-01 -2.15273455e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[ 0.09746442 -0.34964576  0.370125   -0.748971...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[ 1.65064052e-01  1.69632614e-01 -1.30764889e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[-2.49707773e-01 -8.20715576e-02 -4.45700377e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142105</th>\n",
       "      <td>142105</td>\n",
       "      <td>[-2.62968183e-01  7.14004040e-01 -6.59219086e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142106</th>\n",
       "      <td>142106</td>\n",
       "      <td>[ 0.8900905   0.12816179 -0.23402554 -0.462681...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142107</th>\n",
       "      <td>142107</td>\n",
       "      <td>[ 8.53470564e-02 -4.69048947e-01  2.52114475e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142108</th>\n",
       "      <td>142108</td>\n",
       "      <td>[-5.05132318e-01 -8.24761018e-02  9.42375064e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142109</th>\n",
       "      <td>142109</td>\n",
       "      <td>[ 2.91047454e-01  2.99765408e-01 -3.98288667e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142110 rows × 2 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe12ed7e-090d-4a3d-ad0a-78178f7047b1')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-fe12ed7e-090d-4a3d-ad0a-78178f7047b1 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-fe12ed7e-090d-4a3d-ad0a-78178f7047b1');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-996d3650-88d2-4e97-9f9d-1641457a47b3\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-996d3650-88d2-4e97-9f9d-1641457a47b3')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-996d3650-88d2-4e97-9f9d-1641457a47b3 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "  <div id=\"id_94d5a16a-b207-43aa-8284-18e3c83dda97\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('aaa')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_94d5a16a-b207-43aa-8284-18e3c83dda97 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('aaa');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "        Unnamed: 0                                     processed_text\n",
       "0                0  [ 1.49779171e-01 -6.70687318e-01 -5.50565541e-...\n",
       "1                1  [-1.26888067e-01  9.07776952e-01 -2.15273455e-...\n",
       "2                2  [ 0.09746442 -0.34964576  0.370125   -0.748971...\n",
       "3                3  [ 1.65064052e-01  1.69632614e-01 -1.30764889e-...\n",
       "4                4  [-2.49707773e-01 -8.20715576e-02 -4.45700377e-...\n",
       "...            ...                                                ...\n",
       "142105      142105  [-2.62968183e-01  7.14004040e-01 -6.59219086e-...\n",
       "142106      142106  [ 0.8900905   0.12816179 -0.23402554 -0.462681...\n",
       "142107      142107  [ 8.53470564e-02 -4.69048947e-01  2.52114475e-...\n",
       "142108      142108  [-5.05132318e-01 -8.24761018e-02  9.42375064e-...\n",
       "142109      142109  [ 2.91047454e-01  2.99765408e-01 -3.98288667e-...\n",
       "\n",
       "[142110 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa # [float(x) for x in aaa['processed_text'].iloc[0].replace('[', '').replace(']', '').replace('\\n', '').split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XsLLA6GlCVy"
   },
   "outputs": [],
   "source": [
    "bbb = pd.read_csv('filtered_cleaned_raw_nursing_notes_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aj7nKFd7BC7b",
    "outputId": "e00ea027-9ae0-49ab-90d9-aa293334ada5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6732"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbb['SUBJECT_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJqiSWi_6RPi"
   },
   "outputs": [],
   "source": [
    "ccc = pd.DataFrame({\n",
    "    'processed_text': aaa['processed_text'],\n",
    "    'SUBJECT_ID': bbb['SUBJECT_ID']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMNrjXlN64XK",
    "outputId": "98096354-4afb-42c7-82b7-4257f283bca1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      processed_text  SUBJECT_ID\n",
      "0  [ 1.49779171e-01 -6.70687318e-01 -5.50565541e-...       31608\n",
      "1  [-1.26888067e-01  9.07776952e-01 -2.15273455e-...       29076\n",
      "2  [ 0.09746442 -0.34964576  0.370125   -0.748971...       31916\n",
      "3  [ 1.65064052e-01  1.69632614e-01 -1.30764889e-...       29959\n",
      "4  [-2.49707773e-01 -8.20715576e-02 -4.45700377e-...       27866\n",
      "   SUBJECT_ID DIAG_GROUPS_OF_FIRST_HADM_ONLY\n",
      "0           2                           [19]\n",
      "1           3          [1, 15, 10, 7, 12, 3]\n",
      "2           4          [1, 17, 3, 16, 9, 19]\n",
      "3           5                           [19]\n",
      "4           6              [7, 18, 3, 4, 19]\n"
     ]
    }
   ],
   "source": [
    "print(ccc.head())\n",
    "print(multi_group_label_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gKI-VfJ7jMZ"
   },
   "outputs": [],
   "source": [
    "doc2vec_dataset_full = pd.merge(ccc, multi_group_label_df, on='SUBJECT_ID', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5SuORrN7nTB",
    "outputId": "b3dae768-80db-4778-c11c-48f8ad92ec5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['processed_text', 'SUBJECT_ID', 'DIAG_GROUPS_OF_FIRST_HADM_ONLY'], dtype='object')\n",
      "\n",
      "Shape of doc2vec_dataset_full: (142110, 3)\n",
      "\n",
      "Number of rows with empty values in each column:\n",
      "processed_text                    0\n",
      "SUBJECT_ID                        0\n",
      "DIAG_GROUPS_OF_FIRST_HADM_ONLY    0\n",
      "dtype: int64\n",
      "\n",
      "Number of patients:\n",
      "6732\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the dataset\n",
    "print(doc2vec_dataset_full.columns)\n",
    "print(\"\\nShape of doc2vec_dataset_full:\", doc2vec_dataset_full.shape)\n",
    "\n",
    "# Count rows with empty (NaN) values in each column\n",
    "empty_values = doc2vec_dataset_full.isna().sum()\n",
    "print(\"\\nNumber of rows with empty values in each column:\")\n",
    "print(empty_values)\n",
    "\n",
    "\n",
    "# Aggregated total of all unique values in DIAG_GROUPS_OF_LAST_HADM_ONLY\n",
    "unique_diag_groups = set().union(*doc2vec_dataset_full['DIAG_GROUPS_OF_FIRST_HADM_ONLY'])\n",
    "total_unique_diag_groups = len(unique_diag_groups)\n",
    "\n",
    "print(\"\\nNumber of patients:\")\n",
    "print(doc2vec_dataset_full['SUBJECT_ID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ifd211QBdOA"
   },
   "outputs": [],
   "source": [
    "# Save to a Pickle file\n",
    "with open(f'{os.getcwd()}/doc2vec_dataset_full.pkl', 'wb') as f:\n",
    "    pickle.dump(doc2vec_dataset_full, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkHP0dIT-IsZ"
   },
   "outputs": [],
   "source": [
    "doc2vec_dataset_lite = doc2vec_dataset[doc2vec_dataset['ONLY_ONE_VISIT'] == 0][['SUBJECT_ID','processed_text', 'DIAG_GROUPS_OF_LAST_HADM_ONLY']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcSBTvX1_fmc"
   },
   "outputs": [],
   "source": [
    "# Save to a Pickle file\n",
    "with open(f'{os.getcwd()}/doc2vec_dataset_lite.pkl', 'wb') as f:\n",
    "    pickle.dump(doc2vec_dataset_lite, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejc2e5e_-OwC",
    "outputId": "3fa9312f-91b3-4c63-9afd-58e18333766f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SUBJECT_ID', 'processed_text', 'DIAG_GROUPS_OF_LAST_HADM_ONLY'], dtype='object')\n",
      "\n",
      "Shape of doc2vec_dataset_lite: (30311, 3)\n",
      "\n",
      "Number of rows with empty values in each column:\n",
      "SUBJECT_ID                       0\n",
      "processed_text                   0\n",
      "DIAG_GROUPS_OF_LAST_HADM_ONLY    0\n",
      "dtype: int64\n",
      "\n",
      "Minimum count of separate values in processed_text: 6015\n",
      "Maximum count of separate values in processed_text: 8125\n",
      "Aggregated total of all unique values in DIAG_GROUPS_OF_LAST_HADM_ONLY: 19\n",
      "\n",
      "Number of patients:\n",
      "1324\n"
     ]
    }
   ],
   "source": [
    "print(doc2vec_dataset_lite.columns)\n",
    "print(\"\\nShape of doc2vec_dataset_lite:\", doc2vec_dataset_lite.shape)\n",
    "\n",
    "# Count rows with empty (NaN) values in each column\n",
    "empty_values = doc2vec_dataset_lite.isna().sum()\n",
    "print(\"\\nNumber of rows with empty values in each column:\")\n",
    "print(empty_values)\n",
    "# Calculate the count of separate values in each row of processed_text\n",
    "processed_text_lengths = doc2vec_dataset_lite['processed_text'].apply(len)\n",
    "\n",
    "# Minimum and maximum count of separate values in processed_text\n",
    "min_processed_text_count = processed_text_lengths.min()\n",
    "max_processed_text_count = processed_text_lengths.max()\n",
    "\n",
    "print(\"\\nMinimum count of separate values in processed_text:\", min_processed_text_count)\n",
    "print(\"Maximum count of separate values in processed_text:\", max_processed_text_count)\n",
    "\n",
    "# Aggregated total of all unique values in DIAG_GROUPS_OF_LAST_HADM_ONLY\n",
    "unique_diag_groups = set().union(*doc2vec_dataset_lite['DIAG_GROUPS_OF_LAST_HADM_ONLY'])\n",
    "total_unique_diag_groups = len(unique_diag_groups)\n",
    "\n",
    "print(\"Aggregated total of all unique values in DIAG_GROUPS_OF_LAST_HADM_ONLY:\", total_unique_diag_groups)\n",
    "print(\"\\nNumber of patients:\")\n",
    "print(doc2vec_dataset_lite['SUBJECT_ID'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfKIDldPA3o1"
   },
   "source": [
    "# Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BLyVsN-JYe3f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxndK3d8Xro9"
   },
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YVJq4LKQXtMI"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 75)\n",
    "        self.fc2 = nn.Linear(75, 19)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLQjL3PeYNGZ"
   },
   "source": [
    "ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Riol93jwYOqZ"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 289)\n",
    "        self.conv1 = nn.Conv2d(1, 19, kernel_size=3)\n",
    "        self.fc2 = nn.Linear(19 * 225, 19) #literally just guessing gor conv output size will need to actually calculate\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 1, int(289**0.5), int(289**0.5))  # reshape for convolution\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.flatten(x, 1)  # lfatten for fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kWTWlHyavEf"
   },
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "z4a2T2xRawiV"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 289)\n",
    "        self.lstm = nn.LSTM(289, 300)\n",
    "        self.fc2 = nn.Linear(300, 19)\n",
    "\n",
    "    def forward(self, x, h0, c0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.reshape(x.size(0), 1, -1) # might need resahping\n",
    "        #print(x.shape)\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc2(out)\n",
    "        return out, h0, c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20llc6sickPc"
   },
   "source": [
    "Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LsTnMFzecNl5"
   },
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 289)\n",
    "        self.lstm = nn.LSTM(289, 300, bidirectional = True)\n",
    "        self.fc2 = nn.Linear(600, 19)\n",
    "\n",
    "    def forward(self, x, h0, c0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(x.size(0), 1, -1) # might need resahping\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        #print(out.shape)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc2(out)\n",
    "        return out, h0, c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ-ttmIgct7I"
   },
   "source": [
    "Conv-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OubygAn7c3Co"
   },
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 289)\n",
    "        self.conv1 = nn.Conv2d(1, 19, kernel_size=3)\n",
    "        self.fc2 = nn.Linear(19 * 225, 289)\n",
    "        self.lstm = nn.LSTM(289, 300)\n",
    "        self.fc3 = nn.Linear(300, 19)\n",
    "\n",
    "    def forward(self, x, h0, c0):\n",
    "      x = F.relu(self.fc1(x))\n",
    "      x = x.view(-1, 1, int(289**0.5), int(289**0.5))\n",
    "      x = F.relu(self.conv1(x)) ## SIZE? !\n",
    "      x = torch.flatten(x, 1)\n",
    "      x = F.relu(self.fc2(x))\n",
    "      x = x.view(x.size(0), 1, -1)\n",
    "      out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "      out = out.squeeze()\n",
    "      out = self.fc3(out)\n",
    "      return out, h0, c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHmscVXOBDMu"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Hyperparam Finding with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5YbqEpAlaWJ"
   },
   "outputs": [],
   "source": [
    "## load data\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "#df_y = pd.read_csv('filtered_cleaned_raw_nursing_notes_processed.csv')\n",
    "#df_y\n",
    "#targets = df_y['ICD9_CODE'].apply(get_multigroup_label).fillna(-1).astype(int)\n",
    "def csv_load_helper(x):\n",
    "  return np.array([float(e) for e in x.replace('[', '').replace(']', '').replace('\\n', '').split()])\n",
    "\n",
    "def y_to_onehot(y):\n",
    "  meep = np.eye(19, dtype='uint8')[y - 1].sum(axis = 0)\n",
    "  return meep\n",
    "with open(f'{os.getcwd()}/doc2vec_dataset_full.pkl', 'rb') as f:\n",
    "  df_X = pickle.load(f)\n",
    "df_X['processed_text'] = df_X['processed_text'].apply(csv_load_helper)\n",
    "X = np.stack(df_X['processed_text'].to_numpy())\n",
    "y = np.stack(df_X['DIAG_GROUPS_OF_FIRST_HADM_ONLY'].apply(np.array).apply(y_to_onehot).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7NIOiaQxsuc",
    "outputId": "5957c44e-d1f6-4a04-9111-c8793cc9cb5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.631    \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.5776   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.7202   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.6689   \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.7155   \u001b[39m | \u001b[39m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.7359   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.7027   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.653    \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.6386   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.6059   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.5724   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.7488   \u001b[39m | \u001b[35m0.005708 \u001b[39m |\n",
      "| \u001b[35m13       \u001b[39m | \u001b[35m0.7523   \u001b[39m | \u001b[35m0.004521 \u001b[39m |\n",
      "| \u001b[35m14       \u001b[39m | \u001b[35m0.7542   \u001b[39m | \u001b[35m0.00361  \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.7538   \u001b[39m | \u001b[39m0.003704 \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.5643   \u001b[39m | \u001b[39m0.08583  \u001b[39m |\n",
      "| \u001b[35m17       \u001b[39m | \u001b[35m0.7575   \u001b[39m | \u001b[35m0.002911 \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.6865   \u001b[39m | \u001b[39m0.02418  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.756    \u001b[39m | \u001b[39m0.003598 \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.7549   \u001b[39m | \u001b[39m0.004133 \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.7498   \u001b[39m | \u001b[39m0.005127 \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.7571   \u001b[39m | \u001b[39m0.00323  \u001b[39m |\n",
      "| \u001b[35m23       \u001b[39m | \u001b[35m0.7582   \u001b[39m | \u001b[35m0.002393 \u001b[39m |\n",
      "| \u001b[35m24       \u001b[39m | \u001b[35m0.7591   \u001b[39m | \u001b[35m0.001816 \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.7577   \u001b[39m | \u001b[39m0.001243 \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.754    \u001b[39m | \u001b[39m0.0007173\u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.7459   \u001b[39m | \u001b[39m0.006364 \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.7442   \u001b[39m | \u001b[39m0.007031 \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.7397   \u001b[39m | \u001b[39m0.007718 \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.7389   \u001b[39m | \u001b[39m0.008523 \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.7328   \u001b[39m | \u001b[39m0.01008  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.7302   \u001b[39m | \u001b[39m0.01087  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.7264   \u001b[39m | \u001b[39m0.01169  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.7235   \u001b[39m | \u001b[39m0.01255  \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.7201   \u001b[39m | \u001b[39m0.01347  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.7124   \u001b[39m | \u001b[39m0.01588  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.5836   \u001b[39m | \u001b[39m0.06246  \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.7087   \u001b[39m | \u001b[39m0.01702  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.5636   \u001b[39m | \u001b[39m0.09281  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.5836   \u001b[39m | \u001b[39m0.07894  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for MLP(\n",
      "  (fc1): Linear(in_features=500, out_features=75, bias=True)\n",
      "  (fc2): Linear(in_features=75, out_features=19, bias=True)\n",
      ") {'target': 0.7591438850198247, 'params': {'lr': 0.001815613662380016}} =================\n",
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.581    \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.5578   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.7692   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.621    \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.7528   \u001b[39m | \u001b[39m0.01476  \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.7664   \u001b[39m | \u001b[39m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.7451   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.6508   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.6256   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.5416   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.8072   \u001b[39m | \u001b[35m0.004329 \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.7908   \u001b[39m | \u001b[39m0.00578  \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.5302   \u001b[39m | \u001b[39m0.08516  \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m0.8075   \u001b[39m | \u001b[35m0.004239 \u001b[39m |\n",
      "| \u001b[35m16       \u001b[39m | \u001b[35m0.812    \u001b[39m | \u001b[35m0.002967 \u001b[39m |\n",
      "| \u001b[35m17       \u001b[39m | \u001b[35m0.8145   \u001b[39m | \u001b[35m0.002467 \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.5579   \u001b[39m | \u001b[39m0.06312  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.8077   \u001b[39m | \u001b[39m0.004319 \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.724    \u001b[39m | \u001b[39m0.02336  \u001b[39m |\n",
      "| \u001b[35m21       \u001b[39m | \u001b[35m0.8174   \u001b[39m | \u001b[35m0.001895 \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.7733   \u001b[39m | \u001b[39m0.01195  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09233  \u001b[39m |\n",
      "| \u001b[35m24       \u001b[39m | \u001b[35m0.8182   \u001b[39m | \u001b[35m0.00163  \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.5588   \u001b[39m | \u001b[39m0.07848  \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.8181   \u001b[39m | \u001b[39m0.001531 \u001b[39m |\n",
      "| \u001b[35m27       \u001b[39m | \u001b[35m0.8189   \u001b[39m | \u001b[35m0.001646 \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.8184   \u001b[39m | \u001b[39m0.001659 \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.8175   \u001b[39m | \u001b[39m0.001656 \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.8184   \u001b[39m | \u001b[39m0.001611 \u001b[39m |\n",
      "| \u001b[35m31       \u001b[39m | \u001b[35m0.8192   \u001b[39m | \u001b[35m0.001563 \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.8177   \u001b[39m | \u001b[39m0.001789 \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.815    \u001b[39m | \u001b[39m0.002618 \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.8131   \u001b[39m | \u001b[39m0.002802 \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.8171   \u001b[39m | \u001b[39m0.002063 \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.8162   \u001b[39m | \u001b[39m0.002261 \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.7793   \u001b[39m | \u001b[39m0.007457 \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.584    \u001b[39m | \u001b[39m0.04775  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.5315   \u001b[39m | \u001b[39m0.05855  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.674    \u001b[39m | \u001b[39m0.02629  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvNet(\n",
      "  (fc1): Linear(in_features=500, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=3600, out_features=19, bias=True)\n",
      ") {'target': 0.8192432301632667, 'params': {'lr': 0.0015634378413837329}} =================\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from bayes_opt import BayesianOptimization\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "models = [\n",
    "    MLP(input_size=500),\n",
    "    ConvNet(input_size=500)\n",
    "]\n",
    "lstm_models = [\n",
    "    LSTM(input_size=500),\n",
    "    BiLSTM(input_size=500),\n",
    "    ConvLSTM(input_size=500)\n",
    "]\n",
    "# BATCH_SIZE always 128, train for 8 epochs\n",
    "for model in models:\n",
    "  def eval_model(lr, model=model, X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    device = torch.device(\"cuda\")\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(X_batch)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "\n",
    "\n",
    "      with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = torch.sigmoid(model(X_test)).cpu().numpy()\n",
    "        #predicted = (outputs > 0.6).float().cpu().numpy()\n",
    "        #print(train_losses)\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        #print(roc)\n",
    "        rocs.append(roc)\n",
    "    #print(rocs)\n",
    "    return np.mean(rocs)\n",
    "  # bayesian optimization on all 5 models....this might take a fews days? :3\n",
    "\n",
    "\n",
    "  bounds = {\n",
    "    'lr': (0.0001, 0.1)\n",
    "  }\n",
    "  optimizer = BayesianOptimization(\n",
    "      f=eval_model,\n",
    "      pbounds=bounds,\n",
    "      verbose=2,  # verbose = 1 prints only when a maximum\n",
    "      # is observed, verbose = 0 is silent\n",
    "      random_state=1,\n",
    "  )\n",
    "  optimizer.maximize(\n",
    "      init_points=10,\n",
    "      n_iter=30\n",
    "  )\n",
    "  print(f'================= BEST MODEL for {model}', optimizer.max, '=================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSivLiGZTpsW",
    "outputId": "0b156917-346f-4b7d-e4d8-912389ca6f12",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.5001   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.51     \u001b[39m | \u001b[35m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.7375   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.5146   \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.7082   \u001b[39m | \u001b[39m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.7736   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.6397   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.4996   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.5001   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.5003   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.8036   \u001b[39m | \u001b[35m0.00588  \u001b[39m |\n",
      "| \u001b[35m13       \u001b[39m | \u001b[35m0.816    \u001b[39m | \u001b[35m0.004541 \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08579  \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m0.8163   \u001b[39m | \u001b[35m0.004459 \u001b[39m |\n",
      "| \u001b[35m16       \u001b[39m | \u001b[35m0.8211   \u001b[39m | \u001b[35m0.003615 \u001b[39m |\n",
      "| \u001b[35m17       \u001b[39m | \u001b[35m0.826    \u001b[39m | \u001b[35m0.003032 \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06307  \u001b[39m |\n",
      "| \u001b[35m19       \u001b[39m | \u001b[35m0.8296   \u001b[39m | \u001b[35m0.002312 \u001b[39m |\n",
      "| \u001b[35m20       \u001b[39m | \u001b[35m0.8298   \u001b[39m | \u001b[35m0.002471 \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.8296   \u001b[39m | \u001b[39m0.002467 \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.8292   \u001b[39m | \u001b[39m0.002413 \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.8284   \u001b[39m | \u001b[39m0.002329 \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.8293   \u001b[39m | \u001b[39m0.002526 \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.8257   \u001b[39m | \u001b[39m0.0013   \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09289  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0789   \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.5435   \u001b[39m | \u001b[39m0.02409  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.791    \u001b[39m | \u001b[39m0.007419 \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.4999   \u001b[39m | \u001b[39m0.04797  \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.7455   \u001b[39m | \u001b[39m0.01198  \u001b[39m |\n",
      "| \u001b[35m32       \u001b[39m | \u001b[35m0.8303   \u001b[39m | \u001b[35m0.001685 \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.8299   \u001b[39m | \u001b[39m0.001831 \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.8302   \u001b[39m | \u001b[39m0.001647 \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.8296   \u001b[39m | \u001b[39m0.001658 \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.8301   \u001b[39m | \u001b[39m0.001737 \u001b[39m |\n",
      "| \u001b[35m37       \u001b[39m | \u001b[35m0.8306   \u001b[39m | \u001b[35m0.001729 \u001b[39m |\n",
      "| \u001b[35m38       \u001b[39m | \u001b[35m0.8307   \u001b[39m | \u001b[35m0.001736 \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.8306   \u001b[39m | \u001b[39m0.001756 \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.8295   \u001b[39m | \u001b[39m0.001769 \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for LSTM(\n",
      "  (fc1): Linear(in_features=500, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc2): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.8307306950133689, 'params': {'lr': 0.001736156863213412}} =================\n",
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.7498   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01476  \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m0.8294   \u001b[39m | \u001b[35m0.001508 \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.6519   \u001b[39m | \u001b[39m0.003782 \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.8236   \u001b[39m | \u001b[39m0.002143 \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.1      \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m0.8311   \u001b[39m | \u001b[35m0.001572 \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08603  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06299  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07904  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09298  \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04787  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0245   \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.8286   \u001b[39m | \u001b[39m0.001802 \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05846  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0675   \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09649  \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07553  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.4996   \u001b[39m | \u001b[39m0.08252  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.5001   \u001b[39m | \u001b[39m0.08952  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.8036   \u001b[39m | \u001b[39m0.002662 \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0448   \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05089  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.8301   \u001b[39m | \u001b[39m0.001654 \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.5001   \u001b[39m | \u001b[39m0.006359 \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0274   \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.02161  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01203  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0372   \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06979  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.8241   \u001b[39m | \u001b[39m0.0007783\u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05619  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvLSTM(\n",
      "  (fc1): Linear(in_features=500, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=3600, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc3): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.8311124257598317, 'params': {'lr': 0.0015719447694928059}} =================\n"
     ]
    }
   ],
   "source": [
    "lstm_models = [\n",
    "    LSTM(input_size=500),\n",
    "    ConvLSTM(input_size=500)\n",
    "]\n",
    "# BATCH_SIZE always 128, train for 8 epochs\n",
    "for model in lstm_models:\n",
    "  def eval_model(lr, model=model, X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    device = torch.device(\"cuda\")\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs, h_o, c_o = model(X_batch, h_o, c_o)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "      with torch.no_grad():\n",
    "        model.eval()\n",
    "        h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "        outputs = torch.sigmoid(model(X_test, h_o, c_o)[0]).cpu().numpy()\n",
    "        #predicted = (outputs > 0.5).float().cpu().numpy()\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        rocs.append(roc)\n",
    "\n",
    "    return np.mean(rocs)\n",
    "\n",
    "\n",
    "  bounds = {\n",
    "    'lr': (0.0001, 0.1)\n",
    "  }\n",
    "  optimizer = BayesianOptimization(\n",
    "      f=eval_model,\n",
    "      pbounds=bounds,\n",
    "      verbose=2,  # verbose = 1 prints only when a maximum\n",
    "      # is observed, verbose = 0 is silent\n",
    "      random_state=1,\n",
    "  )\n",
    "  optimizer.maximize(\n",
    "      init_points=10,\n",
    "      n_iter=30\n",
    "  )\n",
    "  print(f'================= BEST MODEL for {model}', optimizer.max, '=================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOQsv5u0Cv5I",
    "outputId": "ae6690b0-5612-4103-ab3a-ff782a46f71a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.4997   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.5001   \u001b[39m | \u001b[35m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.7466   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.5256   \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.7139   \u001b[39m | \u001b[39m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.7469   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.6954   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.5146   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.5021   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.5088   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.8278   \u001b[39m | \u001b[35m0.004414 \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.8206   \u001b[39m | \u001b[39m0.005462 \u001b[39m |\n",
      "| \u001b[35m14       \u001b[39m | \u001b[35m0.835    \u001b[39m | \u001b[35m0.003736 \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m0.8361   \u001b[39m | \u001b[35m0.003651 \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.4998   \u001b[39m | \u001b[39m0.08599  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.5005   \u001b[39m | \u001b[39m0.06292  \u001b[39m |\n",
      "| \u001b[35m18       \u001b[39m | \u001b[35m0.8448   \u001b[39m | \u001b[35m0.002552 \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.8423   \u001b[39m | \u001b[39m0.002817 \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.6036   \u001b[39m | \u001b[39m0.02369  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.5042   \u001b[39m | \u001b[39m0.07898  \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09296  \u001b[39m |\n",
      "| \u001b[35m23       \u001b[39m | \u001b[35m0.8487   \u001b[39m | \u001b[35m0.001951 \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.8478   \u001b[39m | \u001b[39m0.002088 \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04813  \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.848    \u001b[39m | \u001b[39m0.001687 \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.8484   \u001b[39m | \u001b[39m0.001854 \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.8472   \u001b[39m | \u001b[39m0.001867 \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.7252   \u001b[39m | \u001b[39m0.01198  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.8056   \u001b[39m | \u001b[39m0.007054 \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.8475   \u001b[39m | \u001b[39m0.001927 \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.8481   \u001b[39m | \u001b[39m0.001618 \u001b[39m |\n",
      "| \u001b[35m33       \u001b[39m | \u001b[35m0.8488   \u001b[39m | \u001b[35m0.001502 \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.5005   \u001b[39m | \u001b[39m0.06751  \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.4998   \u001b[39m | \u001b[39m0.05838  \u001b[39m |\n",
      "| \u001b[35m36       \u001b[39m | \u001b[35m0.8489   \u001b[39m | \u001b[35m0.001298 \u001b[39m |\n",
      "| \u001b[35m37       \u001b[39m | \u001b[35m0.849    \u001b[39m | \u001b[35m0.001381 \u001b[39m |\n",
      "| \u001b[35m38       \u001b[39m | \u001b[35m0.8495   \u001b[39m | \u001b[35m0.001371 \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.8489   \u001b[39m | \u001b[39m0.001376 \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.8494   \u001b[39m | \u001b[39m0.001381 \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvLSTM(\n",
      "  (fc1): Linear(in_features=500, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=3600, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc3): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.8495349786439992, 'params': {'lr': 0.00137116789509993}} =================\n"
     ]
    }
   ],
   "source": [
    "def eval_model(lr, model=BiLSTM(input_size=500), X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    device = torch.device(\"cuda\")\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs, h_o, c_o = model(X_batch, h_o, c_o)\n",
    "          #print(outputs, y_batch)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "        outputs = torch.sigmoid(model(X_test, h_o, c_o)[0]).cpu().numpy()\n",
    "        #predicted = (outputs > 0.5).float().cpu().numpy()\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        rocs.append(roc)\n",
    "\n",
    "    return np.mean(rocs)\n",
    "\n",
    "\n",
    "bounds = {\n",
    "'lr': (0.0001, 0.1)\n",
    "}\n",
    "optimizer = BayesianOptimization(\n",
    "  f=eval_model,\n",
    "  pbounds=bounds,\n",
    "  verbose=2,  # verbose = 1 prints only when a maximum\n",
    "  # is observed, verbose = 0 is silent\n",
    "  random_state=1,\n",
    ")\n",
    "optimizer.maximize(\n",
    "  init_points=10,\n",
    "  n_iter=30\n",
    ")\n",
    "print(f'================= BEST MODEL for {model}', optimizer.max, '=================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation with Optimal Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "45Zm2ZCCCv5J"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score, roc_auc_score, average_precision_score\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from bayes_opt import BayesianOptimization\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def csv_load_helper(x):\n",
    "  return np.array([float(e) for e in x.replace('[', '').replace(']', '').replace('\\n', '').split()])\n",
    "\n",
    "def y_to_onehot(y):\n",
    "  meep = np.eye(19, dtype='uint8')[y - 1].sum(axis = 0)\n",
    "  return meep\n",
    "with open(f'{os.getcwd()}/doc2vec_dataset_full.pkl', 'rb') as f:\n",
    "  df_X = pickle.load(f)\n",
    "df_X['processed_text'] = df_X['processed_text'].apply(csv_load_helper)\n",
    "X = np.stack(df_X['processed_text'].to_numpy())\n",
    "y = np.stack(df_X['DIAG_GROUPS_OF_FIRST_HADM_ONLY'].apply(np.array).apply(y_to_onehot).to_numpy())\n",
    "\n",
    "#instantiate models and params\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "lstm = LSTM(input_size=500).to(device)\n",
    "bilstm = BiLSTM(input_size=500).to(device)\n",
    "convlstm = ConvLSTM(input_size=500).to(device)\n",
    "\n",
    "splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "\n",
    "\n",
    "\n",
    "lstm_optimizer = optim.Adam(lstm.parameters(), lr=0.001736156863213412)\n",
    "bilstm_optimizer = optim.Adam(bilstm.parameters(), lr=0.00137116789509993)\n",
    "convlstm_optimizer = optim.Adam(convlstm.parameters(), lr=0.0015719447694928059)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "wnVxVni0Cv5J",
    "outputId": "475feeaf-c4fa-43c6-d324-508f02108ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.781204 & 0.299198 & 0.678178 & 0.647826 & 0.758894\n",
      "0.001391 & 0.004091 & 0.005104 & 0.002717 & 0.001308\n"
     ]
    }
   ],
   "source": [
    "#train mlp\n",
    "mlp_train_losses, mlp_train_accuracies, mlp_train_mcc, mlp_train_f1, mlp_train_auprc, mlp_train_auroc = [], [], [], [], [], []\n",
    "mlp_test_losses, mlp_test_accuracies, mlp_test_mcc, mlp_test_f1, mlp_test_auprc, mlp_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for i in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        mlp = MLP(input_size=500).to(device)\n",
    "        mlp_optimizer = optim.Adam(mlp.parameters(), lr=0.001815613662380016)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            mlp.train()\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              mlp_optimizer.zero_grad()\n",
    "              outputs = mlp(X_batch)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              mlp_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              mlp_optimizer.step()\n",
    "        with torch.no_grad():\n",
    "          mlp.eval()\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(mlp(X_train))\n",
    "          test_outputs = torch.sigmoid(mlp(X_test))\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          mlp_test_losses.append(test_loss)\n",
    "          mlp_test_accuracies.append(test_acc)\n",
    "          mlp_test_mcc.append(test_mcc)\n",
    "          mlp_test_f1.append(test_f1)\n",
    "          mlp_test_auprc.append(test_auprc)\n",
    "          mlp_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #mlp_train_losses.append(train_loss.item())\n",
    "          mlp_train_accuracies.append(train_acc)\n",
    "          mlp_train_mcc.append(train_mcc)\n",
    "          mlp_train_f1.append(train_f1)\n",
    "          mlp_train_auprc.append(train_auprc)\n",
    "          mlp_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(mlp_test_accuracies), np.mean(mlp_test_mcc), np.mean(mlp_test_f1), np.mean(mlp_test_auprc), np.mean(mlp_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(mlp_test_accuracies), np.std(mlp_test_mcc), np.std(mlp_test_f1), np.std(mlp_test_auprc), np.std(mlp_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "u6cXl1fOCv5J",
    "outputId": "f35cfac6-a7dc-400f-b466-b0c82de27218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.810427 & 0.432274 & 0.749642 & 0.730863 & 0.819879\n",
      "0.001211 & 0.003758 & 0.003676 & 0.003138 & 0.001583\n"
     ]
    }
   ],
   "source": [
    "convnet_train_losses, convnet_train_accuracies, convnet_train_mcc, convnet_train_f1, convnet_train_auprc, convnet_train_auroc = [], [], [], [], [], []\n",
    "convnet_test_losses, convnet_test_accuracies, convnet_test_mcc, convnet_test_f1, convnet_test_auprc, convnet_test_auroc = [], [], [], [], [], []\n",
    "for i in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        convnet = ConvNet(input_size=500).to(device)\n",
    "        convnet_optimizer = optim.Adam(convnet.parameters(), lr=0.0015634378413837329)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            convnet.train()\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              convnet_optimizer.zero_grad()\n",
    "              outputs = convnet(X_batch)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              convnet_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              convnet_optimizer.step()\n",
    "        convlstm.eval()\n",
    "        with torch.no_grad():\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(convnet(X_train))\n",
    "          test_outputs = torch.sigmoid(convnet(X_test))\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          convnet_test_losses.append(test_loss)\n",
    "          convnet_test_accuracies.append(test_acc)\n",
    "          convnet_test_mcc.append(test_mcc)\n",
    "          convnet_test_f1.append(test_f1)\n",
    "          convnet_test_auprc.append(test_auprc)\n",
    "          convnet_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #convnet_train_losses.append(train_loss.item())\n",
    "          convnet_train_accuracies.append(train_acc)\n",
    "          convnet_train_mcc.append(train_mcc)\n",
    "          convnet_train_f1.append(train_f1)\n",
    "          convnet_train_auprc.append(train_auprc)\n",
    "          convnet_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convnet_test_accuracies), np.mean(convnet_test_mcc), np.mean(convnet_test_f1), np.mean(convnet_test_auprc), np.mean(convnet_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(convnet_test_accuracies), np.std(convnet_test_mcc), np.std(convnet_test_f1), np.std(convnet_test_auprc), np.std(convnet_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "jI38lwtCCv5J",
    "outputId": "5ea32f23-2d4c-4717-af89-6ca0a3caf8e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.817079 & 0.443159 & 0.756501 & 0.737908 & 0.83042\n",
      "0.001435 & 0.003738 & 0.003907 & 0.002523 & 0.001384\n"
     ]
    }
   ],
   "source": [
    "lstm_train_losses, lstm_train_accuracies, lstm_train_mcc, lstm_train_f1, lstm_train_auprc, lstm_train_auroc = [], [], [], [], [], []\n",
    "lstm_test_losses, lstm_test_accuracies, lstm_test_mcc, lstm_test_f1, lstm_test_auprc, lstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for i in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        #print(len(train))\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        lstm = LSTM(input_size=500).to(device)\n",
    "        lstm_optimizer = optim.Adam(lstm.parameters(), lr=0.001736156863213412)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            lstm.train()\n",
    "            h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              lstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = lstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              lstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              lstm_optimizer.step()\n",
    "            lstm.eval()\n",
    "        with torch.no_grad():\n",
    "          h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(lstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          test_outputs = torch.sigmoid(lstm(X_test, h_o, c_o)[0])\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          lstm_test_losses.append(test_loss)\n",
    "          lstm_test_accuracies.append(test_acc)\n",
    "          lstm_test_mcc.append(test_mcc)\n",
    "          lstm_test_f1.append(test_f1)\n",
    "          lstm_test_auprc.append(test_auprc)\n",
    "          lstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #lstm_train_losses.append(train_loss.item())\n",
    "          lstm_train_accuracies.append(train_acc)\n",
    "          lstm_train_mcc.append(train_mcc)\n",
    "          lstm_train_f1.append(train_f1)\n",
    "          lstm_train_auprc.append(train_auprc)\n",
    "          lstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(lstm_test_accuracies), np.mean(lstm_test_mcc), np.mean(lstm_test_f1), np.mean(lstm_test_auprc), np.mean(lstm_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(lstm_test_accuracies), np.std(lstm_test_mcc), np.std(lstm_test_f1), np.std(lstm_test_auprc), np.std(lstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "begUd48HCv5J",
    "outputId": "39401ca5-b9f5-452f-dac3-6a007f1e7723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.828317 & 0.485122 & 0.778385 & 0.765175 & 0.848445\n",
      "0.001144 & 0.003261 & 0.002746 & 0.002648 & 0.001132\n"
     ]
    }
   ],
   "source": [
    "bilstm_train_losses, bilstm_train_accuracies, bilstm_train_mcc, bilstm_train_f1, bilstm_train_auprc, bilstm_train_auroc = [], [], [], [], [], []\n",
    "bilstm_test_losses, bilstm_test_accuracies, bilstm_test_mcc, bilstm_test_f1, bilstm_test_auprc, bilstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        bilstm = BiLSTM(input_size=500).to(device)\n",
    "        bilstm_optimizer = optim.Adam(bilstm.parameters(), lr=0.001815613662380016)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            bilstm.train()\n",
    "            h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              bilstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = bilstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              bilstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              bilstm_optimizer.step()\n",
    "            bilstm.eval()\n",
    "        with torch.no_grad():\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(bilstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          test_outputs = torch.sigmoid(bilstm(X_test, h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          bilstm_test_losses.append(test_loss)\n",
    "          bilstm_test_accuracies.append(test_acc)\n",
    "          bilstm_test_mcc.append(test_mcc)\n",
    "          bilstm_test_f1.append(test_f1)\n",
    "          bilstm_test_auprc.append(test_auprc)\n",
    "          bilstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #bilstm_train_losses.append(train_loss.item())\n",
    "          bilstm_train_accuracies.append(train_acc)\n",
    "          bilstm_train_mcc.append(train_mcc)\n",
    "          bilstm_train_f1.append(train_f1)\n",
    "          bilstm_train_auprc.append(train_auprc)\n",
    "          bilstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(bilstm_test_accuracies), np.mean(bilstm_test_mcc), np.mean(bilstm_test_f1), np.mean(bilstm_test_auprc), np.mean(bilstm_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(bilstm_test_accuracies), np.std(bilstm_test_mcc), np.std(bilstm_test_f1), np.std(bilstm_test_auprc), np.std(bilstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "leffgdVUCv5K",
    "outputId": "9a1fbb2b-4078-4084-f59c-d8533f0f0f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.821685 & 0.453206 & 0.76661 & 0.736864 & 0.831877\n",
      "0.001673 & 0.005678 & 0.004173 & 0.003979 & 0.002615\n"
     ]
    }
   ],
   "source": [
    "convlstm_train_losses, convlstm_train_accuracies, convlstm_train_mcc, convlstm_train_f1, convlstm_train_auprc, convlstm_train_auroc = [], [], [], [], [], []\n",
    "convlstm_test_losses, convlstm_test_accuracies, convlstm_test_mcc, convlstm_test_f1, convlstm_test_auprc, convlstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        convlstm = ConvLSTM(input_size=500).to(device)\n",
    "        convlstm_optimizer = optim.Adam(convlstm.parameters(), lr=0.0015719447694928059)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            convlstm.train()\n",
    "            h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              convlstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = convlstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              convlstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              convlstm_optimizer.step()\n",
    "            convlstm.eval()\n",
    "        with torch.no_grad():\n",
    "          h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(convlstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          test_outputs = torch.sigmoid(convlstm(X_test, h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          convlstm_test_losses.append(test_loss)\n",
    "          convlstm_test_accuracies.append(test_acc)\n",
    "          convlstm_test_mcc.append(test_mcc)\n",
    "          convlstm_test_f1.append(test_f1)\n",
    "          convlstm_test_auprc.append(test_auprc)\n",
    "          convlstm_test_auroc.append(test_auroc)\n",
    "\n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #convlstm_train_losses.append(train_loss.item())\n",
    "          convlstm_train_accuracies.append(train_acc)\n",
    "          convlstm_train_mcc.append(train_mcc)\n",
    "          convlstm_train_f1.append(train_f1)\n",
    "          convlstm_train_auprc.append(train_auprc)\n",
    "          convlstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convlstm_test_accuracies), np.mean(convlstm_test_mcc), np.mean(convlstm_test_f1), np.mean(convlstm_test_auprc), np.mean(convlstm_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(convlstm_test_accuracies), np.std(convlstm_test_mcc), np.std(convlstm_test_f1), np.std(convlstm_test_auprc), np.std(convlstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     Model    |  Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "MLP Metrics:  0.781204 & 0.299198 & 0.678178 & 0.647826 & 0.758894\n",
      "MLP Deviations:  0.001391 & 0.004091 & 0.005104 & 0.002717 & 0.001308\n",
      "ConvNet Metrics:  0.810427 & 0.432274 & 0.749642 & 0.730863 & 0.819879\n",
      "ConvNet Deviations:  0.001211 & 0.003758 & 0.003676 & 0.003138 & 0.001583\n",
      "LSTM Metrics:  0.817079 & 0.443159 & 0.756501 & 0.737908 & 0.83042\n",
      "LSTM Deviations:  0.001435 & 0.003738 & 0.003907 & 0.002523 & 0.001384\n",
      "BiLSTM Metrics:  0.828317 & 0.485122 & 0.778385 & 0.765175 & 0.848445\n",
      "BiLSTM Deviations:  0.001144 & 0.003261 & 0.002746 & 0.002648 & 0.001132\n",
      "ConvLSTM Metrics:  0.821685 & 0.453206 & 0.76661 & 0.736864 & 0.831877\n",
      "ConvLSTM Deviations:  0.001673 & 0.005678 & 0.004173 & 0.003979 & 0.002615\n"
     ]
    }
   ],
   "source": [
    "print('|     Model    |  Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print('MLP Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(mlp_test_accuracies), np.mean(mlp_test_mcc), np.mean(mlp_test_f1), np.mean(mlp_test_auprc), np.mean(mlp_test_auroc))])[:-3])\n",
    "print('MLP Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(mlp_test_accuracies), np.std(mlp_test_mcc), np.std(mlp_test_f1), np.std(mlp_test_auprc), np.std(mlp_test_auroc))])[:-3])\n",
    "print('ConvNet Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convnet_test_accuracies), np.mean(convnet_test_mcc), np.mean(convnet_test_f1), np.mean(convnet_test_auprc), np.mean(convnet_test_auroc))])[:-3])\n",
    "print('ConvNet Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convnet_test_accuracies), np.std(convnet_test_mcc), np.std(convnet_test_f1), np.std(convnet_test_auprc), np.std(convnet_test_auroc))])[:-3])\n",
    "print('LSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(lstm_test_accuracies), np.mean(lstm_test_mcc), np.mean(lstm_test_f1), np.mean(lstm_test_auprc), np.mean(lstm_test_auroc))])[:-3])\n",
    "print('LSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(lstm_test_accuracies), np.std(lstm_test_mcc), np.std(lstm_test_f1), np.std(lstm_test_auprc), np.std(lstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(bilstm_test_accuracies), np.mean(bilstm_test_mcc), np.mean(bilstm_test_f1), np.mean(bilstm_test_auprc), np.mean(bilstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(bilstm_test_accuracies), np.std(bilstm_test_mcc), np.std(bilstm_test_f1), np.std(bilstm_test_auprc), np.std(bilstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convlstm_test_accuracies), np.mean(convlstm_test_mcc), np.mean(convlstm_test_f1), np.mean(convlstm_test_auprc), np.mean(convlstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convlstm_test_accuracies), np.std(convlstm_test_mcc), np.std(convlstm_test_f1), np.std(convlstm_test_auprc), np.std(convlstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW NMF Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36I_lRa0Cv5K"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Nmf\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import pandas as pd\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# generate nmf bag of words model (without SC)\n",
    "\n",
    "fcrnn = pd.read_csv('filtered_cleaned_raw_nursing_notes_processed.csv')\n",
    "fcrnn['processed_text'] = fcrnn['processed_text'].apply(eval)\n",
    "\n",
    "texts_dict = Dictionary(fcrnn['processed_text'])\n",
    "corpus = [texts_dict.doc2bow(text) for text in fcrnn['processed_text']]\n",
    "nmf = Nmf(corpus, num_topics=150, id2word=texts_dict, passes=10)\n",
    "temp_file = datapath(\"nmf_bow_model\")\n",
    "nmf.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_topics = nmf.get_document_topics(corpus, normalize = True)\n",
    "topic_list = []\n",
    "for a in corp_topics:\n",
    "  try:\n",
    "    topic_list.append(list(zip(*a))[0])\n",
    "  except:\n",
    "    #print(list(zip(*a)), a)\n",
    "    topic_list.append([])\n",
    "zs = np.zeros((142110, 150))\n",
    "for i, topic in enumerate(topic_list):\n",
    "    zs[i, topic] = 1\n",
    "with open('nmf_bow_topics.pkl', 'wb') as f:  # 'wb' for writing in binary mode\n",
    "    pickle.dump(zs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "#df_y = pd.read_csv('filtered_cleaned_raw_nursing_notes_processed.csv')\n",
    "#df_y\n",
    "#targets = df_y['ICD9_CODE'].apply(get_multigroup_label).fillna(-1).astype(int)\n",
    "def csv_load_helper(x):\n",
    "  return np.array([float(e) for e in x.replace('[', '').replace(']', '').replace('\\n', '').split()])\n",
    "\n",
    "def y_to_onehot(y):\n",
    "  meep = np.eye(19, dtype='uint8')[y - 1].sum(axis = 0)\n",
    "  return meep\n",
    "\n",
    "with open('nmf_bow_topics.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('y_labels.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparam search with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7NIOiaQxsuc",
    "outputId": "5957c44e-d1f6-4a04-9111-c8793cc9cb5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.668    \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.632    \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.6882   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.6825   \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.7127   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.7222   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.7075   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.6779   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.6719   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.6522   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.5976   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.7266   \u001b[39m | \u001b[35m0.007299 \u001b[39m |\n",
      "| \u001b[35m13       \u001b[39m | \u001b[35m0.7282   \u001b[39m | \u001b[35m0.005712 \u001b[39m |\n",
      "| \u001b[35m14       \u001b[39m | \u001b[35m0.7304   \u001b[39m | \u001b[35m0.006148 \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.6098   \u001b[39m | \u001b[39m0.0853   \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.6957   \u001b[39m | \u001b[39m0.02402  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.6405   \u001b[39m | \u001b[39m0.06253  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.7177   \u001b[39m | \u001b[39m0.01195  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.7291   \u001b[39m | \u001b[39m0.006137 \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.7276   \u001b[39m | \u001b[39m0.00673  \u001b[39m |\n",
      "| \u001b[35m21       \u001b[39m | \u001b[35m0.7313   \u001b[39m | \u001b[35m0.005081 \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.7257   \u001b[39m | \u001b[39m0.008237 \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.7209   \u001b[39m | \u001b[39m0.0102   \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.7198   \u001b[39m | \u001b[39m0.01109  \u001b[39m |\n",
      "| \u001b[35m25       \u001b[39m | \u001b[35m0.733    \u001b[39m | \u001b[35m0.004175 \u001b[39m |\n",
      "| \u001b[35m26       \u001b[39m | \u001b[35m0.7331   \u001b[39m | \u001b[35m0.003378 \u001b[39m |\n",
      "| \u001b[35m27       \u001b[39m | \u001b[35m0.7335   \u001b[39m | \u001b[35m0.002525 \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.7323   \u001b[39m | \u001b[39m0.001686 \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.7144   \u001b[39m | \u001b[39m0.01322  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.71     \u001b[39m | \u001b[39m0.01595  \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.7079   \u001b[39m | \u001b[39m0.0173   \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.7018   \u001b[39m | \u001b[39m0.02009  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.5975   \u001b[39m | \u001b[39m0.09263  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.6986   \u001b[39m | \u001b[39m0.02175  \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.6257   \u001b[39m | \u001b[39m0.07864  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.6923   \u001b[39m | \u001b[39m0.02622  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.6584   \u001b[39m | \u001b[39m0.04777  \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.6344   \u001b[39m | \u001b[39m0.06727  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.6477   \u001b[39m | \u001b[39m0.05819  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.6061   \u001b[39m | \u001b[39m0.08894  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for MLP(\n",
      "  (fc1): Linear(in_features=150, out_features=75, bias=True)\n",
      "  (fc2): Linear(in_features=75, out_features=19, bias=True)\n",
      ") {'target': 0.7334797975848225, 'params': {'lr': 0.00252479821832401}} =================\n",
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.6859   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.6221   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.7336   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.6925   \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.7051   \u001b[39m | \u001b[39m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.7491   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.71     \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.7078   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.6871   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.6797   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.7418   \u001b[39m | \u001b[39m0.009617 \u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.766    \u001b[39m | \u001b[35m0.006451 \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.5942   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.6377   \u001b[39m | \u001b[39m0.08564  \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.6677   \u001b[39m | \u001b[39m0.06174  \u001b[39m |\n",
      "| \u001b[35m16       \u001b[39m | \u001b[35m0.7849   \u001b[39m | \u001b[35m0.003424 \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.7017   \u001b[39m | \u001b[39m0.02415  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.6825   \u001b[39m | \u001b[39m0.04793  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.7837   \u001b[39m | \u001b[39m0.003433 \u001b[39m |\n",
      "| \u001b[35m20       \u001b[39m | \u001b[35m0.7861   \u001b[39m | \u001b[35m0.002646 \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.76     \u001b[39m | \u001b[39m0.007595 \u001b[39m |\n",
      "| \u001b[35m22       \u001b[39m | \u001b[35m0.7862   \u001b[39m | \u001b[35m0.001434 \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.5522   \u001b[39m | \u001b[39m0.0927   \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.7245   \u001b[39m | \u001b[39m0.01201  \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.6183   \u001b[39m | \u001b[39m0.07891  \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.7064   \u001b[39m | \u001b[39m0.02124  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.6013   \u001b[39m | \u001b[39m0.06668  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.6986   \u001b[39m | \u001b[39m0.02697  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.6496   \u001b[39m | \u001b[39m0.05771  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.6976   \u001b[39m | \u001b[39m0.03686  \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.6935   \u001b[39m | \u001b[39m0.04478  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.6792   \u001b[39m | \u001b[39m0.01679  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.7563   \u001b[39m | \u001b[39m0.005413 \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.6536   \u001b[39m | \u001b[39m0.0509   \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.7082   \u001b[39m | \u001b[39m0.03262  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.594    \u001b[39m | \u001b[39m0.08236  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.5522   \u001b[39m | \u001b[39m0.07548  \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.5666   \u001b[39m | \u001b[39m0.08884  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.5598   \u001b[39m | \u001b[39m0.09648  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.7171   \u001b[39m | \u001b[39m0.01332  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvNet(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=19, bias=True)\n",
      ") {'target': 0.786196750550735, 'params': {'lr': 0.0014337669014513697}} =================\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score, roc_auc_score, average_precision_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from bayes_opt import BayesianOptimization\n",
    "import time\n",
    "\n",
    "models = [\n",
    "    MLP(input_size=150),\n",
    "    ConvNet(input_size=150)\n",
    "]\n",
    "lstm_models = [\n",
    "    LSTM(input_size=150),\n",
    "    BiLSTM(input_size=150),\n",
    "    ConvLSTM(input_size=150)\n",
    "]\n",
    "# BATCH_SIZE always 128, train for 8 epochs\n",
    "for model in models:\n",
    "  def eval_model(lr, model=model, X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    device = torch.device(0)\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(X_batch)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "      with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = torch.sigmoid(model(X_test)).cpu().numpy()\n",
    "        #predicted = (outputs > 0.6).float().cpu().numpy()\n",
    "        #print(train_losses)\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        #print(roc)\n",
    "        rocs.append(roc)\n",
    "    #print(rocs)\n",
    "    return np.mean(rocs)\n",
    "  # bayesian optimization on all 5 models....this might take a fews days? :3\n",
    "\n",
    "\n",
    "  bounds = {\n",
    "    'lr': (0.0001, 0.1)\n",
    "  }\n",
    "  optimizer = BayesianOptimization(\n",
    "      f=eval_model,\n",
    "      pbounds=bounds,\n",
    "      verbose=2,  # verbose = 1 prints only when a maximum\n",
    "      # is observed, verbose = 0 is silent\n",
    "      random_state=1,\n",
    "  )\n",
    "  optimizer.maximize(\n",
    "      init_points=10,\n",
    "      n_iter=30\n",
    "  )\n",
    "  print(f'================= BEST MODEL for {model}', optimizer.max, '=================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "HSivLiGZTpsW",
    "outputId": "0b156917-346f-4b7d-e4d8-912389ca6f12",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.7066   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.5177   \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.6995   \u001b[39m | \u001b[39m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.7537   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.674    \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.5179   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.531    \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.7817   \u001b[39m | \u001b[35m0.005218 \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.7754   \u001b[39m | \u001b[39m0.006596 \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08599  \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m0.7826   \u001b[39m | \u001b[35m0.005132 \u001b[39m |\n",
      "| \u001b[35m16       \u001b[39m | \u001b[35m0.7893   \u001b[39m | \u001b[35m0.003252 \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.7888   \u001b[39m | \u001b[39m0.00388  \u001b[39m |\n",
      "| \u001b[35m18       \u001b[39m | \u001b[35m0.7901   \u001b[39m | \u001b[35m0.003503 \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06288  \u001b[39m |\n",
      "| \u001b[35m20       \u001b[39m | \u001b[35m0.7914   \u001b[39m | \u001b[35m0.003509 \u001b[39m |\n",
      "| \u001b[35m21       \u001b[39m | \u001b[35m0.7933   \u001b[39m | \u001b[35m0.00254  \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.7735   \u001b[39m | \u001b[39m0.007427 \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.7635   \u001b[39m | \u001b[39m0.008381 \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.7862   \u001b[39m | \u001b[39m0.001533 \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.748    \u001b[39m | \u001b[39m0.01037  \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.6538   \u001b[39m | \u001b[39m0.02375  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.7379   \u001b[39m | \u001b[39m0.01141  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07899  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.5002   \u001b[39m | \u001b[39m0.09291  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0481   \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.7928   \u001b[39m | \u001b[39m0.002099 \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.6603   \u001b[39m | \u001b[39m0.02123  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.7915   \u001b[39m | \u001b[39m0.002343 \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.5156   \u001b[39m | \u001b[39m0.06749  \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.5001   \u001b[39m | \u001b[39m0.0584   \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.5809   \u001b[39m | \u001b[39m0.02663  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.6831   \u001b[39m | \u001b[39m0.01663  \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.7156   \u001b[39m | \u001b[39m0.01304  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09643  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0825   \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for LSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc2): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.7932830011451333, 'params': {'lr': 0.0025401209929924555}} =================\n",
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.7206   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01476  \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m0.8036   \u001b[39m | \u001b[35m0.001508 \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.6555   \u001b[39m | \u001b[39m0.003691 \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.8013   \u001b[39m | \u001b[39m0.002147 \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.1      \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m0.8041   \u001b[39m | \u001b[35m0.001572 \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08603  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06299  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07904  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09299  \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0478   \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0245   \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05845  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06752  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.8041   \u001b[39m | \u001b[39m0.001868 \u001b[39m |\n",
      "| \u001b[35m25       \u001b[39m | \u001b[35m0.8041   \u001b[39m | \u001b[35m0.001706 \u001b[39m |\n",
      "| \u001b[35m26       \u001b[39m | \u001b[35m0.8069   \u001b[39m | \u001b[35m0.001677 \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.7973   \u001b[39m | \u001b[39m0.0008428\u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.7863   \u001b[39m | \u001b[39m0.002831 \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.569    \u001b[39m | \u001b[39m0.005003 \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09649  \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08254  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07555  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08952  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05086  \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.8021   \u001b[39m | \u001b[39m0.001133 \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04478  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0274   \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.02159  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01203  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.03718  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc3): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.8069193078130159, 'params': {'lr': 0.0016770902937105622}} =================\n"
     ]
    }
   ],
   "source": [
    "lstm_models = [\n",
    "    LSTM(input_size=150),\n",
    "    ConvLSTM(input_size=150)\n",
    "]\n",
    "# BATCH_SIZE always 128, train for 8 epochs\n",
    "for model in lstm_models:\n",
    "  def eval_model(lr, model=model, X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    device = torch.device(\"cuda\")\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs, h_o, c_o = model(X_batch, h_o, c_o)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "      with torch.no_grad():\n",
    "        model.eval()\n",
    "        h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "        outputs = torch.sigmoid(model(X_test, h_o, c_o)[0]).cpu().numpy()\n",
    "        #predicted = (outputs > 0.5).float().cpu().numpy()\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        rocs.append(roc)\n",
    "\n",
    "    return np.mean(rocs)\n",
    "\n",
    "\n",
    "  bounds = {\n",
    "    'lr': (0.0001, 0.1)\n",
    "  }\n",
    "  optimizer = BayesianOptimization(\n",
    "      f=eval_model,\n",
    "      pbounds=bounds,\n",
    "      verbose=2,  # verbose = 1 prints only when a maximum\n",
    "      # is observed, verbose = 0 is silent\n",
    "      random_state=1,\n",
    "  )\n",
    "  optimizer.maximize(\n",
    "      init_points=10,\n",
    "      n_iter=30\n",
    "  )\n",
    "  print(f'================= BEST MODEL for {model}', optimizer.max, '=================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WOQsv5u0Cv5I",
    "outputId": "ae6690b0-5612-4103-ab3a-ff782a46f71a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.4997   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.5001   \u001b[39m | \u001b[35m0.07206  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.7138   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.588    \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.662    \u001b[39m | \u001b[39m0.01476  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.7412   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.6457   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.539    \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.5489   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.5097   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m0.7505   \u001b[39m | \u001b[35m0.009046 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.8021   \u001b[39m | \u001b[35m0.004703 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08599  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.7909   \u001b[39m | \u001b[39m0.006231 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m16       \u001b[39m | \u001b[35m0.8105   \u001b[39m | \u001b[35m0.003081 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.5145   \u001b[39m | \u001b[39m0.0628   \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.8095   \u001b[39m | \u001b[39m0.003644 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.8084   \u001b[39m | \u001b[39m0.003654 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.7055   \u001b[39m | \u001b[39m0.01199  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.6091   \u001b[39m | \u001b[39m0.02357  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09298  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.7823   \u001b[39m | \u001b[39m0.007504 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m24       \u001b[39m | \u001b[35m0.8116   \u001b[39m | \u001b[35m0.002696 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m25       \u001b[39m | \u001b[35m0.8145   \u001b[39m | \u001b[35m0.002254 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07905  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m27       \u001b[39m | \u001b[35m0.8147   \u001b[39m | \u001b[35m0.00184  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m28       \u001b[39m | \u001b[35m0.8151   \u001b[39m | \u001b[35m0.002024 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.8143   \u001b[39m | \u001b[39m0.00202  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.5001   \u001b[39m | \u001b[39m0.04829  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.8141   \u001b[39m | \u001b[39m0.001995 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06741  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05839  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.6221   \u001b[39m | \u001b[39m0.02686  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.4999   \u001b[39m | \u001b[39m0.09648  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.813    \u001b[39m | \u001b[39m0.001633 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.815    \u001b[39m | \u001b[39m0.002135 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.8148   \u001b[39m | \u001b[39m0.002154 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.814    \u001b[39m | \u001b[39m0.00215  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m40       \u001b[39m | \u001b[35m0.8153   \u001b[39m | \u001b[35m0.001914 \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc3): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.8152871592327673, 'params': {'lr': 0.0019144172571157274}} =================\n"
     ]
    }
   ],
   "source": [
    "def eval_model(lr, model=BiLSTM(input_size=150), X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    print(model)\n",
    "    device = torch.device(\"cuda\")\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs, h_o, c_o = model(X_batch, h_o, c_o)\n",
    "          #print(outputs, y_batch)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "        outputs = torch.sigmoid(model(X_test, h_o, c_o)[0]).cpu().numpy()\n",
    "        #predicted = (outputs > 0.5).float().cpu().numpy()\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        rocs.append(roc)\n",
    "\n",
    "    return np.mean(rocs)\n",
    "\n",
    "\n",
    "bounds = {\n",
    "'lr': (0.0001, 0.1)\n",
    "}\n",
    "optimizer = BayesianOptimization(\n",
    "  f=eval_model,\n",
    "  pbounds=bounds,\n",
    "  verbose=2,  # verbose = 1 prints only when a maximum\n",
    "  # is observed, verbose = 0 is silent\n",
    "  random_state=1,\n",
    ")\n",
    "optimizer.maximize(\n",
    "  init_points=10,\n",
    "  n_iter=30\n",
    ")\n",
    "print(f'================= BEST MODEL for {model}', optimizer.max, '=================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation with optimial hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "splitter = ShuffleSplit(n_splits=5, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "wnVxVni0Cv5J",
    "outputId": "475feeaf-c4fa-43c6-d324-508f02108ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.773285 & 0.239727 & 0.655454 & 0.614666 & 0.733751\n",
      "0.001422 & 0.004171 & 0.005545 & 0.001687 & 0.00142\n"
     ]
    }
   ],
   "source": [
    "#train mlp\n",
    "mlp_train_losses, mlp_train_accuracies, mlp_train_mcc, mlp_train_f1, mlp_train_auprc, mlp_train_auroc = [], [], [], [], [], []\n",
    "mlp_test_losses, mlp_test_accuracies, mlp_test_mcc, mlp_test_f1, mlp_test_auprc, mlp_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        mlp = MLP(input_size=150).to(device)\n",
    "        mlp_optimizer = optim.Adam(mlp.parameters(), lr=0.00252479821832401)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            mlp.train()\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              mlp_optimizer.zero_grad()\n",
    "              outputs = mlp(X_batch)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              mlp_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              mlp_optimizer.step()\n",
    "        with torch.no_grad():\n",
    "          mlp.eval()\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(mlp(X_train))\n",
    "          test_outputs = torch.sigmoid(mlp(X_test))\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          mlp_test_losses.append(test_loss)\n",
    "          mlp_test_accuracies.append(test_acc)\n",
    "          mlp_test_mcc.append(test_mcc)\n",
    "          mlp_test_f1.append(test_f1)\n",
    "          mlp_test_auprc.append(test_auprc)\n",
    "          mlp_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #mlp_train_losses.append(train_loss.item())\n",
    "          mlp_train_accuracies.append(train_acc)\n",
    "          mlp_train_mcc.append(train_mcc)\n",
    "          mlp_train_f1.append(train_f1)\n",
    "          mlp_train_auprc.append(train_auprc)\n",
    "          mlp_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(mlp_test_accuracies), np.mean(mlp_test_mcc), np.mean(mlp_test_f1), np.mean(mlp_test_auprc), np.mean(mlp_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(mlp_test_accuracies), np.std(mlp_test_mcc), np.std(mlp_test_f1), np.std(mlp_test_auprc), np.std(mlp_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "u6cXl1fOCv5J",
    "outputId": "f35cfac6-a7dc-400f-b466-b0c82de27218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.792179 & 0.359717 & 0.710326 & 0.688781 & 0.786973\n",
      "0.00115 & 0.004965 & 0.004086 & 0.003821 & 0.001917\n"
     ]
    }
   ],
   "source": [
    "convnet_train_losses, convnet_train_accuracies, convnet_train_mcc, convnet_train_f1, convnet_train_auprc, convnet_train_auroc = [], [], [], [], [], []\n",
    "convnet_test_losses, convnet_test_accuracies, convnet_test_mcc, convnet_test_f1, convnet_test_auprc, convnet_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        convnet = ConvNet(input_size=150).to(device)\n",
    "        convnet_optimizer = optim.Adam(convnet.parameters(), lr=0.0014337669014513697)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            convnet.train()\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              convnet_optimizer.zero_grad()\n",
    "              outputs = convnet(X_batch)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              convnet_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              convnet_optimizer.step()\n",
    "        convnet.eval()\n",
    "        with torch.no_grad():\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(convnet(X_train))\n",
    "          test_outputs = torch.sigmoid(convnet(X_test))\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          convnet_test_losses.append(test_loss)\n",
    "          convnet_test_accuracies.append(test_acc)\n",
    "          convnet_test_mcc.append(test_mcc)\n",
    "          convnet_test_f1.append(test_f1)\n",
    "          convnet_test_auprc.append(test_auprc)\n",
    "          convnet_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #convnet_train_losses.append(train_loss.item())\n",
    "          convnet_train_accuracies.append(train_acc)\n",
    "          convnet_train_mcc.append(train_mcc)\n",
    "          convnet_train_f1.append(train_f1)\n",
    "          convnet_train_auprc.append(train_auprc)\n",
    "          convnet_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convnet_test_accuracies), np.mean(convnet_test_mcc), np.mean(convnet_test_f1), np.mean(convnet_test_auprc), np.mean(convnet_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(convnet_test_accuracies), np.std(convnet_test_mcc), np.std(convnet_test_f1), np.std(convnet_test_auprc), np.std(convnet_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "jI38lwtCCv5J",
    "outputId": "5ea32f23-2d4c-4717-af89-6ca0a3caf8e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.796317 & 0.360581 & 0.713759 & 0.687668 & 0.792528\n",
      "LSTM Deviations:  0.001124 & 0.00441 & 0.004007 & 0.002619 & 0.00151\n"
     ]
    }
   ],
   "source": [
    "lstm_train_losses, lstm_train_accuracies, lstm_train_mcc, lstm_train_f1, lstm_train_auprc, lstm_train_auroc = [], [], [], [], [], []\n",
    "lstm_test_losses, lstm_test_accuracies, lstm_test_mcc, lstm_test_f1, lstm_test_auprc, lstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        #print(len(train))\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        lstm = LSTM(input_size=150).to(device)\n",
    "        lstm_optimizer = optim.Adam(lstm.parameters(), lr=0.0025401209929924555)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            lstm.train()\n",
    "            h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              lstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = lstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              lstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              lstm_optimizer.step()\n",
    "            lstm.eval()\n",
    "        with torch.no_grad():\n",
    "          h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(lstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          test_outputs = torch.sigmoid(lstm(X_test, h_o, c_o)[0])\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          lstm_test_losses.append(test_loss)\n",
    "          lstm_test_accuracies.append(test_acc)\n",
    "          lstm_test_mcc.append(test_mcc)\n",
    "          lstm_test_f1.append(test_f1)\n",
    "          lstm_test_auprc.append(test_auprc)\n",
    "          lstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #lstm_train_losses.append(train_loss.item())\n",
    "          lstm_train_accuracies.append(train_acc)\n",
    "          lstm_train_mcc.append(train_mcc)\n",
    "          lstm_train_f1.append(train_f1)\n",
    "          lstm_train_auprc.append(train_auprc)\n",
    "          lstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(lstm_test_accuracies), np.mean(lstm_test_mcc), np.mean(lstm_test_f1), np.mean(lstm_test_auprc), np.mean(lstm_test_auroc))])[:-3])\n",
    "print('LSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(lstm_test_accuracies), np.std(lstm_test_mcc), np.std(lstm_test_f1), np.std(lstm_test_auprc), np.std(lstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "begUd48HCv5J",
    "outputId": "39401ca5-b9f5-452f-dac3-6a007f1e7723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.807793 & 0.41189 & 0.740718 & 0.720339 & 0.814366\n",
      "BiLSTM Deviations:  0.001248 & 0.004087 & 0.003319 & 0.003334 & 0.001734\n"
     ]
    }
   ],
   "source": [
    "bilstm_train_losses, bilstm_train_accuracies, bilstm_train_mcc, bilstm_train_f1, bilstm_train_auprc, bilstm_train_auroc = [], [], [], [], [], []\n",
    "bilstm_test_losses, bilstm_test_accuracies, bilstm_test_mcc, bilstm_test_f1, bilstm_test_auprc, bilstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        bilstm = BiLSTM(input_size=150).to(device)\n",
    "        bilstm_optimizer = optim.Adam(bilstm.parameters(), lr=0.001914)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            bilstm.train()\n",
    "            h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              bilstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = bilstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              bilstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              bilstm_optimizer.step()\n",
    "            bilstm.eval()\n",
    "        with torch.no_grad():\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(bilstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          test_outputs = torch.sigmoid(bilstm(X_test, h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          bilstm_test_losses.append(test_loss)\n",
    "          bilstm_test_accuracies.append(test_acc)\n",
    "          bilstm_test_mcc.append(test_mcc)\n",
    "          bilstm_test_f1.append(test_f1)\n",
    "          bilstm_test_auprc.append(test_auprc)\n",
    "          bilstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #bilstm_train_losses.append(train_loss.item())\n",
    "          bilstm_train_accuracies.append(train_acc)\n",
    "          bilstm_train_mcc.append(train_mcc)\n",
    "          bilstm_train_f1.append(train_f1)\n",
    "          bilstm_train_auprc.append(train_auprc)\n",
    "          bilstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(bilstm_test_accuracies), np.mean(bilstm_test_mcc), np.mean(bilstm_test_f1), np.mean(bilstm_test_auprc), np.mean(bilstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(bilstm_test_accuracies), np.std(bilstm_test_mcc), np.std(bilstm_test_f1), np.std(bilstm_test_auprc), np.std(bilstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "leffgdVUCv5K",
    "outputId": "9a1fbb2b-4078-4084-f59c-d8533f0f0f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.804692 & 0.394203 & 0.735987 & 0.703019 & 0.80426\n",
      "ConvLSTM Deviations:  0.002288 & 0.007253 & 0.006224 & 0.004799 & 0.003663\n"
     ]
    }
   ],
   "source": [
    "convlstm_train_losses, convlstm_train_accuracies, convlstm_train_mcc, convlstm_train_f1, convlstm_train_auprc, convlstm_train_auroc = [], [], [], [], [], []\n",
    "convlstm_test_losses, convlstm_test_accuracies, convlstm_test_mcc, convlstm_test_f1, convlstm_test_auprc, convlstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        convlstm = ConvLSTM(input_size=150).to(device)\n",
    "        convlstm_optimizer = optim.Adam(convlstm.parameters(), lr=0.0016770902937105622)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            convlstm.train()\n",
    "            h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              convlstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = convlstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              convlstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              convlstm_optimizer.step()\n",
    "            convlstm.eval()\n",
    "        with torch.no_grad():\n",
    "          h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(convlstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          test_outputs = torch.sigmoid(convlstm(X_test, h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          convlstm_test_losses.append(test_loss)\n",
    "          convlstm_test_accuracies.append(test_acc)\n",
    "          convlstm_test_mcc.append(test_mcc)\n",
    "          convlstm_test_f1.append(test_f1)\n",
    "          convlstm_test_auprc.append(test_auprc)\n",
    "          convlstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #convlstm_train_losses.append(train_loss.item())\n",
    "          convlstm_train_accuracies.append(train_acc)\n",
    "          convlstm_train_mcc.append(train_mcc)\n",
    "          convlstm_train_f1.append(train_f1)\n",
    "          convlstm_train_auprc.append(train_auprc)\n",
    "          convlstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convlstm_test_accuracies), np.mean(convlstm_test_mcc), np.mean(convlstm_test_f1), np.mean(convlstm_test_auprc), np.mean(convlstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convlstm_test_accuracies), np.std(convlstm_test_mcc), np.std(convlstm_test_f1), np.std(convlstm_test_auprc), np.std(convlstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     Model    |  Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "MLP Metrics:  0.773285 & 0.239727 & 0.655454 & 0.614666 & 0.733751\n",
      "MLP Deviations:  0.001422 & 0.004171 & 0.005545 & 0.001687 & 0.00142\n",
      "ConvNet Metrics:  0.792179 & 0.359717 & 0.710326 & 0.688781 & 0.786973\n",
      "ConvNet Deviations:  0.00115 & 0.004965 & 0.004086 & 0.003821 & 0.001917\n",
      "LSTM Metrics:  0.796317 & 0.360581 & 0.713759 & 0.687668 & 0.792528\n",
      "LSTM Deviations:  0.001124 & 0.00441 & 0.004007 & 0.002619 & 0.00151\n",
      "BiLSTM Metrics:  0.807793 & 0.41189 & 0.740718 & 0.720339 & 0.814366\n",
      "BiLSTM Deviations:  0.001248 & 0.004087 & 0.003319 & 0.003334 & 0.001734\n",
      "ConvLSTM Metrics:  0.804692 & 0.394203 & 0.735987 & 0.703019 & 0.80426\n",
      "ConvLSTM Deviations:  0.002288 & 0.007253 & 0.006224 & 0.004799 & 0.003663\n"
     ]
    }
   ],
   "source": [
    "print('|     Model    |  Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print('MLP Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(mlp_test_accuracies), np.mean(mlp_test_mcc), np.mean(mlp_test_f1), np.mean(mlp_test_auprc), np.mean(mlp_test_auroc))])[:-3])\n",
    "print('MLP Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(mlp_test_accuracies), np.std(mlp_test_mcc), np.std(mlp_test_f1), np.std(mlp_test_auprc), np.std(mlp_test_auroc))])[:-3])\n",
    "print('ConvNet Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convnet_test_accuracies), np.mean(convnet_test_mcc), np.mean(convnet_test_f1), np.mean(convnet_test_auprc), np.mean(convnet_test_auroc))])[:-3])\n",
    "print('ConvNet Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convnet_test_accuracies), np.std(convnet_test_mcc), np.std(convnet_test_f1), np.std(convnet_test_auprc), np.std(convnet_test_auroc))])[:-3])\n",
    "print('LSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(lstm_test_accuracies), np.mean(lstm_test_mcc), np.mean(lstm_test_f1), np.mean(lstm_test_auprc), np.mean(lstm_test_auroc))])[:-3])\n",
    "print('LSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(lstm_test_accuracies), np.std(lstm_test_mcc), np.std(lstm_test_f1), np.std(lstm_test_auprc), np.std(lstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(bilstm_test_accuracies), np.mean(bilstm_test_mcc), np.mean(bilstm_test_f1), np.mean(bilstm_test_auprc), np.mean(bilstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(bilstm_test_accuracies), np.std(bilstm_test_mcc), np.std(bilstm_test_f1), np.std(bilstm_test_auprc), np.std(bilstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convlstm_test_accuracies), np.mean(convlstm_test_mcc), np.mean(convlstm_test_f1), np.mean(convlstm_test_auprc), np.mean(convlstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convlstm_test_accuracies), np.std(convlstm_test_mcc), np.std(convlstm_test_f1), np.std(convlstm_test_auprc), np.std(convlstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TW NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "36I_lRa0Cv5K"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Nmf, TfidfModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import pandas as pd\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# generate nmf bag of words model (without SC)\n",
    "\n",
    "fcrnn = pd.read_csv('filtered_cleaned_raw_nursing_notes_processed.csv')\n",
    "fcrnn['processed_text'] = fcrnn['processed_text'].apply(eval)\n",
    "\n",
    "texts_dict = Dictionary(fcrnn['processed_text'])\n",
    "corpus = [texts_dict.doc2bow(text) for text in fcrnn['processed_text']]\n",
    "model = TfidfModel(corpus, smartirs='ltn')\n",
    "corpus = model[corpus]\n",
    "nmf = Nmf(corpus, num_topics=150, id2word=texts_dict, passes=10)\n",
    "temp_file = datapath(\"nmf_TW_model\")\n",
    "nmf.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_topics = nmf.get_document_topics(corpus, normalize = True)\n",
    "zs = gensim.matutils.corpus2csc(corp_topics).todense()\n",
    "with open('nmf_tw_topics.pkl', 'wb') as f:  # 'wb' for writing in binary mode\n",
    "    pickle.dump(zs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "#df_y = pd.read_csv('filtered_cleaned_raw_nursing_notes_processed.csv')\n",
    "#df_y\n",
    "#targets = df_y['ICD9_CODE'].apply(get_multigroup_label).fillna(-1).astype(int)\n",
    "def csv_load_helper(x):\n",
    "  return np.array([float(e) for e in x.replace('[', '').replace(']', '').replace('\\n', '').split()])\n",
    "\n",
    "def y_to_onehot(y):\n",
    "  meep = np.eye(19, dtype='uint8')[y - 1].sum(axis = 0)\n",
    "  return meep\n",
    "\n",
    "with open('nmf_tw_topics.pkl', 'rb') as f:\n",
    "    X = pickle.load(f).T\n",
    "with open('y_labels.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Hyperparam Search with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7NIOiaQxsuc",
    "outputId": "5957c44e-d1f6-4a04-9111-c8793cc9cb5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.7663   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.7493   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.6844   \u001b[39m | \u001b[39m0.0001114\u001b[39m |\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.7705   \u001b[39m | \u001b[35m0.0303   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.7778   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.7781   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.7756   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.7712   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.7659   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.7608   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.7339   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.7414   \u001b[39m | \u001b[39m0.08491  \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.7775   \u001b[39m | \u001b[39m0.009033 \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.7558   \u001b[39m | \u001b[39m0.06252  \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.7767   \u001b[39m | \u001b[39m0.02427  \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.7644   \u001b[39m | \u001b[39m0.04802  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.7769   \u001b[39m | \u001b[39m0.02174  \u001b[39m |\n",
      "| \u001b[35m18       \u001b[39m | \u001b[35m0.7799   \u001b[39m | \u001b[35m0.01223  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.7791   \u001b[39m | \u001b[39m0.01225  \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.7781   \u001b[39m | \u001b[39m0.01118  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.7747   \u001b[39m | \u001b[39m0.02302  \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.776    \u001b[39m | \u001b[39m0.02022  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.7793   \u001b[39m | \u001b[39m0.01618  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.7779   \u001b[39m | \u001b[39m0.0174   \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.7734   \u001b[39m | \u001b[39m0.02593  \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.7731   \u001b[39m | \u001b[39m0.02794  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.7714   \u001b[39m | \u001b[39m0.03254  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.7761   \u001b[39m | \u001b[39m0.007063 \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.7752   \u001b[39m | \u001b[39m0.005446 \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.7711   \u001b[39m | \u001b[39m0.003846 \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.769    \u001b[39m | \u001b[39m0.03665  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.7394   \u001b[39m | \u001b[39m0.09239  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.7466   \u001b[39m | \u001b[39m0.0784   \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.764    \u001b[39m | \u001b[39m0.04459  \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.7514   \u001b[39m | \u001b[39m0.06717  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.7569   \u001b[39m | \u001b[39m0.05804  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.7603   \u001b[39m | \u001b[39m0.05072  \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.7393   \u001b[39m | \u001b[39m0.08865  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.7347   \u001b[39m | \u001b[39m0.09613  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.7472   \u001b[39m | \u001b[39m0.07518  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for MLP(\n",
      "  (fc1): Linear(in_features=150, out_features=75, bias=True)\n",
      "  (fc2): Linear(in_features=75, out_features=19, bias=True)\n",
      ") {'target': 0.7799088530617988, 'params': {'lr': 0.012234847929677781}} =================\n",
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.7717   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.7092   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.7475   \u001b[39m | \u001b[39m0.0001114\u001b[39m |\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.7827   \u001b[39m | \u001b[35m0.0303   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.8088   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.8402   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.8082   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.7952   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.7946   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.7709   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.8366   \u001b[39m | \u001b[39m0.009334 \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.8265   \u001b[39m | \u001b[39m0.009187 \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.817    \u001b[39m | \u001b[39m0.01451  \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.8124   \u001b[39m | \u001b[39m0.01429  \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.8211   \u001b[39m | \u001b[39m0.01402  \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.8241   \u001b[39m | \u001b[39m0.01381  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.8286   \u001b[39m | \u001b[39m0.01359  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.8208   \u001b[39m | \u001b[39m0.01339  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.821    \u001b[39m | \u001b[39m0.01312  \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.8126   \u001b[39m | \u001b[39m0.01287  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.8195   \u001b[39m | \u001b[39m0.01255  \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.8365   \u001b[39m | \u001b[39m0.01229  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.8215   \u001b[39m | \u001b[39m0.01209  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.8309   \u001b[39m | \u001b[39m0.0118   \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.8283   \u001b[39m | \u001b[39m0.01158  \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.8279   \u001b[39m | \u001b[39m0.01133  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.8263   \u001b[39m | \u001b[39m0.01109  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.8246   \u001b[39m | \u001b[39m0.01083  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.8089   \u001b[39m | \u001b[39m0.01056  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.7533   \u001b[39m | \u001b[39m0.07883  \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.8097   \u001b[39m | \u001b[39m0.01813  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.8197   \u001b[39m | \u001b[39m0.01764  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.7941   \u001b[39m | \u001b[39m0.01735  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.8232   \u001b[39m | \u001b[39m0.01926  \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.8132   \u001b[39m | \u001b[39m0.01952  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.8073   \u001b[39m | \u001b[39m0.01904  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.8131   \u001b[39m | \u001b[39m0.01992  \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.8077   \u001b[39m | \u001b[39m0.02028  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.7991   \u001b[39m | \u001b[39m0.01786  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.8323   \u001b[39m | \u001b[39m0.005293 \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvNet(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=19, bias=True)\n",
      ") {'target': 0.8401864658702396, 'params': {'lr': 0.0093246256174029}} =================\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score, roc_auc_score, average_precision_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from bayes_opt import BayesianOptimization\n",
    "import time\n",
    "\n",
    "models = [\n",
    "    MLP(input_size=150),\n",
    "    ConvNet(input_size=150)\n",
    "]\n",
    "lstm_models = [\n",
    "    LSTM(input_size=150),\n",
    "    BiLSTM(input_size=150),\n",
    "    ConvLSTM(input_size=150)\n",
    "]\n",
    "# BATCH_SIZE always 128, train for 8 epochs\n",
    "for model in models:\n",
    "  def eval_model(lr, model=model, X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    device = torch.device(0)\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(X_batch)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "      with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = torch.sigmoid(model(X_test)).cpu().numpy()\n",
    "        #predicted = (outputs > 0.6).float().cpu().numpy()\n",
    "        #print(train_losses)\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        #print(roc)\n",
    "        rocs.append(roc)\n",
    "    #print(rocs)\n",
    "    return np.mean(rocs)\n",
    "  # bayesian optimization on all 5 models....this might take a fews days? :3\n",
    "\n",
    "\n",
    "  bounds = {\n",
    "    'lr': (0.0001, 0.1)\n",
    "  }\n",
    "  optimizer = BayesianOptimization(\n",
    "      f=eval_model,\n",
    "      pbounds=bounds,\n",
    "      verbose=2,  # verbose = 1 prints only when a maximum\n",
    "      # is observed, verbose = 0 is silent\n",
    "      random_state=1,\n",
    "  )\n",
    "  optimizer.maximize(\n",
    "      init_points=10,\n",
    "      n_iter=30\n",
    "  )\n",
    "  print(f'================= BEST MODEL for {model}', optimizer.max, '=================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "HSivLiGZTpsW",
    "outputId": "0b156917-346f-4b7d-e4d8-912389ca6f12",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.7427   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.7386   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.7191   \u001b[39m | \u001b[39m0.0001114\u001b[39m |\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.7881   \u001b[39m | \u001b[35m0.0303   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.8501   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.8607   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.826    \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.762    \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.7796   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.7427   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m0.8608   \u001b[39m | \u001b[35m0.009046 \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.7162   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.7008   \u001b[39m | \u001b[39m0.0855   \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.8023   \u001b[39m | \u001b[39m0.02435  \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.7309   \u001b[39m | \u001b[39m0.06284  \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.8545   \u001b[39m | \u001b[39m0.01229  \u001b[39m |\n",
      "| \u001b[35m17       \u001b[39m | \u001b[35m0.8654   \u001b[39m | \u001b[35m0.006487 \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.8577   \u001b[39m | \u001b[39m0.00522  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.8632   \u001b[39m | \u001b[39m0.006498 \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.8612   \u001b[39m | \u001b[39m0.00585  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.8534   \u001b[39m | \u001b[39m0.01311  \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.8525   \u001b[39m | \u001b[39m0.01395  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.8578   \u001b[39m | \u001b[39m0.004317 \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.8569   \u001b[39m | \u001b[39m0.01119  \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.8561   \u001b[39m | \u001b[39m0.01031  \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.8476   \u001b[39m | \u001b[39m0.003228 \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.8479   \u001b[39m | \u001b[39m0.01598  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.8377   \u001b[39m | \u001b[39m0.01714  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.83     \u001b[39m | \u001b[39m0.02031  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.8195   \u001b[39m | \u001b[39m0.02175  \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.8317   \u001b[39m | \u001b[39m0.002189 \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.6983   \u001b[39m | \u001b[39m0.09275  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.7286   \u001b[39m | \u001b[39m0.0787   \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.7583   \u001b[39m | \u001b[39m0.04787  \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.7413   \u001b[39m | \u001b[39m0.06747  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.7481   \u001b[39m | \u001b[39m0.05835  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.796    \u001b[39m | \u001b[39m0.02692  \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.7098   \u001b[39m | \u001b[39m0.09641  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.7109   \u001b[39m | \u001b[39m0.08912  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.7303   \u001b[39m | \u001b[39m0.07536  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for LSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc2): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.865443033797559, 'params': {'lr': 0.006486677380339516}} =================\n",
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.7171   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01476  \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m0.8329   \u001b[39m | \u001b[35m0.001508 \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.759    \u001b[39m | \u001b[39m0.00329  \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[35m14       \u001b[39m | \u001b[35m0.8417   \u001b[39m | \u001b[35m0.00216  \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.8264   \u001b[39m | \u001b[39m0.002091 \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.8029   \u001b[39m | \u001b[39m0.0008906\u001b[39m |\n",
      "| \u001b[35m17       \u001b[39m | \u001b[35m0.8444   \u001b[39m | \u001b[35m0.002697 \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.5654   \u001b[39m | \u001b[39m0.005006 \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.086    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06299  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.5001   \u001b[39m | \u001b[39m0.09299  \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07903  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04783  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0245   \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06752  \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05845  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09649  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08949  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08251  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07554  \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05088  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04478  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.8417   \u001b[39m | \u001b[39m0.002444 \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0274   \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.82     \u001b[39m | \u001b[39m0.001222 \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.02161  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01206  \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.03719  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.5632   \u001b[39m | \u001b[39m0.006956 \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0698   \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc3): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.8444472959566433, 'params': {'lr': 0.002697021837162594}} =================\n"
     ]
    }
   ],
   "source": [
    "lstm_models = [\n",
    "    LSTM(input_size=150),\n",
    "    ConvLSTM(input_size=150)\n",
    "]\n",
    "# BATCH_SIZE always 128, train for 8 epochs\n",
    "for model in lstm_models:\n",
    "  def eval_model(lr, model=model, X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    device = torch.device(\"cuda\")\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs, h_o, c_o = model(X_batch, h_o, c_o)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "      with torch.no_grad():\n",
    "        model.eval()\n",
    "        h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "        outputs = torch.sigmoid(model(X_test, h_o, c_o)[0]).cpu().numpy()\n",
    "        #predicted = (outputs > 0.5).float().cpu().numpy()\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        rocs.append(roc)\n",
    "\n",
    "    return np.mean(rocs)\n",
    "\n",
    "\n",
    "  bounds = {\n",
    "    'lr': (0.0001, 0.1)\n",
    "  }\n",
    "  optimizer = BayesianOptimization(\n",
    "      f=eval_model,\n",
    "      pbounds=bounds,\n",
    "      verbose=2,  # verbose = 1 prints only when a maximum\n",
    "      # is observed, verbose = 0 is silent\n",
    "      random_state=1,\n",
    "  )\n",
    "  optimizer.maximize(\n",
    "      init_points=10,\n",
    "      n_iter=30\n",
    "  )\n",
    "  print(f'================= BEST MODEL for {model}', optimizer.max, '=================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "WOQsv5u0Cv5I",
    "outputId": "ae6690b0-5612-4103-ab3a-ff782a46f71a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.7757   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.7263   \u001b[39m | \u001b[39m0.0001114\u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.7946   \u001b[39m | \u001b[35m0.0303   \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.8653   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.8856   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.8342   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.7898   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.7635   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.755    \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.8831   \u001b[39m | \u001b[39m0.009587 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.7168   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.7315   \u001b[39m | \u001b[39m0.08546  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m14       \u001b[39m | \u001b[35m0.8894   \u001b[39m | \u001b[35m0.006382 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.7455   \u001b[39m | \u001b[39m0.06288  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m16       \u001b[39m | \u001b[35m0.8895   \u001b[39m | \u001b[35m0.007559 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.8299   \u001b[39m | \u001b[39m0.02417  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.7566   \u001b[39m | \u001b[39m0.0477   \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.8895   \u001b[39m | \u001b[39m0.006371 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.8864   \u001b[39m | \u001b[39m0.005062 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.888    \u001b[39m | \u001b[39m0.008312 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.7273   \u001b[39m | \u001b[39m0.09252  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.7314   \u001b[39m | \u001b[39m0.07864  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.8732   \u001b[39m | \u001b[39m0.01275  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.8895   \u001b[39m | \u001b[39m0.005727 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m26       \u001b[39m | \u001b[35m0.8901   \u001b[39m | \u001b[35m0.007036 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.8892   \u001b[39m | \u001b[39m0.007212 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.8161   \u001b[39m | \u001b[39m0.02149  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.806    \u001b[39m | \u001b[39m0.02699  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.7583   \u001b[39m | \u001b[39m0.0583   \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.7323   \u001b[39m | \u001b[39m0.06747  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.8416   \u001b[39m | \u001b[39m0.01656  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.8779   \u001b[39m | \u001b[39m0.01136  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.8891   \u001b[39m | \u001b[39m0.00674  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.8726   \u001b[39m | \u001b[39m0.003393 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.7663   \u001b[39m | \u001b[39m0.04463  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.8793   \u001b[39m | \u001b[39m0.00422  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.7247   \u001b[39m | \u001b[39m0.09617  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.7575   \u001b[39m | \u001b[39m0.05084  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.8887   \u001b[39m | \u001b[39m0.007902 \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvLSTM(\n",
      "  (fc1): Linear(in_features=150, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc3): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.8900552066760412, 'params': {'lr': 0.007036460928329656}} =================\n"
     ]
    }
   ],
   "source": [
    "def eval_model(lr, model=BiLSTM(input_size=150), X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    print(model)\n",
    "    device = torch.device(\"cuda\")\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs, h_o, c_o = model(X_batch, h_o, c_o)\n",
    "          #print(outputs, y_batch)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "        outputs = torch.sigmoid(model(X_test, h_o, c_o)[0]).cpu().numpy()\n",
    "        #predicted = (outputs > 0.5).float().cpu().numpy()\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        rocs.append(roc)\n",
    "\n",
    "    return np.mean(rocs)\n",
    "\n",
    "\n",
    "bounds = {\n",
    "'lr': (0.0001, 0.1)\n",
    "}\n",
    "optimizer = BayesianOptimization(\n",
    "  f=eval_model,\n",
    "  pbounds=bounds,\n",
    "  verbose=2,  # verbose = 1 prints only when a maximum\n",
    "  # is observed, verbose = 0 is silent\n",
    "  random_state=1,\n",
    ")\n",
    "optimizer.maximize(\n",
    "  init_points=10,\n",
    "  n_iter=30\n",
    ")\n",
    "print(f'================= BEST MODEL for {model}', optimizer.max, '=================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation with Optimal Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "splitter = ShuffleSplit(n_splits=5, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "wnVxVni0Cv5J",
    "outputId": "475feeaf-c4fa-43c6-d324-508f02108ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.792415 & 0.312762 & 0.695666 & 0.664487 & 0.778604\n",
      "0.0022 & 0.005407 & 0.007613 & 0.003418 & 0.002021\n"
     ]
    }
   ],
   "source": [
    "#train mlp\n",
    "mlp_train_losses, mlp_train_accuracies, mlp_train_mcc, mlp_train_f1, mlp_train_auprc, mlp_train_auroc = [], [], [], [], [], []\n",
    "mlp_test_losses, mlp_test_accuracies, mlp_test_mcc, mlp_test_f1, mlp_test_auprc, mlp_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        mlp = MLP(input_size=150).to(device)\n",
    "        mlp_optimizer = optim.Adam(mlp.parameters(), lr=0.012234847929677781)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            mlp.train()\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              mlp_optimizer.zero_grad()\n",
    "              outputs = mlp(X_batch)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              mlp_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              mlp_optimizer.step()\n",
    "        with torch.no_grad():\n",
    "          mlp.eval()\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(mlp(X_train))\n",
    "          test_outputs = torch.sigmoid(mlp(X_test))\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          mlp_test_losses.append(test_loss)\n",
    "          mlp_test_accuracies.append(test_acc)\n",
    "          mlp_test_mcc.append(test_mcc)\n",
    "          mlp_test_f1.append(test_f1)\n",
    "          mlp_test_auprc.append(test_auprc)\n",
    "          mlp_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #mlp_train_losses.append(train_loss.item())\n",
    "          mlp_train_accuracies.append(train_acc)\n",
    "          mlp_train_mcc.append(train_mcc)\n",
    "          mlp_train_f1.append(train_f1)\n",
    "          mlp_train_auprc.append(train_auprc)\n",
    "          mlp_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(mlp_test_accuracies), np.mean(mlp_test_mcc), np.mean(mlp_test_f1), np.mean(mlp_test_auprc), np.mean(mlp_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(mlp_test_accuracies), np.std(mlp_test_mcc), np.std(mlp_test_f1), np.std(mlp_test_auprc), np.std(mlp_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "u6cXl1fOCv5J",
    "outputId": "f35cfac6-a7dc-400f-b466-b0c82de27218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.814835 & 0.427532 & 0.745268 & 0.739339 & 0.827698\n",
      "0.010454 & 0.048242 & 0.022887 & 0.031495 & 0.020237\n"
     ]
    }
   ],
   "source": [
    "convnet_train_losses, convnet_train_accuracies, convnet_train_mcc, convnet_train_f1, convnet_train_auprc, convnet_train_auroc = [], [], [], [], [], []\n",
    "convnet_test_losses, convnet_test_accuracies, convnet_test_mcc, convnet_test_f1, convnet_test_auprc, convnet_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        convnet = ConvNet(input_size=150).to(device)\n",
    "        convnet_optimizer = optim.Adam(convnet.parameters(), lr=0.0093246256174029)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            convnet.train()\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              convnet_optimizer.zero_grad()\n",
    "              outputs = convnet(X_batch)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              convnet_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              convnet_optimizer.step()\n",
    "        convnet.eval()\n",
    "        with torch.no_grad():\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(convnet(X_train))\n",
    "          test_outputs = torch.sigmoid(convnet(X_test))\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          convnet_test_losses.append(test_loss)\n",
    "          convnet_test_accuracies.append(test_acc)\n",
    "          convnet_test_mcc.append(test_mcc)\n",
    "          convnet_test_f1.append(test_f1)\n",
    "          convnet_test_auprc.append(test_auprc)\n",
    "          convnet_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #convnet_train_losses.append(train_loss.item())\n",
    "          convnet_train_accuracies.append(train_acc)\n",
    "          convnet_train_mcc.append(train_mcc)\n",
    "          convnet_train_f1.append(train_f1)\n",
    "          convnet_train_auprc.append(train_auprc)\n",
    "          convnet_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convnet_test_accuracies), np.mean(convnet_test_mcc), np.mean(convnet_test_f1), np.mean(convnet_test_auprc), np.mean(convnet_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(convnet_test_accuracies), np.std(convnet_test_mcc), np.std(convnet_test_f1), np.std(convnet_test_auprc), np.std(convnet_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "jI38lwtCCv5J",
    "outputId": "5ea32f23-2d4c-4717-af89-6ca0a3caf8e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.836007 & 0.505963 & 0.780153 & 0.788579 & 0.864475\n",
      "0.002225 & 0.006416 & 0.004854 & 0.004018 & 0.002097\n"
     ]
    }
   ],
   "source": [
    "lstm_train_losses, lstm_train_accuracies, lstm_train_mcc, lstm_train_f1, lstm_train_auprc, lstm_train_auroc = [], [], [], [], [], []\n",
    "lstm_test_losses, lstm_test_accuracies, lstm_test_mcc, lstm_test_f1, lstm_test_auprc, lstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        #print(len(train))\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        lstm = LSTM(input_size=150).to(device)\n",
    "        lstm_optimizer = optim.Adam(lstm.parameters(), lr=0.006486677380339516)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            lstm.train()\n",
    "            h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              lstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = lstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              lstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              lstm_optimizer.step()\n",
    "            lstm.eval()\n",
    "        with torch.no_grad():\n",
    "          h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(lstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          test_outputs = torch.sigmoid(lstm(X_test, h_o, c_o)[0])\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          lstm_test_losses.append(test_loss)\n",
    "          lstm_test_accuracies.append(test_acc)\n",
    "          lstm_test_mcc.append(test_mcc)\n",
    "          lstm_test_f1.append(test_f1)\n",
    "          lstm_test_auprc.append(test_auprc)\n",
    "          lstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #lstm_train_losses.append(train_loss.item())\n",
    "          lstm_train_accuracies.append(train_acc)\n",
    "          lstm_train_mcc.append(train_mcc)\n",
    "          lstm_train_f1.append(train_f1)\n",
    "          lstm_train_auprc.append(train_auprc)\n",
    "          lstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(lstm_test_accuracies), np.mean(lstm_test_mcc), np.mean(lstm_test_f1), np.mean(lstm_test_auprc), np.mean(lstm_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(lstm_test_accuracies), np.std(lstm_test_mcc), np.std(lstm_test_f1), np.std(lstm_test_auprc), np.std(lstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "begUd48HCv5J",
    "outputId": "39401ca5-b9f5-452f-dac3-6a007f1e7723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.854142 & 0.570873 & 0.811046 & 0.828703 & 0.890397\n",
      "0.001137 & 0.004172 & 0.002422 & 0.002735 & 0.001331\n"
     ]
    }
   ],
   "source": [
    "bilstm_train_losses, bilstm_train_accuracies, bilstm_train_mcc, bilstm_train_f1, bilstm_train_auprc, bilstm_train_auroc = [], [], [], [], [], []\n",
    "bilstm_test_losses, bilstm_test_accuracies, bilstm_test_mcc, bilstm_test_f1, bilstm_test_auprc, bilstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        bilstm = BiLSTM(input_size=150).to(device)\n",
    "        bilstm_optimizer = optim.Adam(bilstm.parameters(), lr=0.007036460928329656)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            bilstm.train()\n",
    "            h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              bilstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = bilstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              bilstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              bilstm_optimizer.step()\n",
    "            bilstm.eval()\n",
    "        with torch.no_grad():\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(bilstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          test_outputs = torch.sigmoid(bilstm(X_test, h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          bilstm_test_losses.append(test_loss)\n",
    "          bilstm_test_accuracies.append(test_acc)\n",
    "          bilstm_test_mcc.append(test_mcc)\n",
    "          bilstm_test_f1.append(test_f1)\n",
    "          bilstm_test_auprc.append(test_auprc)\n",
    "          bilstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #bilstm_train_losses.append(train_loss.item())\n",
    "          bilstm_train_accuracies.append(train_acc)\n",
    "          bilstm_train_mcc.append(train_mcc)\n",
    "          bilstm_train_f1.append(train_f1)\n",
    "          bilstm_train_auprc.append(train_auprc)\n",
    "          bilstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(bilstm_test_accuracies), np.mean(bilstm_test_mcc), np.mean(bilstm_test_f1), np.mean(bilstm_test_auprc), np.mean(bilstm_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(bilstm_test_accuracies), np.std(bilstm_test_mcc), np.std(bilstm_test_f1), np.std(bilstm_test_auprc), np.std(bilstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "leffgdVUCv5K",
    "outputId": "9a1fbb2b-4078-4084-f59c-d8533f0f0f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.826136 & 0.458443 & 0.761183 & 0.755122 & 0.843895\n",
      "0.00852 & 0.034958 & 0.019537 & 0.022073 & 0.014923\n"
     ]
    }
   ],
   "source": [
    "convlstm_train_losses, convlstm_train_accuracies, convlstm_train_mcc, convlstm_train_f1, convlstm_train_auprc, convlstm_train_auroc = [], [], [], [], [], []\n",
    "convlstm_test_losses, convlstm_test_accuracies, convlstm_test_mcc, convlstm_test_f1, convlstm_test_auprc, convlstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        convlstm = ConvLSTM(input_size=150).to(device)\n",
    "        convlstm_optimizer = optim.Adam(convlstm.parameters(), lr=0.002697021837162594)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            convlstm.train()\n",
    "            h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              convlstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = convlstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              convlstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              convlstm_optimizer.step()\n",
    "            convlstm.eval()\n",
    "        with torch.no_grad():\n",
    "          h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(convlstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          test_outputs = torch.sigmoid(convlstm(X_test, h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          convlstm_test_losses.append(test_loss)\n",
    "          convlstm_test_accuracies.append(test_acc)\n",
    "          convlstm_test_mcc.append(test_mcc)\n",
    "          convlstm_test_f1.append(test_f1)\n",
    "          convlstm_test_auprc.append(test_auprc)\n",
    "          convlstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #convlstm_train_losses.append(train_loss.item())\n",
    "          convlstm_train_accuracies.append(train_acc)\n",
    "          convlstm_train_mcc.append(train_mcc)\n",
    "          convlstm_train_f1.append(train_f1)\n",
    "          convlstm_train_auprc.append(train_auprc)\n",
    "          convlstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convlstm_test_accuracies), np.mean(convlstm_test_mcc), np.mean(convlstm_test_f1), np.mean(convlstm_test_auprc), np.mean(convlstm_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(convlstm_test_accuracies), np.std(convlstm_test_mcc), np.std(convlstm_test_f1), np.std(convlstm_test_auprc), np.std(convlstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     Model    |  Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "MLP Metrics:  0.792415 & 0.312762 & 0.695666 & 0.664487 & 0.778604\n",
      "MLP Deviations:  0.0022 & 0.005407 & 0.007613 & 0.003418 & 0.002021\n",
      "ConvNet Metrics:  0.814835 & 0.427532 & 0.745268 & 0.739339 & 0.827698\n",
      "ConvNet Deviations:  0.010454 & 0.048242 & 0.022887 & 0.031495 & 0.020237\n",
      "LSTM Metrics:  0.836007 & 0.505963 & 0.780153 & 0.788579 & 0.864475\n",
      "LSTM Deviations:  0.002225 & 0.006416 & 0.004854 & 0.004018 & 0.002097\n",
      "BiLSTM Metrics:  0.854142 & 0.570873 & 0.811046 & 0.828703 & 0.890397\n",
      "BiLSTM Deviations:  0.001137 & 0.004172 & 0.002422 & 0.002735 & 0.001331\n",
      "ConvLSTM Metrics:  0.826136 & 0.458443 & 0.761183 & 0.755122 & 0.843895\n",
      "ConvLSTM Deviations:  0.00852 & 0.034958 & 0.019537 & 0.022073 & 0.014923\n"
     ]
    }
   ],
   "source": [
    "print('|     Model    |  Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print('MLP Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(mlp_test_accuracies), np.mean(mlp_test_mcc), np.mean(mlp_test_f1), np.mean(mlp_test_auprc), np.mean(mlp_test_auroc))])[:-3])\n",
    "print('MLP Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(mlp_test_accuracies), np.std(mlp_test_mcc), np.std(mlp_test_f1), np.std(mlp_test_auprc), np.std(mlp_test_auroc))])[:-3])\n",
    "print('ConvNet Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convnet_test_accuracies), np.mean(convnet_test_mcc), np.mean(convnet_test_f1), np.mean(convnet_test_auprc), np.mean(convnet_test_auroc))])[:-3])\n",
    "print('ConvNet Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convnet_test_accuracies), np.std(convnet_test_mcc), np.std(convnet_test_f1), np.std(convnet_test_auprc), np.std(convnet_test_auroc))])[:-3])\n",
    "print('LSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(lstm_test_accuracies), np.mean(lstm_test_mcc), np.mean(lstm_test_f1), np.mean(lstm_test_auprc), np.mean(lstm_test_auroc))])[:-3])\n",
    "print('LSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(lstm_test_accuracies), np.std(lstm_test_mcc), np.std(lstm_test_f1), np.std(lstm_test_auprc), np.std(lstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(bilstm_test_accuracies), np.mean(bilstm_test_mcc), np.mean(bilstm_test_f1), np.mean(bilstm_test_auprc), np.mean(bilstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(bilstm_test_accuracies), np.std(bilstm_test_mcc), np.std(bilstm_test_f1), np.std(bilstm_test_auprc), np.std(bilstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convlstm_test_accuracies), np.mean(convlstm_test_mcc), np.mean(convlstm_test_f1), np.mean(convlstm_test_auprc), np.mean(convlstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convlstm_test_accuracies), np.std(convlstm_test_mcc), np.std(convlstm_test_f1), np.std(convlstm_test_auprc), np.std(convlstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW NMF with Semantic Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nmf_bow_topics.pkl', 'rb') as f:\n",
    "    nmf_bow_topics = pickle.load(f).T\n",
    "\n",
    "fcrnn = pd.read_csv('filtered_cleaned_raw_nursing_notes_processed.csv')\n",
    "fcrnn['processed_text'] = fcrnn['processed_text'].apply(eval)\n",
    "\n",
    "texts_dict = Dictionary(fcrnn['processed_text'])\n",
    "corpus = [texts_dict.doc2bow(text) for text in fcrnn['processed_text']]\n",
    "cm = CoherenceModel(topics=nmf_bow_topics, dictionary=texts_dict, coherence='c_v', texts=fcrnn['processed_text'])\n",
    "coherences = cm.get_coherence_per_topic()\n",
    "inds = np.argsort(coherences)[-100:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_weighted_bow_topics = nmf_bow_topics[inds]\n",
    "best_coherences = np.array(coherences)[inds]\n",
    "cw_bow_topics = np.multiply(coherence_weighted_bow_topics.T, best_coherences)\n",
    "with open('nmf_bow_w_sc_topics_weighted.pkl', 'wb') as f:\n",
    "    pickle.dump(cw_bow_topics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_bow_topics = nmf_bow_topics[inds]\n",
    "with open('nmf_bow_w_sc_topics.pkl', 'wb') as f:\n",
    "    pickle.dump(coherence_bow_topics.T, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation with Unweighted Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('nmf_bow_w_sc_topics.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('y_labels.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7NIOiaQxsuc",
    "outputId": "5957c44e-d1f6-4a04-9111-c8793cc9cb5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.6639   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.6328   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.6698   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.6785   \u001b[39m | \u001b[35m0.0303   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.7019   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.7086   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.6953   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.674    \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.6669   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.6525   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.6126   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.7056   \u001b[39m | \u001b[39m0.0108   \u001b[39m |\n",
      "| \u001b[35m13       \u001b[39m | \u001b[35m0.7124   \u001b[39m | \u001b[35m0.006555 \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.7117   \u001b[39m | \u001b[39m0.00729  \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m0.7129   \u001b[39m | \u001b[35m0.006459 \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.6216   \u001b[39m | \u001b[39m0.08557  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.6863   \u001b[39m | \u001b[39m0.02417  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.6428   \u001b[39m | \u001b[39m0.06243  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.7125   \u001b[39m | \u001b[39m0.006447 \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.709    \u001b[39m | \u001b[39m0.01004  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.709    \u001b[39m | \u001b[39m0.008057 \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.7029   \u001b[39m | \u001b[39m0.01389  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.7065   \u001b[39m | \u001b[39m0.01172  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.7036   \u001b[39m | \u001b[39m0.0127   \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.6986   \u001b[39m | \u001b[39m0.01575  \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.699    \u001b[39m | \u001b[39m0.01686  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.6972   \u001b[39m | \u001b[39m0.01777  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.6933   \u001b[39m | \u001b[39m0.01989  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.6918   \u001b[39m | \u001b[39m0.02114  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.7097   \u001b[39m | \u001b[39m0.008701 \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.6912   \u001b[39m | \u001b[39m0.02248  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.6148   \u001b[39m | \u001b[39m0.09275  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.6843   \u001b[39m | \u001b[39m0.02602  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.6255   \u001b[39m | \u001b[39m0.07879  \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.6555   \u001b[39m | \u001b[39m0.04777  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.682    \u001b[39m | \u001b[39m0.02777  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.6343   \u001b[39m | \u001b[39m0.06721  \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.6461   \u001b[39m | \u001b[39m0.05813  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.6153   \u001b[39m | \u001b[39m0.09636  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.6171   \u001b[39m | \u001b[39m0.08914  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for MLP(\n",
      "  (fc1): Linear(in_features=100, out_features=75, bias=True)\n",
      "  (fc2): Linear(in_features=75, out_features=19, bias=True)\n",
      ") {'target': 0.7128873820311801, 'params': {'lr': 0.006458983173787745}} =================\n",
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.6921   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.5995   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.7156   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.6934   \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.7187   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.7408   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.714    \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.6833   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.6894   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.6492   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.6035   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.754    \u001b[39m | \u001b[35m0.005859 \u001b[39m |\n",
      "| \u001b[35m13       \u001b[39m | \u001b[35m0.7613   \u001b[39m | \u001b[35m0.004767 \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.6515   \u001b[39m | \u001b[39m0.08607  \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.7604   \u001b[39m | \u001b[39m0.004682 \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.7068   \u001b[39m | \u001b[39m0.02413  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.6744   \u001b[39m | \u001b[39m0.06233  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.6864   \u001b[39m | \u001b[39m0.0472   \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.7145   \u001b[39m | \u001b[39m0.01179  \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.5527   \u001b[39m | \u001b[39m0.07971  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.6107   \u001b[39m | \u001b[39m0.09209  \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.6942   \u001b[39m | \u001b[39m0.02707  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.6687   \u001b[39m | \u001b[39m0.0664   \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.7042   \u001b[39m | \u001b[39m0.02135  \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.7598   \u001b[39m | \u001b[39m0.005156 \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.645    \u001b[39m | \u001b[39m0.05842  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.007696 \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.6731   \u001b[39m | \u001b[39m0.05025  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.7074   \u001b[39m | \u001b[39m0.01668  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.693    \u001b[39m | \u001b[39m0.0445   \u001b[39m |\n",
      "| \u001b[35m31       \u001b[39m | \u001b[35m0.7657   \u001b[39m | \u001b[35m0.002099 \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.7655   \u001b[39m | \u001b[39m0.002824 \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.6915   \u001b[39m | \u001b[39m0.03715  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.765    \u001b[39m | \u001b[39m0.002455 \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.5602   \u001b[39m | \u001b[39m0.09598  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.7642   \u001b[39m | \u001b[39m0.001431 \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.764    \u001b[39m | \u001b[39m0.003321 \u001b[39m |\n",
      "| \u001b[35m38       \u001b[39m | \u001b[35m0.7661   \u001b[39m | \u001b[35m0.001736 \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.765    \u001b[39m | \u001b[39m0.001877 \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.6693   \u001b[39m | \u001b[39m0.07569  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvNet(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=19, bias=True)\n",
      ") {'target': 0.7661453407736014, 'params': {'lr': 0.001736156863213412}} =================\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score, roc_auc_score, average_precision_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from bayes_opt import BayesianOptimization\n",
    "import time\n",
    "\n",
    "models = [\n",
    "    MLP(input_size=100),\n",
    "    ConvNet(input_size=100)\n",
    "]\n",
    "# BATCH_SIZE always 128, train for 8 epochs\n",
    "for model in models:\n",
    "  def eval_model(lr, model=model, X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    device = torch.device(0)\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(X_batch)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "      with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = torch.sigmoid(model(X_test)).cpu().numpy()\n",
    "        #predicted = (outputs > 0.6).float().cpu().numpy()\n",
    "        #print(train_losses)\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        #print(roc)\n",
    "        rocs.append(roc)\n",
    "    #print(rocs)\n",
    "    return np.mean(rocs)\n",
    "  # bayesian optimization on all 5 models....this might take a fews days? :3\n",
    "\n",
    "\n",
    "  bounds = {\n",
    "    'lr': (0.0001, 0.1)\n",
    "  }\n",
    "  optimizer = BayesianOptimization(\n",
    "      f=eval_model,\n",
    "      pbounds=bounds,\n",
    "      verbose=2,  # verbose = 1 prints only when a maximum\n",
    "      # is observed, verbose = 0 is silent\n",
    "      random_state=1,\n",
    "  )\n",
    "  optimizer.maximize(\n",
    "      init_points=10,\n",
    "      n_iter=30\n",
    "  )\n",
    "  print(f'================= BEST MODEL for {model}', optimizer.max, '=================')\n",
    "    if model.__class__.__name__ == 'MLP':\n",
    "      mlp_lr = optimizer.max['params']['lr']\n",
    "  elif model.__class__.__name__ == 'ConvNet':\n",
    "      convnet_lr = optimizer.max['params']['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "HSivLiGZTpsW",
    "outputId": "0b156917-346f-4b7d-e4d8-912389ca6f12",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.5423   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.5151   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.6918   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.6647   \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.7101   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.745    \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.6865   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.6498   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.6327   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.5168   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m0.7451   \u001b[39m | \u001b[35m0.009651 \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.5086   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08593  \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.6798   \u001b[39m | \u001b[39m0.02455  \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m0.7631   \u001b[39m | \u001b[35m0.004563 \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.7581   \u001b[39m | \u001b[39m0.006146 \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06275  \u001b[39m |\n",
      "| \u001b[35m18       \u001b[39m | \u001b[35m0.7695   \u001b[39m | \u001b[35m0.003279 \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.7651   \u001b[39m | \u001b[39m0.004577 \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.7677   \u001b[39m | \u001b[39m0.002002 \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.7298   \u001b[39m | \u001b[39m0.0122   \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.6834   \u001b[39m | \u001b[39m0.02154  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.6769   \u001b[39m | \u001b[39m0.02727  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.6909   \u001b[39m | \u001b[39m0.01664  \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09299  \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.5094   \u001b[39m | \u001b[39m0.07895  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.531    \u001b[39m | \u001b[39m0.04778  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.5063   \u001b[39m | \u001b[39m0.06745  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.6168   \u001b[39m | \u001b[39m0.03719  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.5114   \u001b[39m | \u001b[39m0.05828  \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.6669   \u001b[39m | \u001b[39m0.03242  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.7532   \u001b[39m | \u001b[39m0.007712 \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07548  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.5155   \u001b[39m | \u001b[39m0.08239  \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.5134   \u001b[39m | \u001b[39m0.08944  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09649  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.7627   \u001b[39m | \u001b[39m0.005358 \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.7178   \u001b[39m | \u001b[39m0.01343  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.7386   \u001b[39m | \u001b[39m0.011    \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.5181   \u001b[39m | \u001b[39m0.05081  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for LSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc2): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.7694891324438583, 'params': {'lr': 0.0032785119459016864}} =================\n",
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.7057   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01476  \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m0.7808   \u001b[39m | \u001b[35m0.001508 \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.6929   \u001b[39m | \u001b[39m0.003691 \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.7746   \u001b[39m | \u001b[39m0.002227 \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m0.7839   \u001b[39m | \u001b[35m0.001572 \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08602  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06299  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07904  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09299  \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04775  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0245   \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.4999   \u001b[39m | \u001b[39m0.05845  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06752  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.5598   \u001b[39m | \u001b[39m0.005895 \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.7832   \u001b[39m | \u001b[39m0.001857 \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07553  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08251  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.5001   \u001b[39m | \u001b[39m0.08949  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09647  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.766    \u001b[39m | \u001b[39m0.002826 \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05085  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04476  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.02159  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0274   \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.7823   \u001b[39m | \u001b[39m0.001701 \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.5199   \u001b[39m | \u001b[39m0.01205  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0372   \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06979  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06526  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.7679   \u001b[39m | \u001b[39m0.0007216\u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc3): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.7838769545259426, 'params': {'lr': 0.0015719447694928059}} =================\n"
     ]
    }
   ],
   "source": [
    "lstm_models = [\n",
    "    LSTM(input_size=100),\n",
    "    ConvLSTM(input_size=100)\n",
    "]\n",
    "# BATCH_SIZE always 128, train for 8 epochs\n",
    "for model in lstm_models:\n",
    "  def eval_model(lr, model=model, X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    device = torch.device(\"cuda\")\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs, h_o, c_o = model(X_batch, h_o, c_o)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "      with torch.no_grad():\n",
    "        model.eval()\n",
    "        h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "        outputs = torch.sigmoid(model(X_test, h_o, c_o)[0]).cpu().numpy()\n",
    "        #predicted = (outputs > 0.5).float().cpu().numpy()\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        rocs.append(roc)\n",
    "\n",
    "    return np.mean(rocs)\n",
    "\n",
    "\n",
    "  bounds = {\n",
    "    'lr': (0.0001, 0.1)\n",
    "  }\n",
    "  optimizer = BayesianOptimization(\n",
    "      f=eval_model,\n",
    "      pbounds=bounds,\n",
    "      verbose=2,  # verbose = 1 prints only when a maximum\n",
    "      # is observed, verbose = 0 is silent\n",
    "      random_state=1,\n",
    "  )\n",
    "  optimizer.maximize(\n",
    "      init_points=10,\n",
    "      n_iter=30\n",
    "  )\n",
    "  print(f'================= BEST MODEL for {model}', optimizer.max, '=================')\n",
    "  if model.__class__.__name__ == 'LSTM':\n",
    "      LSTM_lr = optimizer.max['params']['lr']\n",
    "  elif model.__class__.__name__ == 'ConvLSTM':\n",
    "      ConvLSTM_lr = optimizer.max['params']['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "WOQsv5u0Cv5I",
    "outputId": "ae6690b0-5612-4103-ab3a-ff782a46f71a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.6562   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.5109   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.6968   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.6749   \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.6999   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.7579   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.6889   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.6705   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.6559   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.5532   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.7761   \u001b[39m | \u001b[35m0.00588  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.7705   \u001b[39m | \u001b[39m0.006913 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m14       \u001b[39m | \u001b[35m0.7867   \u001b[39m | \u001b[35m0.004278 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.7855   \u001b[39m | \u001b[39m0.004371 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.5003   \u001b[39m | \u001b[39m0.08591  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.6875   \u001b[39m | \u001b[39m0.02437  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.5328   \u001b[39m | \u001b[39m0.06268  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.7839   \u001b[39m | \u001b[39m0.00429  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m20       \u001b[39m | \u001b[35m0.7882   \u001b[39m | \u001b[35m0.00394  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.782    \u001b[39m | \u001b[39m0.004779 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.7742   \u001b[39m | \u001b[39m0.006378 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.7782   \u001b[39m | \u001b[39m0.00538  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.7701   \u001b[39m | \u001b[39m0.00742  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.7668   \u001b[39m | \u001b[39m0.007951 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.7631   \u001b[39m | \u001b[39m0.008659 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.7443   \u001b[39m | \u001b[39m0.009901 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.7394   \u001b[39m | \u001b[39m0.01055  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.73     \u001b[39m | \u001b[39m0.01122  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m30       \u001b[39m | \u001b[35m0.7893   \u001b[39m | \u001b[35m0.003241 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m31       \u001b[39m | \u001b[35m0.7903   \u001b[39m | \u001b[35m0.002759 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m32       \u001b[39m | \u001b[35m0.7922   \u001b[39m | \u001b[35m0.002256 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.789    \u001b[39m | \u001b[39m0.001757 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.7809   \u001b[39m | \u001b[39m0.001237 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.7207   \u001b[39m | \u001b[39m0.01209  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.7626   \u001b[39m | \u001b[39m0.0007712\u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.7162   \u001b[39m | \u001b[39m0.01314  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.5091   \u001b[39m | \u001b[39m0.07868  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.5094   \u001b[39m | \u001b[39m0.09264  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.6039   \u001b[39m | \u001b[39m0.04773  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc3): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.7922227582993449, 'params': {'lr': 0.002256485322469999}} =================\n"
     ]
    }
   ],
   "source": [
    "def eval_model(lr, model=BiLSTM(input_size=100), X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    print(model)\n",
    "    device = torch.device(\"cuda\")\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs, h_o, c_o = model(X_batch, h_o, c_o)\n",
    "          #print(outputs, y_batch)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "        outputs = torch.sigmoid(model(X_test, h_o, c_o)[0]).cpu().numpy()\n",
    "        #predicted = (outputs > 0.5).float().cpu().numpy()\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        rocs.append(roc)\n",
    "\n",
    "    return np.mean(rocs)\n",
    "\n",
    "\n",
    "bounds = {\n",
    "'lr': (0.0001, 0.1)\n",
    "}\n",
    "optimizer = BayesianOptimization(\n",
    "  f=eval_model,\n",
    "  pbounds=bounds,\n",
    "  verbose=2,  # verbose = 1 prints only when a maximum\n",
    "  # is observed, verbose = 0 is silent\n",
    "  random_state=1,\n",
    ")\n",
    "optimizer.maximize(\n",
    "  init_points=10,\n",
    "  n_iter=30\n",
    ")\n",
    "print(f'================= BEST MODEL for {model}', optimizer.max, '=================')\n",
    "BiLSTM_lr = optimizer.max['params']['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Learning Rate: 0.006458983173787745\n",
      "ConvNet Learning Rate: 0.001736156863213412\n",
      "LSTM Learning Rate: 0.0032785119459016864\n",
      "BiLSTM Learning Rate: 0.002256485322469999\n",
      "ConvLSTM Learning Rate: 0.0015719447694928059\n"
     ]
    }
   ],
   "source": [
    "print(f'MLP Learning Rate: {mlp_lr}')\n",
    "print(f'ConvNet Learning Rate: {convnet_lr}')\n",
    "print(f'LSTM Learning Rate: {LSTM_lr}')\n",
    "print(f'BiLSTM Learning Rate: {BiLSTM_lr}')\n",
    "print(f'ConvLSTM Learning Rate: {ConvLSTM_lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation with Optimal Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "splitter = ShuffleSplit(n_splits=5, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "wnVxVni0Cv5J",
    "outputId": "475feeaf-c4fa-43c6-d324-508f02108ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.765326 & 0.219426 & 0.638095 & 0.598131 & 0.71319\n",
      "0.001351 & 0.004257 & 0.005639 & 0.001832 & 0.001444\n"
     ]
    }
   ],
   "source": [
    "#train mlp\n",
    "mlp_train_losses, mlp_train_accuracies, mlp_train_mcc, mlp_train_f1, mlp_train_auprc, mlp_train_auroc = [], [], [], [], [], []\n",
    "mlp_test_losses, mlp_test_accuracies, mlp_test_mcc, mlp_test_f1, mlp_test_auprc, mlp_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        mlp = MLP(input_size=100).to(device)\n",
    "        mlp_optimizer = optim.Adam(mlp.parameters(), lr=mlp_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            mlp.train()\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              mlp_optimizer.zero_grad()\n",
    "              outputs = mlp(X_batch)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              mlp_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              mlp_optimizer.step()\n",
    "        with torch.no_grad():\n",
    "          mlp.eval()\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(mlp(X_train))\n",
    "          test_outputs = torch.sigmoid(mlp(X_test))\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          mlp_test_losses.append(test_loss)\n",
    "          mlp_test_accuracies.append(test_acc)\n",
    "          mlp_test_mcc.append(test_mcc)\n",
    "          mlp_test_f1.append(test_f1)\n",
    "          mlp_test_auprc.append(test_auprc)\n",
    "          mlp_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #mlp_train_losses.append(train_loss.item())\n",
    "          mlp_train_accuracies.append(train_acc)\n",
    "          mlp_train_mcc.append(train_mcc)\n",
    "          mlp_train_f1.append(train_f1)\n",
    "          mlp_train_auprc.append(train_auprc)\n",
    "          mlp_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(mlp_test_accuracies), np.mean(mlp_test_mcc), np.mean(mlp_test_f1), np.mean(mlp_test_auprc), np.mean(mlp_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(mlp_test_accuracies), np.std(mlp_test_mcc), np.std(mlp_test_f1), np.std(mlp_test_auprc), np.std(mlp_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "u6cXl1fOCv5J",
    "outputId": "f35cfac6-a7dc-400f-b466-b0c82de27218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.782195 & 0.322166 & 0.689659 & 0.664868 & 0.764801\n",
      "0.00123 & 0.00412 & 0.005031 & 0.003162 & 0.001491\n"
     ]
    }
   ],
   "source": [
    "convnet_train_losses, convnet_train_accuracies, convnet_train_mcc, convnet_train_f1, convnet_train_auprc, convnet_train_auroc = [], [], [], [], [], []\n",
    "convnet_test_losses, convnet_test_accuracies, convnet_test_mcc, convnet_test_f1, convnet_test_auprc, convnet_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        convnet = ConvNet(input_size=100).to(device)\n",
    "        convnet_optimizer = optim.Adam(convnet.parameters(), lr=convnet_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            convnet.train()\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              convnet_optimizer.zero_grad()\n",
    "              outputs = convnet(X_batch)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              convnet_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              convnet_optimizer.step()\n",
    "        convnet.eval()\n",
    "        with torch.no_grad():\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(convnet(X_train))\n",
    "          test_outputs = torch.sigmoid(convnet(X_test))\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          convnet_test_losses.append(test_loss)\n",
    "          convnet_test_accuracies.append(test_acc)\n",
    "          convnet_test_mcc.append(test_mcc)\n",
    "          convnet_test_f1.append(test_f1)\n",
    "          convnet_test_auprc.append(test_auprc)\n",
    "          convnet_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #convnet_train_losses.append(train_loss.item())\n",
    "          convnet_train_accuracies.append(train_acc)\n",
    "          convnet_train_mcc.append(train_mcc)\n",
    "          convnet_train_f1.append(train_f1)\n",
    "          convnet_train_auprc.append(train_auprc)\n",
    "          convnet_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convnet_test_accuracies), np.mean(convnet_test_mcc), np.mean(convnet_test_f1), np.mean(convnet_test_auprc), np.mean(convnet_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(convnet_test_accuracies), np.std(convnet_test_mcc), np.std(convnet_test_f1), np.std(convnet_test_auprc), np.std(convnet_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "jI38lwtCCv5J",
    "outputId": "5ea32f23-2d4c-4717-af89-6ca0a3caf8e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.785243 & 0.323532 & 0.692422 & 0.663507 & 0.768399\n",
      "LSTM Deviations:  0.001398 & 0.003996 & 0.004624 & 0.002204 & 0.001395\n"
     ]
    }
   ],
   "source": [
    "lstm_train_losses, lstm_train_accuracies, lstm_train_mcc, lstm_train_f1, lstm_train_auprc, lstm_train_auroc = [], [], [], [], [], []\n",
    "lstm_test_losses, lstm_test_accuracies, lstm_test_mcc, lstm_test_f1, lstm_test_auprc, lstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        #print(len(train))\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        lstm = LSTM(input_size=100).to(device)\n",
    "        lstm_optimizer = optim.Adam(lstm.parameters(), lr=LSTM_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            lstm.train()\n",
    "            h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              lstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = lstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              lstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              lstm_optimizer.step()\n",
    "            lstm.eval()\n",
    "        with torch.no_grad():\n",
    "          h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(lstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          test_outputs = torch.sigmoid(lstm(X_test, h_o, c_o)[0])\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          lstm_test_losses.append(test_loss)\n",
    "          lstm_test_accuracies.append(test_acc)\n",
    "          lstm_test_mcc.append(test_mcc)\n",
    "          lstm_test_f1.append(test_f1)\n",
    "          lstm_test_auprc.append(test_auprc)\n",
    "          lstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #lstm_train_losses.append(train_loss.item())\n",
    "          lstm_train_accuracies.append(train_acc)\n",
    "          lstm_train_mcc.append(train_mcc)\n",
    "          lstm_train_f1.append(train_f1)\n",
    "          lstm_train_auprc.append(train_auprc)\n",
    "          lstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(lstm_test_accuracies), np.mean(lstm_test_mcc), np.mean(lstm_test_f1), np.mean(lstm_test_auprc), np.mean(lstm_test_auroc))])[:-3])\n",
    "print('LSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(lstm_test_accuracies), np.std(lstm_test_mcc), np.std(lstm_test_f1), np.std(lstm_test_auprc), np.std(lstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "begUd48HCv5J",
    "outputId": "39401ca5-b9f5-452f-dac3-6a007f1e7723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.797004 & 0.377771 & 0.721987 & 0.697284 & 0.792703\n",
      "BiLSTM Deviations:  0.001053 & 0.003726 & 0.003293 & 0.002846 & 0.001569\n"
     ]
    }
   ],
   "source": [
    "bilstm_train_losses, bilstm_train_accuracies, bilstm_train_mcc, bilstm_train_f1, bilstm_train_auprc, bilstm_train_auroc = [], [], [], [], [], []\n",
    "bilstm_test_losses, bilstm_test_accuracies, bilstm_test_mcc, bilstm_test_f1, bilstm_test_auprc, bilstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        bilstm = BiLSTM(input_size=100).to(device)\n",
    "        bilstm_optimizer = optim.Adam(bilstm.parameters(), lr=BiLSTM_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            bilstm.train()\n",
    "            h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              bilstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = bilstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              bilstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              bilstm_optimizer.step()\n",
    "            bilstm.eval()\n",
    "        with torch.no_grad():\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(bilstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          test_outputs = torch.sigmoid(bilstm(X_test, h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          bilstm_test_losses.append(test_loss)\n",
    "          bilstm_test_accuracies.append(test_acc)\n",
    "          bilstm_test_mcc.append(test_mcc)\n",
    "          bilstm_test_f1.append(test_f1)\n",
    "          bilstm_test_auprc.append(test_auprc)\n",
    "          bilstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #bilstm_train_losses.append(train_loss.item())\n",
    "          bilstm_train_accuracies.append(train_acc)\n",
    "          bilstm_train_mcc.append(train_mcc)\n",
    "          bilstm_train_f1.append(train_f1)\n",
    "          bilstm_train_auprc.append(train_auprc)\n",
    "          bilstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(bilstm_test_accuracies), np.mean(bilstm_test_mcc), np.mean(bilstm_test_f1), np.mean(bilstm_test_auprc), np.mean(bilstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(bilstm_test_accuracies), np.std(bilstm_test_mcc), np.std(bilstm_test_f1), np.std(bilstm_test_auprc), np.std(bilstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "leffgdVUCv5K",
    "outputId": "9a1fbb2b-4078-4084-f59c-d8533f0f0f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.794051 & 0.359931 & 0.716901 & 0.681836 & 0.783718\n",
      "ConvLSTM Deviations:  0.001945 & 0.007371 & 0.005514 & 0.004873 & 0.003461\n"
     ]
    }
   ],
   "source": [
    "convlstm_train_losses, convlstm_train_accuracies, convlstm_train_mcc, convlstm_train_f1, convlstm_train_auprc, convlstm_train_auroc = [], [], [], [], [], []\n",
    "convlstm_test_losses, convlstm_test_accuracies, convlstm_test_mcc, convlstm_test_f1, convlstm_test_auprc, convlstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        convlstm = ConvLSTM(input_size=100).to(device)\n",
    "        convlstm_optimizer = optim.Adam(convlstm.parameters(), lr=ConvLSTM_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            convlstm.train()\n",
    "            h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              convlstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = convlstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              convlstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              convlstm_optimizer.step()\n",
    "            convlstm.eval()\n",
    "        with torch.no_grad():\n",
    "          h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(convlstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          test_outputs = torch.sigmoid(convlstm(X_test, h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          convlstm_test_losses.append(test_loss)\n",
    "          convlstm_test_accuracies.append(test_acc)\n",
    "          convlstm_test_mcc.append(test_mcc)\n",
    "          convlstm_test_f1.append(test_f1)\n",
    "          convlstm_test_auprc.append(test_auprc)\n",
    "          convlstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #convlstm_train_losses.append(train_loss.item())\n",
    "          convlstm_train_accuracies.append(train_acc)\n",
    "          convlstm_train_mcc.append(train_mcc)\n",
    "          convlstm_train_f1.append(train_f1)\n",
    "          convlstm_train_auprc.append(train_auprc)\n",
    "          convlstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convlstm_test_accuracies), np.mean(convlstm_test_mcc), np.mean(convlstm_test_f1), np.mean(convlstm_test_auprc), np.mean(convlstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convlstm_test_accuracies), np.std(convlstm_test_mcc), np.std(convlstm_test_f1), np.std(convlstm_test_auprc), np.std(convlstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     Model    |  Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "MLP Metrics:  0.765326 & 0.219426 & 0.638095 & 0.598131 & 0.71319\n",
      "MLP Deviations:  0.001351 & 0.004257 & 0.005639 & 0.001832 & 0.001444\n",
      "ConvNet Metrics:  0.782195 & 0.322166 & 0.689659 & 0.664868 & 0.764801\n",
      "ConvNet Deviations:  0.00123 & 0.00412 & 0.005031 & 0.003162 & 0.001491\n",
      "LSTM Metrics:  0.785243 & 0.323532 & 0.692422 & 0.663507 & 0.768399\n",
      "LSTM Deviations:  0.001398 & 0.003996 & 0.004624 & 0.002204 & 0.001395\n",
      "BiLSTM Metrics:  0.797004 & 0.377771 & 0.721987 & 0.697284 & 0.792703\n",
      "BiLSTM Deviations:  0.001053 & 0.003726 & 0.003293 & 0.002846 & 0.001569\n",
      "ConvLSTM Metrics:  0.794051 & 0.359931 & 0.716901 & 0.681836 & 0.783718\n",
      "ConvLSTM Deviations:  0.001945 & 0.007371 & 0.005514 & 0.004873 & 0.003461\n"
     ]
    }
   ],
   "source": [
    "print('|     Model    |  Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print('MLP Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(mlp_test_accuracies), np.mean(mlp_test_mcc), np.mean(mlp_test_f1), np.mean(mlp_test_auprc), np.mean(mlp_test_auroc))])[:-3])\n",
    "print('MLP Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(mlp_test_accuracies), np.std(mlp_test_mcc), np.std(mlp_test_f1), np.std(mlp_test_auprc), np.std(mlp_test_auroc))])[:-3])\n",
    "print('ConvNet Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convnet_test_accuracies), np.mean(convnet_test_mcc), np.mean(convnet_test_f1), np.mean(convnet_test_auprc), np.mean(convnet_test_auroc))])[:-3])\n",
    "print('ConvNet Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convnet_test_accuracies), np.std(convnet_test_mcc), np.std(convnet_test_f1), np.std(convnet_test_auprc), np.std(convnet_test_auroc))])[:-3])\n",
    "print('LSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(lstm_test_accuracies), np.mean(lstm_test_mcc), np.mean(lstm_test_f1), np.mean(lstm_test_auprc), np.mean(lstm_test_auroc))])[:-3])\n",
    "print('LSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(lstm_test_accuracies), np.std(lstm_test_mcc), np.std(lstm_test_f1), np.std(lstm_test_auprc), np.std(lstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(bilstm_test_accuracies), np.mean(bilstm_test_mcc), np.mean(bilstm_test_f1), np.mean(bilstm_test_auprc), np.mean(bilstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(bilstm_test_accuracies), np.std(bilstm_test_mcc), np.std(bilstm_test_f1), np.std(bilstm_test_auprc), np.std(bilstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convlstm_test_accuracies), np.mean(convlstm_test_mcc), np.mean(convlstm_test_f1), np.mean(convlstm_test_auprc), np.mean(convlstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convlstm_test_accuracies), np.std(convlstm_test_mcc), np.std(convlstm_test_f1), np.std(convlstm_test_auprc), np.std(convlstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation with Weighted Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('nmf_bow_w_sc_topics_weighted.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('y_labels.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7NIOiaQxsuc",
    "outputId": "5957c44e-d1f6-4a04-9111-c8793cc9cb5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.6639   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.6328   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.6698   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.6785   \u001b[39m | \u001b[35m0.0303   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.7019   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.7086   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.6953   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.674    \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.6669   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.6525   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.6126   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.7056   \u001b[39m | \u001b[39m0.0108   \u001b[39m |\n",
      "| \u001b[35m13       \u001b[39m | \u001b[35m0.7124   \u001b[39m | \u001b[35m0.006555 \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.7117   \u001b[39m | \u001b[39m0.00729  \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m0.7129   \u001b[39m | \u001b[35m0.006459 \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.6216   \u001b[39m | \u001b[39m0.08557  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.6863   \u001b[39m | \u001b[39m0.02417  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.6428   \u001b[39m | \u001b[39m0.06243  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.7125   \u001b[39m | \u001b[39m0.006447 \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.709    \u001b[39m | \u001b[39m0.01004  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.709    \u001b[39m | \u001b[39m0.008057 \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.7029   \u001b[39m | \u001b[39m0.01389  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.7065   \u001b[39m | \u001b[39m0.01172  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.7036   \u001b[39m | \u001b[39m0.0127   \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.6986   \u001b[39m | \u001b[39m0.01575  \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.699    \u001b[39m | \u001b[39m0.01686  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.6972   \u001b[39m | \u001b[39m0.01777  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.6933   \u001b[39m | \u001b[39m0.01989  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.6918   \u001b[39m | \u001b[39m0.02114  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.7097   \u001b[39m | \u001b[39m0.008701 \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.6912   \u001b[39m | \u001b[39m0.02248  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.6148   \u001b[39m | \u001b[39m0.09275  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.6843   \u001b[39m | \u001b[39m0.02602  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.6255   \u001b[39m | \u001b[39m0.07879  \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.6555   \u001b[39m | \u001b[39m0.04777  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.682    \u001b[39m | \u001b[39m0.02777  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.6343   \u001b[39m | \u001b[39m0.06721  \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.6461   \u001b[39m | \u001b[39m0.05813  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.6153   \u001b[39m | \u001b[39m0.09636  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.6171   \u001b[39m | \u001b[39m0.08914  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for MLP(\n",
      "  (fc1): Linear(in_features=100, out_features=75, bias=True)\n",
      "  (fc2): Linear(in_features=75, out_features=19, bias=True)\n",
      ") {'target': 0.7128873820311801, 'params': {'lr': 0.006458983173787745}} =================\n",
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.6921   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.5995   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.7156   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.6934   \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.7187   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.7408   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.714    \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.6833   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.6894   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.6492   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.6035   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.754    \u001b[39m | \u001b[35m0.005859 \u001b[39m |\n",
      "| \u001b[35m13       \u001b[39m | \u001b[35m0.7613   \u001b[39m | \u001b[35m0.004767 \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.6515   \u001b[39m | \u001b[39m0.08607  \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.7604   \u001b[39m | \u001b[39m0.004682 \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.7068   \u001b[39m | \u001b[39m0.02413  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.6744   \u001b[39m | \u001b[39m0.06233  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.6864   \u001b[39m | \u001b[39m0.0472   \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.7145   \u001b[39m | \u001b[39m0.01179  \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.5527   \u001b[39m | \u001b[39m0.07971  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.6107   \u001b[39m | \u001b[39m0.09209  \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.6942   \u001b[39m | \u001b[39m0.02707  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.6687   \u001b[39m | \u001b[39m0.0664   \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.7042   \u001b[39m | \u001b[39m0.02135  \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.7598   \u001b[39m | \u001b[39m0.005156 \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.645    \u001b[39m | \u001b[39m0.05842  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.007696 \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.6731   \u001b[39m | \u001b[39m0.05025  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.7074   \u001b[39m | \u001b[39m0.01668  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.693    \u001b[39m | \u001b[39m0.0445   \u001b[39m |\n",
      "| \u001b[35m31       \u001b[39m | \u001b[35m0.7657   \u001b[39m | \u001b[35m0.002099 \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.7655   \u001b[39m | \u001b[39m0.002824 \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.6915   \u001b[39m | \u001b[39m0.03715  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.765    \u001b[39m | \u001b[39m0.002455 \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.5602   \u001b[39m | \u001b[39m0.09598  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.7642   \u001b[39m | \u001b[39m0.001431 \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.764    \u001b[39m | \u001b[39m0.003321 \u001b[39m |\n",
      "| \u001b[35m38       \u001b[39m | \u001b[35m0.7661   \u001b[39m | \u001b[35m0.001736 \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.765    \u001b[39m | \u001b[39m0.001877 \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.6693   \u001b[39m | \u001b[39m0.07569  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvNet(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=19, bias=True)\n",
      ") {'target': 0.7661453407736014, 'params': {'lr': 0.001736156863213412}} =================\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score, roc_auc_score, average_precision_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from bayes_opt import BayesianOptimization\n",
    "import time\n",
    "\n",
    "models = [\n",
    "    MLP(input_size=100),\n",
    "    ConvNet(input_size=100)\n",
    "]\n",
    "# BATCH_SIZE always 128, train for 8 epochs\n",
    "for model in models:\n",
    "  def eval_model(lr, model=model, X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    device = torch.device(0)\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(X_batch)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "      with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = torch.sigmoid(model(X_test)).cpu().numpy()\n",
    "        #predicted = (outputs > 0.6).float().cpu().numpy()\n",
    "        #print(train_losses)\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        #print(roc)\n",
    "        rocs.append(roc)\n",
    "    #print(rocs)\n",
    "    return np.mean(rocs)\n",
    "  # bayesian optimization on all 5 models....this might take a fews days? :3\n",
    "\n",
    "\n",
    "  bounds = {\n",
    "    'lr': (0.0001, 0.1)\n",
    "  }\n",
    "  optimizer = BayesianOptimization(\n",
    "      f=eval_model,\n",
    "      pbounds=bounds,\n",
    "      verbose=2,  # verbose = 1 prints only when a maximum\n",
    "      # is observed, verbose = 0 is silent\n",
    "      random_state=1,\n",
    "  )\n",
    "  optimizer.maximize(\n",
    "      init_points=10,\n",
    "      n_iter=30\n",
    "  )\n",
    "  print(f'================= BEST MODEL for {model}', optimizer.max, '=================')\n",
    "    if model.__class__.__name__ == 'MLP':\n",
    "      mlp_lr = optimizer.max['params']['lr']\n",
    "  elif model.__class__.__name__ == 'ConvNet':\n",
    "      convnet_lr = optimizer.max['params']['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "HSivLiGZTpsW",
    "outputId": "0b156917-346f-4b7d-e4d8-912389ca6f12",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.5423   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.5151   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.6918   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.6647   \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.7101   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.745    \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.6865   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.6498   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.6327   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.5168   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m0.7451   \u001b[39m | \u001b[35m0.009651 \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.5086   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08593  \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.6798   \u001b[39m | \u001b[39m0.02455  \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m0.7631   \u001b[39m | \u001b[35m0.004563 \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.7581   \u001b[39m | \u001b[39m0.006146 \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06275  \u001b[39m |\n",
      "| \u001b[35m18       \u001b[39m | \u001b[35m0.7695   \u001b[39m | \u001b[35m0.003279 \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.7651   \u001b[39m | \u001b[39m0.004577 \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.7677   \u001b[39m | \u001b[39m0.002002 \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.7298   \u001b[39m | \u001b[39m0.0122   \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.6834   \u001b[39m | \u001b[39m0.02154  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.6769   \u001b[39m | \u001b[39m0.02727  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.6909   \u001b[39m | \u001b[39m0.01664  \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09299  \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.5094   \u001b[39m | \u001b[39m0.07895  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.531    \u001b[39m | \u001b[39m0.04778  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.5063   \u001b[39m | \u001b[39m0.06745  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.6168   \u001b[39m | \u001b[39m0.03719  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.5114   \u001b[39m | \u001b[39m0.05828  \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.6669   \u001b[39m | \u001b[39m0.03242  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.7532   \u001b[39m | \u001b[39m0.007712 \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07548  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.5155   \u001b[39m | \u001b[39m0.08239  \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.5134   \u001b[39m | \u001b[39m0.08944  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09649  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.7627   \u001b[39m | \u001b[39m0.005358 \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.7178   \u001b[39m | \u001b[39m0.01343  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.7386   \u001b[39m | \u001b[39m0.011    \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.5181   \u001b[39m | \u001b[39m0.05081  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for LSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc2): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.7694891324438583, 'params': {'lr': 0.0032785119459016864}} =================\n",
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.7057   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01476  \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m0.7808   \u001b[39m | \u001b[35m0.001508 \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.6929   \u001b[39m | \u001b[39m0.003691 \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.7746   \u001b[39m | \u001b[39m0.002227 \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m0.7839   \u001b[39m | \u001b[35m0.001572 \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08602  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06299  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07904  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09299  \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04775  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0245   \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.4999   \u001b[39m | \u001b[39m0.05845  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06752  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.5598   \u001b[39m | \u001b[39m0.005895 \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.7832   \u001b[39m | \u001b[39m0.001857 \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07553  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08251  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.5001   \u001b[39m | \u001b[39m0.08949  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09647  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.766    \u001b[39m | \u001b[39m0.002826 \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05085  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04476  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.02159  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0274   \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.7823   \u001b[39m | \u001b[39m0.001701 \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.5199   \u001b[39m | \u001b[39m0.01205  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0372   \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06979  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06526  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.7679   \u001b[39m | \u001b[39m0.0007216\u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc3): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.7838769545259426, 'params': {'lr': 0.0015719447694928059}} =================\n"
     ]
    }
   ],
   "source": [
    "lstm_models = [\n",
    "    LSTM(input_size=100),\n",
    "    ConvLSTM(input_size=100)\n",
    "]\n",
    "# BATCH_SIZE always 128, train for 8 epochs\n",
    "for model in lstm_models:\n",
    "  def eval_model(lr, model=model, X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    device = torch.device(\"cuda\")\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs, h_o, c_o = model(X_batch, h_o, c_o)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "      with torch.no_grad():\n",
    "        model.eval()\n",
    "        h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "        outputs = torch.sigmoid(model(X_test, h_o, c_o)[0]).cpu().numpy()\n",
    "        #predicted = (outputs > 0.5).float().cpu().numpy()\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        rocs.append(roc)\n",
    "\n",
    "    return np.mean(rocs)\n",
    "\n",
    "\n",
    "  bounds = {\n",
    "    'lr': (0.0001, 0.1)\n",
    "  }\n",
    "  optimizer = BayesianOptimization(\n",
    "      f=eval_model,\n",
    "      pbounds=bounds,\n",
    "      verbose=2,  # verbose = 1 prints only when a maximum\n",
    "      # is observed, verbose = 0 is silent\n",
    "      random_state=1,\n",
    "  )\n",
    "  optimizer.maximize(\n",
    "      init_points=10,\n",
    "      n_iter=30\n",
    "  )\n",
    "  print(f'================= BEST MODEL for {model}', optimizer.max, '=================')\n",
    "  if model.__class__.__name__ == 'LSTM':\n",
    "      LSTM_lr = optimizer.max['params']['lr']\n",
    "  elif model.__class__.__name__ == 'ConvLSTM':\n",
    "      ConvLSTM_lr = optimizer.max['params']['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "WOQsv5u0Cv5I",
    "outputId": "ae6690b0-5612-4103-ab3a-ff782a46f71a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.6562   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.5109   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.6968   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.6749   \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.6999   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.7579   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.6889   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.6705   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.6559   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.5532   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.7761   \u001b[39m | \u001b[35m0.00588  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.7705   \u001b[39m | \u001b[39m0.006913 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m14       \u001b[39m | \u001b[35m0.7867   \u001b[39m | \u001b[35m0.004278 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.7855   \u001b[39m | \u001b[39m0.004371 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.5003   \u001b[39m | \u001b[39m0.08591  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.6875   \u001b[39m | \u001b[39m0.02437  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.5328   \u001b[39m | \u001b[39m0.06268  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.7839   \u001b[39m | \u001b[39m0.00429  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m20       \u001b[39m | \u001b[35m0.7882   \u001b[39m | \u001b[35m0.00394  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.782    \u001b[39m | \u001b[39m0.004779 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.7742   \u001b[39m | \u001b[39m0.006378 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.7782   \u001b[39m | \u001b[39m0.00538  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.7701   \u001b[39m | \u001b[39m0.00742  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.7668   \u001b[39m | \u001b[39m0.007951 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.7631   \u001b[39m | \u001b[39m0.008659 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.7443   \u001b[39m | \u001b[39m0.009901 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.7394   \u001b[39m | \u001b[39m0.01055  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.73     \u001b[39m | \u001b[39m0.01122  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m30       \u001b[39m | \u001b[35m0.7893   \u001b[39m | \u001b[35m0.003241 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m31       \u001b[39m | \u001b[35m0.7903   \u001b[39m | \u001b[35m0.002759 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m32       \u001b[39m | \u001b[35m0.7922   \u001b[39m | \u001b[35m0.002256 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.789    \u001b[39m | \u001b[39m0.001757 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.7809   \u001b[39m | \u001b[39m0.001237 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.7207   \u001b[39m | \u001b[39m0.01209  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.7626   \u001b[39m | \u001b[39m0.0007712\u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.7162   \u001b[39m | \u001b[39m0.01314  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.5091   \u001b[39m | \u001b[39m0.07868  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.5094   \u001b[39m | \u001b[39m0.09264  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.6039   \u001b[39m | \u001b[39m0.04773  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc3): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.7922227582993449, 'params': {'lr': 0.002256485322469999}} =================\n"
     ]
    }
   ],
   "source": [
    "def eval_model(lr, model=BiLSTM(input_size=100), X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    print(model)\n",
    "    device = torch.device(\"cuda\")\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs, h_o, c_o = model(X_batch, h_o, c_o)\n",
    "          #print(outputs, y_batch)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "        outputs = torch.sigmoid(model(X_test, h_o, c_o)[0]).cpu().numpy()\n",
    "        #predicted = (outputs > 0.5).float().cpu().numpy()\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        rocs.append(roc)\n",
    "\n",
    "    return np.mean(rocs)\n",
    "\n",
    "\n",
    "bounds = {\n",
    "'lr': (0.0001, 0.1)\n",
    "}\n",
    "optimizer = BayesianOptimization(\n",
    "  f=eval_model,\n",
    "  pbounds=bounds,\n",
    "  verbose=2,  # verbose = 1 prints only when a maximum\n",
    "  # is observed, verbose = 0 is silent\n",
    "  random_state=1,\n",
    ")\n",
    "optimizer.maximize(\n",
    "  init_points=10,\n",
    "  n_iter=30\n",
    ")\n",
    "print(f'================= BEST MODEL for {model}', optimizer.max, '=================')\n",
    "BiLSTM_lr = optimizer.max['params']['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Learning Rate: 0.006458983173787745\n",
      "ConvNet Learning Rate: 0.001736156863213412\n",
      "LSTM Learning Rate: 0.0032785119459016864\n",
      "BiLSTM Learning Rate: 0.002256485322469999\n",
      "ConvLSTM Learning Rate: 0.0015719447694928059\n"
     ]
    }
   ],
   "source": [
    "print(f'MLP Learning Rate: {mlp_lr}')\n",
    "print(f'ConvNet Learning Rate: {convnet_lr}')\n",
    "print(f'LSTM Learning Rate: {LSTM_lr}')\n",
    "print(f'BiLSTM Learning Rate: {BiLSTM_lr}')\n",
    "print(f'ConvLSTM Learning Rate: {ConvLSTM_lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation with Optimal Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "splitter = ShuffleSplit(n_splits=5, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "wnVxVni0Cv5J",
    "outputId": "475feeaf-c4fa-43c6-d324-508f02108ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.765326 & 0.219426 & 0.638095 & 0.598131 & 0.71319\n",
      "0.001351 & 0.004257 & 0.005639 & 0.001832 & 0.001444\n"
     ]
    }
   ],
   "source": [
    "#train mlp\n",
    "mlp_train_losses, mlp_train_accuracies, mlp_train_mcc, mlp_train_f1, mlp_train_auprc, mlp_train_auroc = [], [], [], [], [], []\n",
    "mlp_test_losses, mlp_test_accuracies, mlp_test_mcc, mlp_test_f1, mlp_test_auprc, mlp_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        mlp = MLP(input_size=100).to(device)\n",
    "        mlp_optimizer = optim.Adam(mlp.parameters(), lr=mlp_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            mlp.train()\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              mlp_optimizer.zero_grad()\n",
    "              outputs = mlp(X_batch)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              mlp_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              mlp_optimizer.step()\n",
    "        with torch.no_grad():\n",
    "          mlp.eval()\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(mlp(X_train))\n",
    "          test_outputs = torch.sigmoid(mlp(X_test))\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          mlp_test_losses.append(test_loss)\n",
    "          mlp_test_accuracies.append(test_acc)\n",
    "          mlp_test_mcc.append(test_mcc)\n",
    "          mlp_test_f1.append(test_f1)\n",
    "          mlp_test_auprc.append(test_auprc)\n",
    "          mlp_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #mlp_train_losses.append(train_loss.item())\n",
    "          mlp_train_accuracies.append(train_acc)\n",
    "          mlp_train_mcc.append(train_mcc)\n",
    "          mlp_train_f1.append(train_f1)\n",
    "          mlp_train_auprc.append(train_auprc)\n",
    "          mlp_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(mlp_test_accuracies), np.mean(mlp_test_mcc), np.mean(mlp_test_f1), np.mean(mlp_test_auprc), np.mean(mlp_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(mlp_test_accuracies), np.std(mlp_test_mcc), np.std(mlp_test_f1), np.std(mlp_test_auprc), np.std(mlp_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "u6cXl1fOCv5J",
    "outputId": "f35cfac6-a7dc-400f-b466-b0c82de27218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.782195 & 0.322166 & 0.689659 & 0.664868 & 0.764801\n",
      "0.00123 & 0.00412 & 0.005031 & 0.003162 & 0.001491\n"
     ]
    }
   ],
   "source": [
    "convnet_train_losses, convnet_train_accuracies, convnet_train_mcc, convnet_train_f1, convnet_train_auprc, convnet_train_auroc = [], [], [], [], [], []\n",
    "convnet_test_losses, convnet_test_accuracies, convnet_test_mcc, convnet_test_f1, convnet_test_auprc, convnet_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        convnet = ConvNet(input_size=100).to(device)\n",
    "        convnet_optimizer = optim.Adam(convnet.parameters(), lr=convnet_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            convnet.train()\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              convnet_optimizer.zero_grad()\n",
    "              outputs = convnet(X_batch)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              convnet_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              convnet_optimizer.step()\n",
    "        convnet.eval()\n",
    "        with torch.no_grad():\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(convnet(X_train))\n",
    "          test_outputs = torch.sigmoid(convnet(X_test))\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          convnet_test_losses.append(test_loss)\n",
    "          convnet_test_accuracies.append(test_acc)\n",
    "          convnet_test_mcc.append(test_mcc)\n",
    "          convnet_test_f1.append(test_f1)\n",
    "          convnet_test_auprc.append(test_auprc)\n",
    "          convnet_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #convnet_train_losses.append(train_loss.item())\n",
    "          convnet_train_accuracies.append(train_acc)\n",
    "          convnet_train_mcc.append(train_mcc)\n",
    "          convnet_train_f1.append(train_f1)\n",
    "          convnet_train_auprc.append(train_auprc)\n",
    "          convnet_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convnet_test_accuracies), np.mean(convnet_test_mcc), np.mean(convnet_test_f1), np.mean(convnet_test_auprc), np.mean(convnet_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(convnet_test_accuracies), np.std(convnet_test_mcc), np.std(convnet_test_f1), np.std(convnet_test_auprc), np.std(convnet_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "jI38lwtCCv5J",
    "outputId": "5ea32f23-2d4c-4717-af89-6ca0a3caf8e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.785243 & 0.323532 & 0.692422 & 0.663507 & 0.768399\n",
      "LSTM Deviations:  0.001398 & 0.003996 & 0.004624 & 0.002204 & 0.001395\n"
     ]
    }
   ],
   "source": [
    "lstm_train_losses, lstm_train_accuracies, lstm_train_mcc, lstm_train_f1, lstm_train_auprc, lstm_train_auroc = [], [], [], [], [], []\n",
    "lstm_test_losses, lstm_test_accuracies, lstm_test_mcc, lstm_test_f1, lstm_test_auprc, lstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        #print(len(train))\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        lstm = LSTM(input_size=100).to(device)\n",
    "        lstm_optimizer = optim.Adam(lstm.parameters(), lr=LSTM_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            lstm.train()\n",
    "            h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              lstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = lstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              lstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              lstm_optimizer.step()\n",
    "            lstm.eval()\n",
    "        with torch.no_grad():\n",
    "          h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(lstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          test_outputs = torch.sigmoid(lstm(X_test, h_o, c_o)[0])\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          lstm_test_losses.append(test_loss)\n",
    "          lstm_test_accuracies.append(test_acc)\n",
    "          lstm_test_mcc.append(test_mcc)\n",
    "          lstm_test_f1.append(test_f1)\n",
    "          lstm_test_auprc.append(test_auprc)\n",
    "          lstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #lstm_train_losses.append(train_loss.item())\n",
    "          lstm_train_accuracies.append(train_acc)\n",
    "          lstm_train_mcc.append(train_mcc)\n",
    "          lstm_train_f1.append(train_f1)\n",
    "          lstm_train_auprc.append(train_auprc)\n",
    "          lstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(lstm_test_accuracies), np.mean(lstm_test_mcc), np.mean(lstm_test_f1), np.mean(lstm_test_auprc), np.mean(lstm_test_auroc))])[:-3])\n",
    "print('LSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(lstm_test_accuracies), np.std(lstm_test_mcc), np.std(lstm_test_f1), np.std(lstm_test_auprc), np.std(lstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "begUd48HCv5J",
    "outputId": "39401ca5-b9f5-452f-dac3-6a007f1e7723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.797004 & 0.377771 & 0.721987 & 0.697284 & 0.792703\n",
      "BiLSTM Deviations:  0.001053 & 0.003726 & 0.003293 & 0.002846 & 0.001569\n"
     ]
    }
   ],
   "source": [
    "bilstm_train_losses, bilstm_train_accuracies, bilstm_train_mcc, bilstm_train_f1, bilstm_train_auprc, bilstm_train_auroc = [], [], [], [], [], []\n",
    "bilstm_test_losses, bilstm_test_accuracies, bilstm_test_mcc, bilstm_test_f1, bilstm_test_auprc, bilstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        bilstm = BiLSTM(input_size=100).to(device)\n",
    "        bilstm_optimizer = optim.Adam(bilstm.parameters(), lr=BiLSTM_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            bilstm.train()\n",
    "            h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              bilstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = bilstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              bilstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              bilstm_optimizer.step()\n",
    "            bilstm.eval()\n",
    "        with torch.no_grad():\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(bilstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          test_outputs = torch.sigmoid(bilstm(X_test, h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          bilstm_test_losses.append(test_loss)\n",
    "          bilstm_test_accuracies.append(test_acc)\n",
    "          bilstm_test_mcc.append(test_mcc)\n",
    "          bilstm_test_f1.append(test_f1)\n",
    "          bilstm_test_auprc.append(test_auprc)\n",
    "          bilstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #bilstm_train_losses.append(train_loss.item())\n",
    "          bilstm_train_accuracies.append(train_acc)\n",
    "          bilstm_train_mcc.append(train_mcc)\n",
    "          bilstm_train_f1.append(train_f1)\n",
    "          bilstm_train_auprc.append(train_auprc)\n",
    "          bilstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(bilstm_test_accuracies), np.mean(bilstm_test_mcc), np.mean(bilstm_test_f1), np.mean(bilstm_test_auprc), np.mean(bilstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(bilstm_test_accuracies), np.std(bilstm_test_mcc), np.std(bilstm_test_f1), np.std(bilstm_test_auprc), np.std(bilstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "leffgdVUCv5K",
    "outputId": "9a1fbb2b-4078-4084-f59c-d8533f0f0f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.794051 & 0.359931 & 0.716901 & 0.681836 & 0.783718\n",
      "ConvLSTM Deviations:  0.001945 & 0.007371 & 0.005514 & 0.004873 & 0.003461\n"
     ]
    }
   ],
   "source": [
    "convlstm_train_losses, convlstm_train_accuracies, convlstm_train_mcc, convlstm_train_f1, convlstm_train_auprc, convlstm_train_auroc = [], [], [], [], [], []\n",
    "convlstm_test_losses, convlstm_test_accuracies, convlstm_test_mcc, convlstm_test_f1, convlstm_test_auprc, convlstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        convlstm = ConvLSTM(input_size=100).to(device)\n",
    "        convlstm_optimizer = optim.Adam(convlstm.parameters(), lr=ConvLSTM_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            convlstm.train()\n",
    "            h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              convlstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = convlstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              convlstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              convlstm_optimizer.step()\n",
    "            convlstm.eval()\n",
    "        with torch.no_grad():\n",
    "          h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(convlstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          test_outputs = torch.sigmoid(convlstm(X_test, h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          convlstm_test_losses.append(test_loss)\n",
    "          convlstm_test_accuracies.append(test_acc)\n",
    "          convlstm_test_mcc.append(test_mcc)\n",
    "          convlstm_test_f1.append(test_f1)\n",
    "          convlstm_test_auprc.append(test_auprc)\n",
    "          convlstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #convlstm_train_losses.append(train_loss.item())\n",
    "          convlstm_train_accuracies.append(train_acc)\n",
    "          convlstm_train_mcc.append(train_mcc)\n",
    "          convlstm_train_f1.append(train_f1)\n",
    "          convlstm_train_auprc.append(train_auprc)\n",
    "          convlstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convlstm_test_accuracies), np.mean(convlstm_test_mcc), np.mean(convlstm_test_f1), np.mean(convlstm_test_auprc), np.mean(convlstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convlstm_test_accuracies), np.std(convlstm_test_mcc), np.std(convlstm_test_f1), np.std(convlstm_test_auprc), np.std(convlstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     Model    |  Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "MLP Metrics:  0.765326 & 0.219426 & 0.638095 & 0.598131 & 0.71319\n",
      "MLP Deviations:  0.001351 & 0.004257 & 0.005639 & 0.001832 & 0.001444\n",
      "ConvNet Metrics:  0.782195 & 0.322166 & 0.689659 & 0.664868 & 0.764801\n",
      "ConvNet Deviations:  0.00123 & 0.00412 & 0.005031 & 0.003162 & 0.001491\n",
      "LSTM Metrics:  0.785243 & 0.323532 & 0.692422 & 0.663507 & 0.768399\n",
      "LSTM Deviations:  0.001398 & 0.003996 & 0.004624 & 0.002204 & 0.001395\n",
      "BiLSTM Metrics:  0.797004 & 0.377771 & 0.721987 & 0.697284 & 0.792703\n",
      "BiLSTM Deviations:  0.001053 & 0.003726 & 0.003293 & 0.002846 & 0.001569\n",
      "ConvLSTM Metrics:  0.794051 & 0.359931 & 0.716901 & 0.681836 & 0.783718\n",
      "ConvLSTM Deviations:  0.001945 & 0.007371 & 0.005514 & 0.004873 & 0.003461\n"
     ]
    }
   ],
   "source": [
    "print('|     Model    |  Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print('MLP Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(mlp_test_accuracies), np.mean(mlp_test_mcc), np.mean(mlp_test_f1), np.mean(mlp_test_auprc), np.mean(mlp_test_auroc))])[:-3])\n",
    "print('MLP Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(mlp_test_accuracies), np.std(mlp_test_mcc), np.std(mlp_test_f1), np.std(mlp_test_auprc), np.std(mlp_test_auroc))])[:-3])\n",
    "print('ConvNet Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convnet_test_accuracies), np.mean(convnet_test_mcc), np.mean(convnet_test_f1), np.mean(convnet_test_auprc), np.mean(convnet_test_auroc))])[:-3])\n",
    "print('ConvNet Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convnet_test_accuracies), np.std(convnet_test_mcc), np.std(convnet_test_f1), np.std(convnet_test_auprc), np.std(convnet_test_auroc))])[:-3])\n",
    "print('LSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(lstm_test_accuracies), np.mean(lstm_test_mcc), np.mean(lstm_test_f1), np.mean(lstm_test_auprc), np.mean(lstm_test_auroc))])[:-3])\n",
    "print('LSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(lstm_test_accuracies), np.std(lstm_test_mcc), np.std(lstm_test_f1), np.std(lstm_test_auprc), np.std(lstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(bilstm_test_accuracies), np.mean(bilstm_test_mcc), np.mean(bilstm_test_f1), np.mean(bilstm_test_auprc), np.mean(bilstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(bilstm_test_accuracies), np.std(bilstm_test_mcc), np.std(bilstm_test_f1), np.std(bilstm_test_auprc), np.std(bilstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convlstm_test_accuracies), np.mean(convlstm_test_mcc), np.mean(convlstm_test_f1), np.mean(convlstm_test_auprc), np.mean(convlstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convlstm_test_accuracies), np.std(convlstm_test_mcc), np.std(convlstm_test_f1), np.std(convlstm_test_auprc), np.std(convlstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TW NMF with Semantic Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Nmf, TfidfModel\n",
    "fcrnn = pd.read_csv('filtered_cleaned_raw_nursing_notes_processed.csv')\n",
    "fcrnn['processed_text'] = fcrnn['processed_text'].apply(eval)\n",
    "\n",
    "texts_dict = Dictionary(fcrnn['processed_text'])\n",
    "corpus = [texts_dict.doc2bow(text) for text in fcrnn['processed_text']]\n",
    "model = TfidfModel(corpus, smartirs='ltn')\n",
    "corpus = model[corpus]\n",
    "nmf = Nmf(corpus, num_topics=150, id2word=texts_dict, passes=10)\n",
    "\n",
    "\n",
    "cm = CoherenceModel(model=nmf, dictionary=texts_dict, coherence='c_v', texts=fcrnn['processed_text'])\n",
    "a = cm.get_coherence_per_topic()\n",
    "inds = np.argsort(a)[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_topics = nmf.get_document_topics(corpus, normalize = True)\n",
    "zs = gensim.matutils.corpus2csc(corp_topics).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_bow_topics = zs[inds]\n",
    "with open('nmf_tw_w_sc_topics.pkl', 'wb') as f:\n",
    "    pickle.dump(coherence_bow_topics.T, f)\n",
    "\n",
    "\n",
    "weights = np.array(a)[inds]\n",
    "weighted_topics = weights*np.array(topics)\n",
    "\n",
    "with open('nmf_tw_w_sc_topics_weighted.pkl', 'wb') as f:\n",
    "    pickle.dump(weighted_topics, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation with Unweighted Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('nmf_tw_w_sc_topics.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('y_labels.pkl', 'rb') as f:\n",
    "    y = pickle.load(f).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7NIOiaQxsuc",
    "outputId": "5957c44e-d1f6-4a04-9111-c8793cc9cb5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.7572   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.7436   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.6784   \u001b[39m | \u001b[39m0.0001114\u001b[39m |\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.7656   \u001b[39m | \u001b[35m0.0303   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.7703   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.7687   \u001b[39m | \u001b[39m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.77     \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.7632   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.7601   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.7505   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.7297   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.7371   \u001b[39m | \u001b[39m0.08497  \u001b[39m |\n",
      "| \u001b[35m13       \u001b[39m | \u001b[35m0.7706   \u001b[39m | \u001b[35m0.01447  \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.7486   \u001b[39m | \u001b[39m0.06269  \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.7663   \u001b[39m | \u001b[39m0.02438  \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.768    \u001b[39m | \u001b[39m0.01144  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.7551   \u001b[39m | \u001b[39m0.04814  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.7351   \u001b[39m | \u001b[39m0.09211  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.77     \u001b[39m | \u001b[39m0.01445  \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.7695   \u001b[39m | \u001b[39m0.01035  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.7695   \u001b[39m | \u001b[39m0.01962  \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.7702   \u001b[39m | \u001b[39m0.01771  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.7705   \u001b[39m | \u001b[39m0.01672  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.7678   \u001b[39m | \u001b[39m0.0207   \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.7679   \u001b[39m | \u001b[39m0.008213 \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.7674   \u001b[39m | \u001b[39m0.007066 \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.767    \u001b[39m | \u001b[39m0.02188  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.7671   \u001b[39m | \u001b[39m0.02314  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.7657   \u001b[39m | \u001b[39m0.005881 \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.7651   \u001b[39m | \u001b[39m0.03154  \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.7679   \u001b[39m | \u001b[39m0.02561  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.7662   \u001b[39m | \u001b[39m0.02675  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.7663   \u001b[39m | \u001b[39m0.02806  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.7658   \u001b[39m | \u001b[39m0.02916  \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.7636   \u001b[39m | \u001b[39m0.03294  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.7642   \u001b[39m | \u001b[39m0.004586 \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.7589   \u001b[39m | \u001b[39m0.003244 \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.7638   \u001b[39m | \u001b[39m0.03607  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.7612   \u001b[39m | \u001b[39m0.03744  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.7683   \u001b[39m | \u001b[39m0.01246  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for MLP(\n",
      "  (fc1): Linear(in_features=100, out_features=75, bias=True)\n",
      "  (fc2): Linear(in_features=75, out_features=19, bias=True)\n",
      ") {'target': 0.7705885004223384, 'params': {'lr': 0.014466885095628584}} =================\n",
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.7304   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.7549   \u001b[39m | \u001b[35m0.07206  \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.7427   \u001b[39m | \u001b[39m0.0001114\u001b[39m |\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.794    \u001b[39m | \u001b[35m0.0303   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.8197   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.8254   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.7985   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.7735   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.7745   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.7801   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.8223   \u001b[39m | \u001b[39m0.009617 \u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.8307   \u001b[39m | \u001b[35m0.006652 \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.7993   \u001b[39m | \u001b[39m0.02487  \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.7247   \u001b[39m | \u001b[39m0.1      \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.6419   \u001b[39m | \u001b[39m0.08541  \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.7678   \u001b[39m | \u001b[39m0.06198  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.7625   \u001b[39m | \u001b[39m0.06688  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.7802   \u001b[39m | \u001b[39m0.04963  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.8267   \u001b[39m | \u001b[39m0.009334 \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.8274   \u001b[39m | \u001b[39m0.006152 \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.8238   \u001b[39m | \u001b[39m0.007266 \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.8152   \u001b[39m | \u001b[39m0.01409  \u001b[39m |\n",
      "| \u001b[35m23       \u001b[39m | \u001b[35m0.8353   \u001b[39m | \u001b[35m0.00544  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.8312   \u001b[39m | \u001b[39m0.004798 \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.8353   \u001b[39m | \u001b[39m0.004053 \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.834    \u001b[39m | \u001b[39m0.003345 \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.8314   \u001b[39m | \u001b[39m0.002586 \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.8241   \u001b[39m | \u001b[39m0.001822 \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.8183   \u001b[39m | \u001b[39m0.01573  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.7944   \u001b[39m | \u001b[39m0.01671  \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.8238   \u001b[39m | \u001b[39m0.01302  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.8163   \u001b[39m | \u001b[39m0.01217  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.823    \u001b[39m | \u001b[39m0.01123  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.8031   \u001b[39m | \u001b[39m0.02326  \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.7985   \u001b[39m | \u001b[39m0.02193  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.8003   \u001b[39m | \u001b[39m0.02648  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.7899   \u001b[39m | \u001b[39m0.02795  \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.7962   \u001b[39m | \u001b[39m0.02031  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.7409   \u001b[39m | \u001b[39m0.09281  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.7465   \u001b[39m | \u001b[39m0.07855  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvNet(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=19, bias=True)\n",
      ") {'target': 0.8353145876000987, 'params': {'lr': 0.005440293686514306}} =================\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score, roc_auc_score, average_precision_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from bayes_opt import BayesianOptimization\n",
    "import time\n",
    "\n",
    "models = [\n",
    "    MLP(input_size=100),\n",
    "    ConvNet(input_size=100)\n",
    "]\n",
    "# BATCH_SIZE always 128, train for 8 epochs\n",
    "for model in models:\n",
    "  def eval_model(lr, model=model, X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    device = torch.device(0)\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(X_batch)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "      with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = torch.sigmoid(model(X_test)).cpu().numpy()\n",
    "        #predicted = (outputs > 0.6).float().cpu().numpy()\n",
    "        #print(train_losses)\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        #print(roc)\n",
    "        rocs.append(roc)\n",
    "    #print(rocs)\n",
    "    return np.mean(rocs)\n",
    "  # bayesian optimization on all 5 models....this might take a fews days? :3\n",
    "\n",
    "\n",
    "  bounds = {\n",
    "    'lr': (0.0001, 0.1)\n",
    "  }\n",
    "  optimizer = BayesianOptimization(\n",
    "      f=eval_model,\n",
    "      pbounds=bounds,\n",
    "      verbose=2,  # verbose = 1 prints only when a maximum\n",
    "      # is observed, verbose = 0 is silent\n",
    "      random_state=1,\n",
    "  )\n",
    "  optimizer.maximize(\n",
    "      init_points=10,\n",
    "      n_iter=30\n",
    "  )\n",
    "  print(f'================= BEST MODEL for {model}', optimizer.max, '=================')\n",
    "  if model.__class__.__name__ == 'MLP':\n",
    "      mlp_lr = optimizer.max['params']['lr']\n",
    "  elif model.__class__.__name__ == 'ConvNet':\n",
    "      convnet_lr = optimizer.max['params']['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "HSivLiGZTpsW",
    "outputId": "0b156917-346f-4b7d-e4d8-912389ca6f12",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.7585   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.7227   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.7148   \u001b[39m | \u001b[39m0.0001114\u001b[39m |\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.7841   \u001b[39m | \u001b[35m0.0303   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.8362   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.8457   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.8227   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.7787   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.773    \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.7543   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.7004   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.8456   \u001b[39m | \u001b[39m0.01153  \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.7103   \u001b[39m | \u001b[39m0.08538  \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.8451   \u001b[39m | \u001b[39m0.01033  \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m0.8479   \u001b[39m | \u001b[35m0.009235 \u001b[39m |\n",
      "| \u001b[35m16       \u001b[39m | \u001b[35m0.85     \u001b[39m | \u001b[35m0.006437 \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.8098   \u001b[39m | \u001b[39m0.02307  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.7342   \u001b[39m | \u001b[39m0.06254  \u001b[39m |\n",
      "| \u001b[35m19       \u001b[39m | \u001b[35m0.8502   \u001b[39m | \u001b[35m0.00645  \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.7574   \u001b[39m | \u001b[39m0.04788  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.7028   \u001b[39m | \u001b[39m0.0926   \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.7304   \u001b[39m | \u001b[39m0.0786   \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.806    \u001b[39m | \u001b[39m0.02642  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.8302   \u001b[39m | \u001b[39m0.01664  \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.7295   \u001b[39m | \u001b[39m0.06721  \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.8478   \u001b[39m | \u001b[39m0.007883 \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.7465   \u001b[39m | \u001b[39m0.05806  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.8113   \u001b[39m | \u001b[39m0.02086  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.8412   \u001b[39m | \u001b[39m0.01319  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.8394   \u001b[39m | \u001b[39m0.003894 \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.7801   \u001b[39m | \u001b[39m0.03711  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.7465   \u001b[39m | \u001b[39m0.05089  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.7686   \u001b[39m | \u001b[39m0.04492  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.8402   \u001b[39m | \u001b[39m0.00489  \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.8154   \u001b[39m | \u001b[39m0.02479  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.8484   \u001b[39m | \u001b[39m0.00858  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.7277   \u001b[39m | \u001b[39m0.07537  \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.7208   \u001b[39m | \u001b[39m0.08182  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.7097   \u001b[39m | \u001b[39m0.08891  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.7167   \u001b[39m | \u001b[39m0.09624  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for LSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc2): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.8501582503756279, 'params': {'lr': 0.006449657550313661}} =================\n",
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.7182   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01476  \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.5611   \u001b[39m | \u001b[39m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m0.7653   \u001b[39m | \u001b[35m0.0004262\u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.8059   \u001b[39m | \u001b[35m0.001151 \u001b[39m |\n",
      "| \u001b[35m13       \u001b[39m | \u001b[35m0.8154   \u001b[39m | \u001b[35m0.00299  \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.5583   \u001b[39m | \u001b[39m0.004876 \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m0.8345   \u001b[39m | \u001b[35m0.002212 \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09998  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08602  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06299  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.8202   \u001b[39m | \u001b[39m0.002998 \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.8143   \u001b[39m | \u001b[39m0.003464 \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.8265   \u001b[39m | \u001b[39m0.001745 \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.093    \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07905  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.5001   \u001b[39m | \u001b[39m0.04784  \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0245   \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06753  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05846  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07557  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.8083   \u001b[39m | \u001b[39m0.003245 \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.7149   \u001b[39m | \u001b[39m0.003881 \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.7879   \u001b[39m | \u001b[39m0.0008062\u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.8279   \u001b[39m | \u001b[39m0.001987 \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0895   \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.4999   \u001b[39m | \u001b[39m0.09647  \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08254  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.8146   \u001b[39m | \u001b[39m0.001442 \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0448   \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05088  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.02741  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.02161  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc3): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.8345407854656572, 'params': {'lr': 0.002212142778389413}} =================\n"
     ]
    }
   ],
   "source": [
    "lstm_models = [\n",
    "    LSTM(input_size=100),\n",
    "    ConvLSTM(input_size=100)\n",
    "]\n",
    "# BATCH_SIZE always 128, train for 8 epochs\n",
    "for model in lstm_models:\n",
    "  def eval_model(lr, model=model, X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    device = torch.device(\"cuda\")\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs, h_o, c_o = model(X_batch, h_o, c_o)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "      with torch.no_grad():\n",
    "        model.eval()\n",
    "        h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "        outputs = torch.sigmoid(model(X_test, h_o, c_o)[0]).cpu().numpy()\n",
    "        #predicted = (outputs > 0.5).float().cpu().numpy()\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        rocs.append(roc)\n",
    "\n",
    "    return np.mean(rocs)\n",
    "\n",
    "\n",
    "  bounds = {\n",
    "    'lr': (0.0001, 0.1)\n",
    "  }\n",
    "  optimizer = BayesianOptimization(\n",
    "      f=eval_model,\n",
    "      pbounds=bounds,\n",
    "      verbose=2,  # verbose = 1 prints only when a maximum\n",
    "      # is observed, verbose = 0 is silent\n",
    "      random_state=1,\n",
    "  )\n",
    "  optimizer.maximize(\n",
    "      init_points=10,\n",
    "      n_iter=30\n",
    "  )\n",
    "  print(f'================= BEST MODEL for {model}', optimizer.max, '=================')\n",
    "  if model.__class__.__name__ == 'LSTM':\n",
    "      LSTM_lr = optimizer.max['params']['lr']\n",
    "  elif model.__class__.__name__ == 'ConvLSTM':\n",
    "      ConvLSTM_lr = optimizer.max['params']['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WOQsv5u0Cv5I",
    "outputId": "ae6690b0-5612-4103-ab3a-ff782a46f71a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.7758   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.7408   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.722    \u001b[39m | \u001b[39m0.0001114\u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.7794   \u001b[39m | \u001b[35m0.0303   \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.8349   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.8726   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.8375   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.7642   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.7656   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.7547   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m0.873    \u001b[39m | \u001b[35m0.009068 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.7102   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.7174   \u001b[39m | \u001b[39m0.08517  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.8207   \u001b[39m | \u001b[39m0.02332  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m0.8766   \u001b[39m | \u001b[35m0.007589 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.749    \u001b[39m | \u001b[39m0.06286  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.8764   \u001b[39m | \u001b[39m0.00597  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m18       \u001b[39m | \u001b[35m0.8766   \u001b[39m | \u001b[35m0.006681 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.8765   \u001b[39m | \u001b[39m0.007602 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.8677   \u001b[39m | \u001b[39m0.01144  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.8736   \u001b[39m | \u001b[39m0.00523  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m22       \u001b[39m | \u001b[35m0.8774   \u001b[39m | \u001b[35m0.007157 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.762    \u001b[39m | \u001b[39m0.04778  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.7324   \u001b[39m | \u001b[39m0.09244  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.8769   \u001b[39m | \u001b[39m0.007177 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.877    \u001b[39m | \u001b[39m0.007106 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m27       \u001b[39m | \u001b[35m0.8776   \u001b[39m | \u001b[35m0.007213 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.8753   \u001b[39m | \u001b[39m0.008333 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.8691   \u001b[39m | \u001b[39m0.01085  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.7272   \u001b[39m | \u001b[39m0.07841  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.7962   \u001b[39m | \u001b[39m0.0265   \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.7434   \u001b[39m | \u001b[39m0.06742  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.7481   \u001b[39m | \u001b[39m0.05833  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.8243   \u001b[39m | \u001b[39m0.02084  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.8698   \u001b[39m | \u001b[39m0.01014  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.8767   \u001b[39m | \u001b[39m0.00732  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.8699   \u001b[39m | \u001b[39m0.004677 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.8633   \u001b[39m | \u001b[39m0.004081 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.865    \u001b[39m | \u001b[39m0.01206  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.8661   \u001b[39m | \u001b[39m0.01268  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc3): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.8776287954786094, 'params': {'lr': 0.007213163764721339}} =================\n"
     ]
    }
   ],
   "source": [
    "def eval_model(lr, model=BiLSTM(input_size=100), X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    print(model)\n",
    "    device = torch.device(\"cuda\")\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs, h_o, c_o = model(X_batch, h_o, c_o)\n",
    "          #print(outputs, y_batch)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "        outputs = torch.sigmoid(model(X_test, h_o, c_o)[0]).cpu().numpy()\n",
    "        #predicted = (outputs > 0.5).float().cpu().numpy()\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        rocs.append(roc)\n",
    "\n",
    "    return np.mean(rocs)\n",
    "\n",
    "\n",
    "bounds = {\n",
    "'lr': (0.0001, 0.1)\n",
    "}\n",
    "optimizer = BayesianOptimization(\n",
    "  f=eval_model,\n",
    "  pbounds=bounds,\n",
    "  verbose=2,  # verbose = 1 prints only when a maximum\n",
    "  # is observed, verbose = 0 is silent\n",
    "  random_state=1,\n",
    ")\n",
    "optimizer.maximize(\n",
    "  init_points=10,\n",
    "  n_iter=30\n",
    ")\n",
    "print(f'================= BEST MODEL for {model}', optimizer.max, '=================')\n",
    "BiLSTM_lr = optimizer.max['params']['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Learning Rate: 0.014466885095628584\n",
      "ConvNet Learning Rate: 0.005440293686514306\n",
      "LSTM Learning Rate: 0.006449657550313661\n",
      "BiLSTM Learning Rate: 0.007213163764721339\n",
      "ConvLSTM Learning Rate: 0.002212142778389413\n"
     ]
    }
   ],
   "source": [
    "print(f'MLP Learning Rate: {mlp_lr}')\n",
    "print(f'ConvNet Learning Rate: {convnet_lr}')\n",
    "print(f'LSTM Learning Rate: {LSTM_lr}')\n",
    "print(f'BiLSTM Learning Rate: {BiLSTM_lr}')\n",
    "print(f'ConvLSTM Learning Rate: {ConvLSTM_lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation with Optimal Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "splitter = ShuffleSplit(n_splits=5, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wnVxVni0Cv5J",
    "outputId": "475feeaf-c4fa-43c6-d324-508f02108ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.789379 & 0.292265 & 0.690685 & 0.652185 & 0.769955\n",
      "0.001643 & 0.00487 & 0.005955 & 0.002941 & 0.001589\n"
     ]
    }
   ],
   "source": [
    "#train mlp\n",
    "mlp_train_losses, mlp_train_accuracies, mlp_train_mcc, mlp_train_f1, mlp_train_auprc, mlp_train_auroc = [], [], [], [], [], []\n",
    "mlp_test_losses, mlp_test_accuracies, mlp_test_mcc, mlp_test_f1, mlp_test_auprc, mlp_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        mlp = MLP(input_size=100).to(device)\n",
    "        mlp_optimizer = optim.Adam(mlp.parameters(), lr=mlp_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            mlp.train()\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              mlp_optimizer.zero_grad()\n",
    "              outputs = mlp(X_batch)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              mlp_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              mlp_optimizer.step()\n",
    "        with torch.no_grad():\n",
    "          mlp.eval()\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(mlp(X_train))\n",
    "          test_outputs = torch.sigmoid(mlp(X_test))\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          mlp_test_losses.append(test_loss)\n",
    "          mlp_test_accuracies.append(test_acc)\n",
    "          mlp_test_mcc.append(test_mcc)\n",
    "          mlp_test_f1.append(test_f1)\n",
    "          mlp_test_auprc.append(test_auprc)\n",
    "          mlp_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #mlp_train_losses.append(train_loss.item())\n",
    "          mlp_train_accuracies.append(train_acc)\n",
    "          mlp_train_mcc.append(train_mcc)\n",
    "          mlp_train_f1.append(train_f1)\n",
    "          mlp_train_auprc.append(train_auprc)\n",
    "          mlp_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(mlp_test_accuracies), np.mean(mlp_test_mcc), np.mean(mlp_test_f1), np.mean(mlp_test_auprc), np.mean(mlp_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(mlp_test_accuracies), np.std(mlp_test_mcc), np.std(mlp_test_f1), np.std(mlp_test_auprc), np.std(mlp_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "u6cXl1fOCv5J",
    "outputId": "f35cfac6-a7dc-400f-b466-b0c82de27218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.815454 & 0.432307 & 0.74666 & 0.745764 & 0.83162\n",
      "0.004743 & 0.020319 & 0.010944 & 0.013321 & 0.007685\n"
     ]
    }
   ],
   "source": [
    "convnet_train_losses, convnet_train_accuracies, convnet_train_mcc, convnet_train_f1, convnet_train_auprc, convnet_train_auroc = [], [], [], [], [], []\n",
    "convnet_test_losses, convnet_test_accuracies, convnet_test_mcc, convnet_test_f1, convnet_test_auprc, convnet_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        convnet = ConvNet(input_size=100).to(device)\n",
    "        convnet_optimizer = optim.Adam(convnet.parameters(), lr=convnet_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            convnet.train()\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              convnet_optimizer.zero_grad()\n",
    "              outputs = convnet(X_batch)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              convnet_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              convnet_optimizer.step()\n",
    "        convnet.eval()\n",
    "        with torch.no_grad():\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(convnet(X_train))\n",
    "          test_outputs = torch.sigmoid(convnet(X_test))\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          convnet_test_losses.append(test_loss)\n",
    "          convnet_test_accuracies.append(test_acc)\n",
    "          convnet_test_mcc.append(test_mcc)\n",
    "          convnet_test_f1.append(test_f1)\n",
    "          convnet_test_auprc.append(test_auprc)\n",
    "          convnet_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #convnet_train_losses.append(train_loss.item())\n",
    "          convnet_train_accuracies.append(train_acc)\n",
    "          convnet_train_mcc.append(train_mcc)\n",
    "          convnet_train_f1.append(train_f1)\n",
    "          convnet_train_auprc.append(train_auprc)\n",
    "          convnet_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convnet_test_accuracies), np.mean(convnet_test_mcc), np.mean(convnet_test_f1), np.mean(convnet_test_auprc), np.mean(convnet_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(convnet_test_accuracies), np.std(convnet_test_mcc), np.std(convnet_test_f1), np.std(convnet_test_auprc), np.std(convnet_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jI38lwtCCv5J",
    "outputId": "5ea32f23-2d4c-4717-af89-6ca0a3caf8e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.825938 & 0.470638 & 0.761595 & 0.766705 & 0.849768\n",
      "LSTM Deviations:  0.001859 & 0.005436 & 0.00471 & 0.003554 & 0.00198\n"
     ]
    }
   ],
   "source": [
    "lstm_train_losses, lstm_train_accuracies, lstm_train_mcc, lstm_train_f1, lstm_train_auprc, lstm_train_auroc = [], [], [], [], [], []\n",
    "lstm_test_losses, lstm_test_accuracies, lstm_test_mcc, lstm_test_f1, lstm_test_auprc, lstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        #print(len(train))\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        lstm = LSTM(input_size=100).to(device)\n",
    "        lstm_optimizer = optim.Adam(lstm.parameters(), lr=LSTM_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            lstm.train()\n",
    "            h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              lstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = lstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              lstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              lstm_optimizer.step()\n",
    "            lstm.eval()\n",
    "        with torch.no_grad():\n",
    "          h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(lstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          test_outputs = torch.sigmoid(lstm(X_test, h_o, c_o)[0])\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          lstm_test_losses.append(test_loss)\n",
    "          lstm_test_accuracies.append(test_acc)\n",
    "          lstm_test_mcc.append(test_mcc)\n",
    "          lstm_test_f1.append(test_f1)\n",
    "          lstm_test_auprc.append(test_auprc)\n",
    "          lstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #lstm_train_losses.append(train_loss.item())\n",
    "          lstm_train_accuracies.append(train_acc)\n",
    "          lstm_train_mcc.append(train_mcc)\n",
    "          lstm_train_f1.append(train_f1)\n",
    "          lstm_train_auprc.append(train_auprc)\n",
    "          lstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(lstm_test_accuracies), np.mean(lstm_test_mcc), np.mean(lstm_test_f1), np.mean(lstm_test_auprc), np.mean(lstm_test_auroc))])[:-3])\n",
    "print('LSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(lstm_test_accuracies), np.std(lstm_test_mcc), np.std(lstm_test_f1), np.std(lstm_test_auprc), np.std(lstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "begUd48HCv5J",
    "outputId": "39401ca5-b9f5-452f-dac3-6a007f1e7723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.844622 & 0.537384 & 0.795569 & 0.80948 & 0.877271\n",
      "BiLSTM Deviations:  0.001511 & 0.004051 & 0.003176 & 0.003218 & 0.001451\n"
     ]
    }
   ],
   "source": [
    "bilstm_train_losses, bilstm_train_accuracies, bilstm_train_mcc, bilstm_train_f1, bilstm_train_auprc, bilstm_train_auroc = [], [], [], [], [], []\n",
    "bilstm_test_losses, bilstm_test_accuracies, bilstm_test_mcc, bilstm_test_f1, bilstm_test_auprc, bilstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        bilstm = BiLSTM(input_size=100).to(device)\n",
    "        bilstm_optimizer = optim.Adam(bilstm.parameters(), lr=BiLSTM_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            bilstm.train()\n",
    "            h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              bilstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = bilstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              bilstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              bilstm_optimizer.step()\n",
    "            bilstm.eval()\n",
    "        with torch.no_grad():\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(bilstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          test_outputs = torch.sigmoid(bilstm(X_test, h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          bilstm_test_losses.append(test_loss)\n",
    "          bilstm_test_accuracies.append(test_acc)\n",
    "          bilstm_test_mcc.append(test_mcc)\n",
    "          bilstm_test_f1.append(test_f1)\n",
    "          bilstm_test_auprc.append(test_auprc)\n",
    "          bilstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #bilstm_train_losses.append(train_loss.item())\n",
    "          bilstm_train_accuracies.append(train_acc)\n",
    "          bilstm_train_mcc.append(train_mcc)\n",
    "          bilstm_train_f1.append(train_f1)\n",
    "          bilstm_train_auprc.append(train_auprc)\n",
    "          bilstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(bilstm_test_accuracies), np.mean(bilstm_test_mcc), np.mean(bilstm_test_f1), np.mean(bilstm_test_auprc), np.mean(bilstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(bilstm_test_accuracies), np.std(bilstm_test_mcc), np.std(bilstm_test_f1), np.std(bilstm_test_auprc), np.std(bilstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "leffgdVUCv5K",
    "outputId": "9a1fbb2b-4078-4084-f59c-d8533f0f0f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.817795 & 0.423523 & 0.745519 & 0.736638 & 0.83263\n",
      "ConvLSTM Deviations:  0.005097 & 0.023687 & 0.011807 & 0.013974 & 0.009396\n"
     ]
    }
   ],
   "source": [
    "convlstm_train_losses, convlstm_train_accuracies, convlstm_train_mcc, convlstm_train_f1, convlstm_train_auprc, convlstm_train_auroc = [], [], [], [], [], []\n",
    "convlstm_test_losses, convlstm_test_accuracies, convlstm_test_mcc, convlstm_test_f1, convlstm_test_auprc, convlstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        convlstm = ConvLSTM(input_size=100).to(device)\n",
    "        convlstm_optimizer = optim.Adam(convlstm.parameters(), lr=ConvLSTM_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            convlstm.train()\n",
    "            h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              convlstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = convlstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              convlstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              convlstm_optimizer.step()\n",
    "            convlstm.eval()\n",
    "        with torch.no_grad():\n",
    "          h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(convlstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          test_outputs = torch.sigmoid(convlstm(X_test, h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          convlstm_test_losses.append(test_loss)\n",
    "          convlstm_test_accuracies.append(test_acc)\n",
    "          convlstm_test_mcc.append(test_mcc)\n",
    "          convlstm_test_f1.append(test_f1)\n",
    "          convlstm_test_auprc.append(test_auprc)\n",
    "          convlstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #convlstm_train_losses.append(train_loss.item())\n",
    "          convlstm_train_accuracies.append(train_acc)\n",
    "          convlstm_train_mcc.append(train_mcc)\n",
    "          convlstm_train_f1.append(train_f1)\n",
    "          convlstm_train_auprc.append(train_auprc)\n",
    "          convlstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convlstm_test_accuracies), np.mean(convlstm_test_mcc), np.mean(convlstm_test_f1), np.mean(convlstm_test_auprc), np.mean(convlstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convlstm_test_accuracies), np.std(convlstm_test_mcc), np.std(convlstm_test_f1), np.std(convlstm_test_auprc), np.std(convlstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     Model    |  Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "MLP Metrics:  0.789379 & 0.292265 & 0.690685 & 0.652185 & 0.769955\n",
      "MLP Deviations:  0.001643 & 0.00487 & 0.005955 & 0.002941 & 0.001589\n",
      "ConvNet Metrics:  0.815454 & 0.432307 & 0.74666 & 0.745764 & 0.83162\n",
      "ConvNet Deviations:  0.004743 & 0.020319 & 0.010944 & 0.013321 & 0.007685\n",
      "LSTM Metrics:  0.825938 & 0.470638 & 0.761595 & 0.766705 & 0.849768\n",
      "LSTM Deviations:  0.001859 & 0.005436 & 0.00471 & 0.003554 & 0.00198\n",
      "BiLSTM Metrics:  0.844622 & 0.537384 & 0.795569 & 0.80948 & 0.877271\n",
      "BiLSTM Deviations:  0.001511 & 0.004051 & 0.003176 & 0.003218 & 0.001451\n",
      "ConvLSTM Metrics:  0.817795 & 0.423523 & 0.745519 & 0.736638 & 0.83263\n",
      "ConvLSTM Deviations:  0.005097 & 0.023687 & 0.011807 & 0.013974 & 0.009396\n"
     ]
    }
   ],
   "source": [
    "print('|     Model    |  Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print('MLP Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(mlp_test_accuracies), np.mean(mlp_test_mcc), np.mean(mlp_test_f1), np.mean(mlp_test_auprc), np.mean(mlp_test_auroc))])[:-3])\n",
    "print('MLP Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(mlp_test_accuracies), np.std(mlp_test_mcc), np.std(mlp_test_f1), np.std(mlp_test_auprc), np.std(mlp_test_auroc))])[:-3])\n",
    "print('ConvNet Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convnet_test_accuracies), np.mean(convnet_test_mcc), np.mean(convnet_test_f1), np.mean(convnet_test_auprc), np.mean(convnet_test_auroc))])[:-3])\n",
    "print('ConvNet Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convnet_test_accuracies), np.std(convnet_test_mcc), np.std(convnet_test_f1), np.std(convnet_test_auprc), np.std(convnet_test_auroc))])[:-3])\n",
    "print('LSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(lstm_test_accuracies), np.mean(lstm_test_mcc), np.mean(lstm_test_f1), np.mean(lstm_test_auprc), np.mean(lstm_test_auroc))])[:-3])\n",
    "print('LSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(lstm_test_accuracies), np.std(lstm_test_mcc), np.std(lstm_test_f1), np.std(lstm_test_auprc), np.std(lstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(bilstm_test_accuracies), np.mean(bilstm_test_mcc), np.mean(bilstm_test_f1), np.mean(bilstm_test_auprc), np.mean(bilstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(bilstm_test_accuracies), np.std(bilstm_test_mcc), np.std(bilstm_test_f1), np.std(bilstm_test_auprc), np.std(bilstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convlstm_test_accuracies), np.mean(convlstm_test_mcc), np.mean(convlstm_test_f1), np.mean(convlstm_test_auprc), np.mean(convlstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convlstm_test_accuracies), np.std(convlstm_test_mcc), np.std(convlstm_test_f1), np.std(convlstm_test_auprc), np.std(convlstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation with Weighted Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('nmf_tw_w_sc_topics_weighted.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('y_labels.pkl', 'rb') as f:\n",
    "    y = pickle.load(f).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7NIOiaQxsuc",
    "outputId": "5957c44e-d1f6-4a04-9111-c8793cc9cb5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.7442   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.7313   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.6506   \u001b[39m | \u001b[39m0.0001114\u001b[39m |\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.7479   \u001b[39m | \u001b[35m0.0303   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.752    \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.752    \u001b[39m | \u001b[39m0.009325 \u001b[39m |\n",
      "| \u001b[35m7        \u001b[39m | \u001b[35m0.7534   \u001b[39m | \u001b[35m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.748    \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.7457   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.7405   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.7203   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.7274   \u001b[39m | \u001b[39m0.08511  \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.751    \u001b[39m | \u001b[39m0.01902  \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.7369   \u001b[39m | \u001b[39m0.06232  \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.74     \u001b[39m | \u001b[39m0.04811  \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.7231   \u001b[39m | \u001b[39m0.09224  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.7288   \u001b[39m | \u001b[39m0.07847  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.752    \u001b[39m | \u001b[39m0.02593  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.7514   \u001b[39m | \u001b[39m0.01872  \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.7517   \u001b[39m | \u001b[39m0.01837  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.7506   \u001b[39m | \u001b[39m0.01943  \u001b[39m |\n",
      "| \u001b[35m22       \u001b[39m | \u001b[35m0.7537   \u001b[39m | \u001b[35m0.01521  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.7508   \u001b[39m | \u001b[39m0.02546  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.7515   \u001b[39m | \u001b[39m0.0098   \u001b[39m |\n",
      "| \u001b[35m25       \u001b[39m | \u001b[35m0.7538   \u001b[39m | \u001b[35m0.0157   \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.7529   \u001b[39m | \u001b[39m0.01619  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.7521   \u001b[39m | \u001b[39m0.0167   \u001b[39m |\n",
      "| \u001b[35m28       \u001b[39m | \u001b[35m0.7542   \u001b[39m | \u001b[35m0.01724  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.7519   \u001b[39m | \u001b[39m0.01769  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.7511   \u001b[39m | \u001b[39m0.02649  \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.7507   \u001b[39m | \u001b[39m0.008763 \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.7541   \u001b[39m | \u001b[39m0.01418  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.7507   \u001b[39m | \u001b[39m0.01366  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.7508   \u001b[39m | \u001b[39m0.0104   \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.7506   \u001b[39m | \u001b[39m0.02707  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.7527   \u001b[39m | \u001b[39m0.01098  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.7517   \u001b[39m | \u001b[39m0.01152  \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.7527   \u001b[39m | \u001b[39m0.01211  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.7527   \u001b[39m | \u001b[39m0.01267  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.7524   \u001b[39m | \u001b[39m0.01314  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for MLP(\n",
      "  (fc1): Linear(in_features=100, out_features=75, bias=True)\n",
      "  (fc2): Linear(in_features=75, out_features=19, bias=True)\n",
      ") {'target': 0.7542298691181013, 'params': {'lr': 0.017238121577126658}} =================\n",
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.7544   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.7452   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.7208   \u001b[39m | \u001b[39m0.0001114\u001b[39m |\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.7743   \u001b[39m | \u001b[35m0.0303   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.7893   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.7967   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.781    \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.7696   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.7714   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.7585   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m0.804    \u001b[39m | \u001b[35m0.009314 \u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.8048   \u001b[39m | \u001b[35m0.009266 \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.7852   \u001b[39m | \u001b[39m0.01466  \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.7911   \u001b[39m | \u001b[39m0.009167 \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.8022   \u001b[39m | \u001b[39m0.009067 \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.8038   \u001b[39m | \u001b[39m0.008987 \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.7949   \u001b[39m | \u001b[39m0.008891 \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.7961   \u001b[39m | \u001b[39m0.008765 \u001b[39m |\n",
      "| \u001b[35m19       \u001b[39m | \u001b[35m0.8049   \u001b[39m | \u001b[35m0.008652 \u001b[39m |\n",
      "| \u001b[35m20       \u001b[39m | \u001b[35m0.8109   \u001b[39m | \u001b[35m0.008566 \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.7987   \u001b[39m | \u001b[39m0.008466 \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.8015   \u001b[39m | \u001b[39m0.008319 \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.805    \u001b[39m | \u001b[39m0.008201 \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.8005   \u001b[39m | \u001b[39m0.008092 \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.8      \u001b[39m | \u001b[39m0.007972 \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.8056   \u001b[39m | \u001b[39m0.007864 \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.7977   \u001b[39m | \u001b[39m0.007753 \u001b[39m |\n",
      "| \u001b[35m28       \u001b[39m | \u001b[35m0.8112   \u001b[39m | \u001b[35m0.007602 \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.8093   \u001b[39m | \u001b[39m0.007505 \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.7979   \u001b[39m | \u001b[39m0.007393 \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.8068   \u001b[39m | \u001b[39m0.007224 \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.795    \u001b[39m | \u001b[39m0.007102 \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.8004   \u001b[39m | \u001b[39m0.006903 \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.8088   \u001b[39m | \u001b[39m0.006762 \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.8053   \u001b[39m | \u001b[39m0.006662 \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.8059   \u001b[39m | \u001b[39m0.006535 \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.8104   \u001b[39m | \u001b[39m0.006418 \u001b[39m |\n",
      "| \u001b[35m38       \u001b[39m | \u001b[35m0.8126   \u001b[39m | \u001b[35m0.006312 \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.8104   \u001b[39m | \u001b[39m0.006211 \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.8105   \u001b[39m | \u001b[39m0.006086 \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvNet(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=19, bias=True)\n",
      ") {'target': 0.8125948124976645, 'params': {'lr': 0.006312312212514831}} =================\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score, roc_auc_score, average_precision_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from bayes_opt import BayesianOptimization\n",
    "import time\n",
    "\n",
    "models = [\n",
    "    MLP(input_size=100),\n",
    "    ConvNet(input_size=100)\n",
    "]\n",
    "# BATCH_SIZE always 128, train for 8 epochs\n",
    "for model in models:\n",
    "  def eval_model(lr, model=model, X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    device = torch.device(0)\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(X_batch)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "      with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = torch.sigmoid(model(X_test)).cpu().numpy()\n",
    "        #predicted = (outputs > 0.6).float().cpu().numpy()\n",
    "        #print(train_losses)\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        #print(roc)\n",
    "        rocs.append(roc)\n",
    "    #print(rocs)\n",
    "    return np.mean(rocs)\n",
    "  # bayesian optimization on all 5 models....this might take a fews days? :3\n",
    "\n",
    "\n",
    "  bounds = {\n",
    "    'lr': (0.0001, 0.1)\n",
    "  }\n",
    "  optimizer = BayesianOptimization(\n",
    "      f=eval_model,\n",
    "      pbounds=bounds,\n",
    "      verbose=2,  # verbose = 1 prints only when a maximum\n",
    "      # is observed, verbose = 0 is silent\n",
    "      random_state=1,\n",
    "  )\n",
    "  optimizer.maximize(\n",
    "      init_points=10,\n",
    "      n_iter=30\n",
    "  )\n",
    "  print(f'================= BEST MODEL for {model}', optimizer.max, '=================')\n",
    "  if model.__class__.__name__ == 'MLP':\n",
    "      mlp_lr = optimizer.max['params']['lr']\n",
    "  elif model.__class__.__name__ == 'ConvNet':\n",
    "      convnet_lr = optimizer.max['params']['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "HSivLiGZTpsW",
    "outputId": "0b156917-346f-4b7d-e4d8-912389ca6f12",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.7413   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.7154   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.6946   \u001b[39m | \u001b[39m0.0001114\u001b[39m |\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.7627   \u001b[39m | \u001b[35m0.0303   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.8139   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.821    \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.7856   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.7498   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.7427   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.7242   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.6922   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.8189   \u001b[39m | \u001b[39m0.01165  \u001b[39m |\n",
      "| \u001b[35m13       \u001b[39m | \u001b[35m0.8223   \u001b[39m | \u001b[35m0.009111 \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.6983   \u001b[39m | \u001b[39m0.0854   \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.7155   \u001b[39m | \u001b[39m0.06284  \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.8196   \u001b[39m | \u001b[39m0.006261 \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.771    \u001b[39m | \u001b[39m0.02444  \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.8222   \u001b[39m | \u001b[39m0.00758  \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.8211   \u001b[39m | \u001b[39m0.007591 \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.8171   \u001b[39m | \u001b[39m0.01307  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.8086   \u001b[39m | \u001b[39m0.01614  \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.819    \u001b[39m | \u001b[39m0.01063  \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.7895   \u001b[39m | \u001b[39m0.02093  \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.809    \u001b[39m | \u001b[39m0.004309 \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.7954   \u001b[39m | \u001b[39m0.002966 \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.6883   \u001b[39m | \u001b[39m0.09267  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.6931   \u001b[39m | \u001b[39m0.07866  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.7152   \u001b[39m | \u001b[39m0.04769  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.76     \u001b[39m | \u001b[39m0.02714  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.722    \u001b[39m | \u001b[39m0.06745  \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.7775   \u001b[39m | \u001b[39m0.02254  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.711    \u001b[39m | \u001b[39m0.05832  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.8211   \u001b[39m | \u001b[39m0.006903 \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.8158   \u001b[39m | \u001b[39m0.0053   \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.8021   \u001b[39m | \u001b[39m0.01732  \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.6738   \u001b[39m | \u001b[39m0.09634  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.6989   \u001b[39m | \u001b[39m0.08899  \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.7012   \u001b[39m | \u001b[39m0.07526  \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.7457   \u001b[39m | \u001b[39m0.04451  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.8158   \u001b[39m | \u001b[39m0.0139   \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for LSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc2): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.8222767021845458, 'params': {'lr': 0.00911121244640684}} =================\n",
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m0.07206  \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.6928   \u001b[39m | \u001b[35m0.0001114\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0303   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01476  \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.009325 \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m0.784    \u001b[39m | \u001b[35m0.001508 \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.7688   \u001b[39m | \u001b[39m0.003455 \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08602  \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m0.7863   \u001b[39m | \u001b[35m0.001572 \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06299  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07902  \u001b[39m |\n",
      "| \u001b[35m18       \u001b[39m | \u001b[35m0.8052   \u001b[39m | \u001b[35m0.002447 \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09296  \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04797  \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.02452  \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.7971   \u001b[39m | \u001b[39m0.002629 \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.4999   \u001b[39m | \u001b[39m0.005633 \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05846  \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06751  \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.09647  \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08252  \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.08949  \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.07555  \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.7932   \u001b[39m | \u001b[39m0.002159 \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.04484  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.05094  \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.02161  \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0274   \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.5776   \u001b[39m | \u001b[39m0.004138 \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01205  \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0372   \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.7595   \u001b[39m | \u001b[39m0.0008163\u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06979  \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.06523  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc3): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.8052099681148311, 'params': {'lr': 0.00244683981412298}} =================\n"
     ]
    }
   ],
   "source": [
    "lstm_models = [\n",
    "    LSTM(input_size=100),\n",
    "    ConvLSTM(input_size=100)\n",
    "]\n",
    "# BATCH_SIZE always 128, train for 8 epochs\n",
    "for model in lstm_models:\n",
    "  def eval_model(lr, model=model, X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    device = torch.device(\"cuda\")\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs, h_o, c_o = model(X_batch, h_o, c_o)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "      with torch.no_grad():\n",
    "        model.eval()\n",
    "        h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "        outputs = torch.sigmoid(model(X_test, h_o, c_o)[0]).cpu().numpy()\n",
    "        #predicted = (outputs > 0.5).float().cpu().numpy()\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        rocs.append(roc)\n",
    "\n",
    "    return np.mean(rocs)\n",
    "\n",
    "\n",
    "  bounds = {\n",
    "    'lr': (0.0001, 0.1)\n",
    "  }\n",
    "  optimizer = BayesianOptimization(\n",
    "      f=eval_model,\n",
    "      pbounds=bounds,\n",
    "      verbose=2,  # verbose = 1 prints only when a maximum\n",
    "      # is observed, verbose = 0 is silent\n",
    "      random_state=1,\n",
    "  )\n",
    "  optimizer.maximize(\n",
    "      init_points=10,\n",
    "      n_iter=30\n",
    "  )\n",
    "  print(f'================= BEST MODEL for {model}', optimizer.max, '=================')\n",
    "  if model.__class__.__name__ == 'LSTM':\n",
    "      LSTM_lr = optimizer.max['params']['lr']\n",
    "  elif model.__class__.__name__ == 'ConvLSTM':\n",
    "      ConvLSTM_lr = optimizer.max['params']['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "WOQsv5u0Cv5I",
    "outputId": "ae6690b0-5612-4103-ab3a-ff782a46f71a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.7427   \u001b[39m | \u001b[39m0.04176  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.6849   \u001b[39m | \u001b[39m0.07206  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.7031   \u001b[39m | \u001b[39m0.0001114\u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.7515   \u001b[39m | \u001b[35m0.0303   \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.8202   \u001b[39m | \u001b[35m0.01476  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.8459   \u001b[39m | \u001b[35m0.009325 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.7945   \u001b[39m | \u001b[39m0.01871  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.03462  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.7095   \u001b[39m | \u001b[39m0.03974  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.7326   \u001b[39m | \u001b[39m0.05393  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.8449   \u001b[39m | \u001b[39m0.009585 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.6959   \u001b[39m | \u001b[39m0.09999  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.6987   \u001b[39m | \u001b[39m0.08625  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m14       \u001b[39m | \u001b[35m0.8476   \u001b[39m | \u001b[35m0.006304 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.7251   \u001b[39m | \u001b[39m0.06225  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.7207   \u001b[39m | \u001b[39m0.04747  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.7796   \u001b[39m | \u001b[39m0.0239   \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[35m18       \u001b[39m | \u001b[35m0.8492   \u001b[39m | \u001b[35m0.007476 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.8485   \u001b[39m | \u001b[39m0.007489 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.8363   \u001b[39m | \u001b[39m0.0124   \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.6708   \u001b[39m | \u001b[39m0.07925  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.7112   \u001b[39m | \u001b[39m0.0931   \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.7931   \u001b[39m | \u001b[39m0.02121  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.6933   \u001b[39m | \u001b[39m0.06682  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.7889   \u001b[39m | \u001b[39m0.02688  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.8158   \u001b[39m | \u001b[39m0.0166   \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.7132   \u001b[39m | \u001b[39m0.05806  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.8277   \u001b[39m | \u001b[39m0.003867 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.8435   \u001b[39m | \u001b[39m0.01118  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.7331   \u001b[39m | \u001b[39m0.05081  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.7348   \u001b[39m | \u001b[39m0.04442  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.6816   \u001b[39m | \u001b[39m0.08976  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.6935   \u001b[39m | \u001b[39m0.09638  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.8069   \u001b[39m | \u001b[39m0.002639 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.8477   \u001b[39m | \u001b[39m0.006872 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.8389   \u001b[39m | \u001b[39m0.005123 \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.7073   \u001b[39m | \u001b[39m0.08293  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.7077   \u001b[39m | \u001b[39m0.07559  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.8368   \u001b[39m | \u001b[39m0.01355  \u001b[39m |\n",
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300, bidirectional=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19, bias=True)\n",
      ")\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.7261   \u001b[39m | \u001b[39m0.03696  \u001b[39m |\n",
      "=====================================\n",
      "================= BEST MODEL for ConvLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=289, bias=True)\n",
      "  (conv1): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc2): Linear(in_features=4275, out_features=289, bias=True)\n",
      "  (lstm): LSTM(289, 300)\n",
      "  (fc3): Linear(in_features=300, out_features=19, bias=True)\n",
      ") {'target': 0.849153606048287, 'params': {'lr': 0.00747649777158413}} =================\n"
     ]
    }
   ],
   "source": [
    "def eval_model(lr, model=BiLSTM(input_size=100), X = X, y = y, epochs = 8):\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2) # 5 fold CV as specified in paper\n",
    "    print(model)\n",
    "    device = torch.device(\"cuda\")\n",
    "    rocs = []\n",
    "    for train, test in splitter.split(X, y):\n",
    "      for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "          layer.reset_parameters()\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      model.to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "      criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "      epochs_needed = []\n",
    "      X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "      y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "      X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "      y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "      dataset_train = TensorDataset(X_train, y_train)\n",
    "      patience = 5\n",
    "      loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "      best_val_acc = 0.0\n",
    "      train_losses, train_accuracies = [], []\n",
    "      test_losses, test_accuracies = [], []\n",
    "\n",
    "      best_loss = float('inf')\n",
    "      # epochs\n",
    "      for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # train for batches\n",
    "        h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "        for X_batch, y_batch in loader_train:\n",
    "          optimizer.zero_grad()\n",
    "          outputs, h_o, c_o = model(X_batch, h_o, c_o)\n",
    "          #print(outputs, y_batch)\n",
    "          loss = criterion(outputs, y_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "        outputs = torch.sigmoid(model(X_test, h_o, c_o)[0]).cpu().numpy()\n",
    "        #predicted = (outputs > 0.5).float().cpu().numpy()\n",
    "        roc = roc_auc_score(y_test.cpu().numpy(), outputs)\n",
    "        rocs.append(roc)\n",
    "\n",
    "    return np.mean(rocs)\n",
    "\n",
    "\n",
    "bounds = {\n",
    "'lr': (0.0001, 0.1)\n",
    "}\n",
    "optimizer = BayesianOptimization(\n",
    "  f=eval_model,\n",
    "  pbounds=bounds,\n",
    "  verbose=2,  # verbose = 1 prints only when a maximum\n",
    "  # is observed, verbose = 0 is silent\n",
    "  random_state=1,\n",
    ")\n",
    "optimizer.maximize(\n",
    "  init_points=10,\n",
    "  n_iter=30\n",
    ")\n",
    "print(f'================= BEST MODEL for {model}', optimizer.max, '=================')\n",
    "BiLSTM_lr = optimizer.max['params']['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Learning Rate: 0.017238121577126658\n",
      "ConvNet Learning Rate: 0.006312312212514831\n",
      "LSTM Learning Rate: 0.00911121244640684\n",
      "BiLSTM Learning Rate: 0.00747649777158413\n",
      "ConvLSTM Learning Rate: 0.00244683981412298\n"
     ]
    }
   ],
   "source": [
    "print(f'MLP Learning Rate: {mlp_lr}')\n",
    "print(f'ConvNet Learning Rate: {convnet_lr}')\n",
    "print(f'LSTM Learning Rate: {LSTM_lr}')\n",
    "print(f'BiLSTM Learning Rate: {BiLSTM_lr}')\n",
    "print(f'ConvLSTM Learning Rate: {ConvLSTM_lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation with Optimal Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "splitter = ShuffleSplit(n_splits=5, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "wnVxVni0Cv5J",
    "outputId": "475feeaf-c4fa-43c6-d324-508f02108ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.779924 & 0.258377 & 0.666219 & 0.629591 & 0.752807\n",
      "0.002279 & 0.00568 & 0.008745 & 0.003223 & 0.00209\n"
     ]
    }
   ],
   "source": [
    "#train mlp\n",
    "mlp_train_losses, mlp_train_accuracies, mlp_train_mcc, mlp_train_f1, mlp_train_auprc, mlp_train_auroc = [], [], [], [], [], []\n",
    "mlp_test_losses, mlp_test_accuracies, mlp_test_mcc, mlp_test_f1, mlp_test_auprc, mlp_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        mlp = MLP(input_size=100).to(device)\n",
    "        mlp_optimizer = optim.Adam(mlp.parameters(), lr=mlp_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            mlp.train()\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              mlp_optimizer.zero_grad()\n",
    "              outputs = mlp(X_batch)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              mlp_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              mlp_optimizer.step()\n",
    "        with torch.no_grad():\n",
    "          mlp.eval()\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(mlp(X_train))\n",
    "          test_outputs = torch.sigmoid(mlp(X_test))\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          mlp_test_losses.append(test_loss)\n",
    "          mlp_test_accuracies.append(test_acc)\n",
    "          mlp_test_mcc.append(test_mcc)\n",
    "          mlp_test_f1.append(test_f1)\n",
    "          mlp_test_auprc.append(test_auprc)\n",
    "          mlp_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #mlp_train_losses.append(train_loss.item())\n",
    "          mlp_train_accuracies.append(train_acc)\n",
    "          mlp_train_mcc.append(train_mcc)\n",
    "          mlp_train_f1.append(train_f1)\n",
    "          mlp_train_auprc.append(train_auprc)\n",
    "          mlp_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(mlp_test_accuracies), np.mean(mlp_test_mcc), np.mean(mlp_test_f1), np.mean(mlp_test_auprc), np.mean(mlp_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(mlp_test_accuracies), np.std(mlp_test_mcc), np.std(mlp_test_f1), np.std(mlp_test_auprc), np.std(mlp_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "u6cXl1fOCv5J",
    "outputId": "f35cfac6-a7dc-400f-b466-b0c82de27218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.800874 & 0.378678 & 0.716682 & 0.709095 & 0.807428\n",
      "0.004366 & 0.024364 & 0.01115 & 0.016473 & 0.009811\n"
     ]
    }
   ],
   "source": [
    "convnet_train_losses, convnet_train_accuracies, convnet_train_mcc, convnet_train_f1, convnet_train_auprc, convnet_train_auroc = [], [], [], [], [], []\n",
    "convnet_test_losses, convnet_test_accuracies, convnet_test_mcc, convnet_test_f1, convnet_test_auprc, convnet_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        convnet = ConvNet(input_size=100).to(device)\n",
    "        convnet_optimizer = optim.Adam(convnet.parameters(), lr=convnet_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            convnet.train()\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              convnet_optimizer.zero_grad()\n",
    "              outputs = convnet(X_batch)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              convnet_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              convnet_optimizer.step()\n",
    "        convnet.eval()\n",
    "        with torch.no_grad():\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(convnet(X_train))\n",
    "          test_outputs = torch.sigmoid(convnet(X_test))\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          convnet_test_losses.append(test_loss)\n",
    "          convnet_test_accuracies.append(test_acc)\n",
    "          convnet_test_mcc.append(test_mcc)\n",
    "          convnet_test_f1.append(test_f1)\n",
    "          convnet_test_auprc.append(test_auprc)\n",
    "          convnet_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #convnet_train_losses.append(train_loss.item())\n",
    "          convnet_train_accuracies.append(train_acc)\n",
    "          convnet_train_mcc.append(train_mcc)\n",
    "          convnet_train_f1.append(train_f1)\n",
    "          convnet_train_auprc.append(train_auprc)\n",
    "          convnet_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convnet_test_accuracies), np.mean(convnet_test_mcc), np.mean(convnet_test_f1), np.mean(convnet_test_auprc), np.mean(convnet_test_auroc))])[:-3])\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.std(convnet_test_accuracies), np.std(convnet_test_mcc), np.std(convnet_test_f1), np.std(convnet_test_auprc), np.std(convnet_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "jI38lwtCCv5J",
    "outputId": "5ea32f23-2d4c-4717-af89-6ca0a3caf8e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.809375 & 0.410421 & 0.730971 & 0.726321 & 0.820975\n",
      "LSTM Deviations:  0.001923 & 0.006713 & 0.005815 & 0.004596 & 0.002354\n"
     ]
    }
   ],
   "source": [
    "lstm_train_losses, lstm_train_accuracies, lstm_train_mcc, lstm_train_f1, lstm_train_auprc, lstm_train_auroc = [], [], [], [], [], []\n",
    "lstm_test_losses, lstm_test_accuracies, lstm_test_mcc, lstm_test_f1, lstm_test_auprc, lstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        #print(len(train))\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        lstm = LSTM(input_size=100).to(device)\n",
    "        lstm_optimizer = optim.Adam(lstm.parameters(), lr=LSTM_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            lstm.train()\n",
    "            h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              lstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = lstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              lstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              lstm_optimizer.step()\n",
    "            lstm.eval()\n",
    "        with torch.no_grad():\n",
    "          h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(lstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          test_outputs = torch.sigmoid(lstm(X_test, h_o, c_o)[0])\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          lstm_test_losses.append(test_loss)\n",
    "          lstm_test_accuracies.append(test_acc)\n",
    "          lstm_test_mcc.append(test_mcc)\n",
    "          lstm_test_f1.append(test_f1)\n",
    "          lstm_test_auprc.append(test_auprc)\n",
    "          lstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #lstm_train_losses.append(train_loss.item())\n",
    "          lstm_train_accuracies.append(train_acc)\n",
    "          lstm_train_mcc.append(train_mcc)\n",
    "          lstm_train_f1.append(train_f1)\n",
    "          lstm_train_auprc.append(train_auprc)\n",
    "          lstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(lstm_test_accuracies), np.mean(lstm_test_mcc), np.mean(lstm_test_f1), np.mean(lstm_test_auprc), np.mean(lstm_test_auroc))])[:-3])\n",
    "print('LSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(lstm_test_accuracies), np.std(lstm_test_mcc), np.std(lstm_test_f1), np.std(lstm_test_auprc), np.std(lstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "begUd48HCv5J",
    "outputId": "39401ca5-b9f5-452f-dac3-6a007f1e7723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.825026 & 0.47291 & 0.761544 & 0.768152 & 0.848419\n",
      "BiLSTM Deviations:  0.001753 & 0.005425 & 0.004429 & 0.003937 & 0.001903\n"
     ]
    }
   ],
   "source": [
    "bilstm_train_losses, bilstm_train_accuracies, bilstm_train_mcc, bilstm_train_f1, bilstm_train_auprc, bilstm_train_auroc = [], [], [], [], [], []\n",
    "bilstm_test_losses, bilstm_test_accuracies, bilstm_test_mcc, bilstm_test_f1, bilstm_test_auprc, bilstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        bilstm = BiLSTM(input_size=100).to(device)\n",
    "        bilstm_optimizer = optim.Adam(bilstm.parameters(), lr=BiLSTM_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            bilstm.train()\n",
    "            h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              bilstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = bilstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              bilstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              bilstm_optimizer.step()\n",
    "            bilstm.eval()\n",
    "        with torch.no_grad():\n",
    "          #h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          h_o, c_o = torch.randn(2, 1, 300).to(device), torch.randn(2, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(bilstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          test_outputs = torch.sigmoid(bilstm(X_test, h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          bilstm_test_losses.append(test_loss)\n",
    "          bilstm_test_accuracies.append(test_acc)\n",
    "          bilstm_test_mcc.append(test_mcc)\n",
    "          bilstm_test_f1.append(test_f1)\n",
    "          bilstm_test_auprc.append(test_auprc)\n",
    "          bilstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #bilstm_train_losses.append(train_loss.item())\n",
    "          bilstm_train_accuracies.append(train_acc)\n",
    "          bilstm_train_mcc.append(train_mcc)\n",
    "          bilstm_train_f1.append(train_f1)\n",
    "          bilstm_train_auprc.append(train_auprc)\n",
    "          bilstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(bilstm_test_accuracies), np.mean(bilstm_test_mcc), np.mean(bilstm_test_f1), np.mean(bilstm_test_auprc), np.mean(bilstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(bilstm_test_accuracies), np.std(bilstm_test_mcc), np.std(bilstm_test_f1), np.std(bilstm_test_auprc), np.std(bilstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "leffgdVUCv5K",
    "outputId": "9a1fbb2b-4078-4084-f59c-d8533f0f0f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "0.7974 & 0.337701 & 0.70326 & 0.680188 & 0.793169\n",
      "ConvLSTM Deviations:  0.006323 & 0.032339 & 0.015705 & 0.021213 & 0.015046\n"
     ]
    }
   ],
   "source": [
    "convlstm_train_losses, convlstm_train_accuracies, convlstm_train_mcc, convlstm_train_f1, convlstm_train_auprc, convlstm_train_auroc = [], [], [], [], [], []\n",
    "convlstm_test_losses, convlstm_test_accuracies, convlstm_test_mcc, convlstm_test_f1, convlstm_test_auprc, convlstm_test_auroc = [], [], [], [], [], []\n",
    "\n",
    "for j in range(10):\n",
    "    for train, test in splitter.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        convlstm = ConvLSTM(input_size=100).to(device)\n",
    "        convlstm_optimizer = optim.Adam(convlstm.parameters(), lr=ConvLSTM_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        epochs_needed = []\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        dataset_train = TensorDataset(X_train, y_train)\n",
    "        loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "        for epoch in range(8):\n",
    "            convlstm.train()\n",
    "            h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "            for X_batch, y_batch in loader_train:\n",
    "              convlstm_optimizer.zero_grad()\n",
    "              outputs, h_o, c_o = convlstm(X_batch, h_o, c_o)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "              convlstm_train_losses.append(loss.item())\n",
    "              loss.backward()\n",
    "              convlstm_optimizer.step()\n",
    "            convlstm.eval()\n",
    "        with torch.no_grad():\n",
    "          h_o, c_o = torch.randn(1, 1, 300).to(device), torch.randn(1, 1, 300).to(device)\n",
    "          train_outputs = torch.sigmoid(convlstm(X_train[:30000, :], h_o, c_o)[0])\n",
    "          test_outputs = torch.sigmoid(convlstm(X_test, h_o, c_o)[0])\n",
    "          y_train = y_train[:30000, :]\n",
    "          train_loss = criterion(train_outputs, y_train)\n",
    "          test_loss = criterion(test_outputs, y_test)\n",
    "          train_predicted = (train_outputs > 0.6).float().cpu().numpy()\n",
    "          test_predicted = (test_outputs > 0.6).float().cpu().numpy()\n",
    "    \n",
    "          test_accs = []\n",
    "          test_mccs = []\n",
    "          for i in range(y_test.shape[1]):\n",
    "            test_accs.append(accuracy_score(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "            test_mccs.append(matthews_corrcoef(y_test.cpu().numpy()[:, i], test_predicted[:, i]))\n",
    "          #print(test_mccs)\n",
    "          test_mcc = np.mean(test_mccs)#matthews_corrcoef(y_test.cpu().numpy(), test_predicted)\n",
    "          test_acc = np.mean(test_accs)\n",
    "          test_f1 = f1_score(y_test.cpu().numpy(), test_predicted, average='weighted')\n",
    "          test_auprc = average_precision_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "          test_auroc = roc_auc_score(y_test.cpu().numpy(), test_outputs.cpu().numpy())\n",
    "    \n",
    "          convlstm_test_losses.append(test_loss)\n",
    "          convlstm_test_accuracies.append(test_acc)\n",
    "          convlstm_test_mcc.append(test_mcc)\n",
    "          convlstm_test_f1.append(test_f1)\n",
    "          convlstm_test_auprc.append(test_auprc)\n",
    "          convlstm_test_auroc.append(test_auroc)\n",
    "    \n",
    "    \n",
    "    \n",
    "          train_accs = []\n",
    "          train_mccs = []\n",
    "          for i in range(y_train.shape[1]):\n",
    "            train_accs.append(accuracy_score(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "            train_mccs.append(matthews_corrcoef(y_train.cpu().numpy()[:, i], train_predicted[:, i]))\n",
    "          #print(train_mccs)\n",
    "          train_mcc = np.mean(train_mccs)\n",
    "          train_acc = np.mean(train_accs)\n",
    "          #train_mcc = matthews_corrcoef(y_train.cpu().numpy(), train_predicted)\n",
    "          train_f1 = f1_score(y_train.cpu().numpy(), train_predicted, average='weighted')\n",
    "          train_auprc = average_precision_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "          train_auroc = roc_auc_score(y_train.cpu().numpy(), train_outputs.cpu().numpy())\n",
    "    \n",
    "          #convlstm_train_losses.append(train_loss.item())\n",
    "          convlstm_train_accuracies.append(train_acc)\n",
    "          convlstm_train_mcc.append(train_mcc)\n",
    "          convlstm_train_f1.append(train_f1)\n",
    "          convlstm_train_auprc.append(train_auprc)\n",
    "          convlstm_train_auroc.append(train_auroc)\n",
    "\n",
    "print('Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print(''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convlstm_test_accuracies), np.mean(convlstm_test_mcc), np.mean(convlstm_test_f1), np.mean(convlstm_test_auprc), np.mean(convlstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convlstm_test_accuracies), np.std(convlstm_test_mcc), np.std(convlstm_test_f1), np.std(convlstm_test_auprc), np.std(convlstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     Model    |  Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC\n",
      "MLP Metrics:  0.779924 & 0.258377 & 0.666219 & 0.629591 & 0.752807\n",
      "MLP Deviations:  0.002279 & 0.00568 & 0.008745 & 0.003223 & 0.00209\n",
      "ConvNet Metrics:  0.800874 & 0.378678 & 0.716682 & 0.709095 & 0.807428\n",
      "ConvNet Deviations:  0.004366 & 0.024364 & 0.01115 & 0.016473 & 0.009811\n",
      "LSTM Metrics:  0.809375 & 0.410421 & 0.730971 & 0.726321 & 0.820975\n",
      "LSTM Deviations:  0.001923 & 0.006713 & 0.005815 & 0.004596 & 0.002354\n",
      "BiLSTM Metrics:  0.825026 & 0.47291 & 0.761544 & 0.768152 & 0.848419\n",
      "BiLSTM Deviations:  0.001753 & 0.005425 & 0.004429 & 0.003937 & 0.001903\n",
      "ConvLSTM Metrics:  0.7974 & 0.337701 & 0.70326 & 0.680188 & 0.793169\n",
      "ConvLSTM Deviations:  0.006323 & 0.032339 & 0.015705 & 0.021213 & 0.015046\n"
     ]
    }
   ],
   "source": [
    "print('|     Model    |  Accuracy   |  MCC  |  F1  |  AU-PRC  |  AU-ROC')\n",
    "print('MLP Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(mlp_test_accuracies), np.mean(mlp_test_mcc), np.mean(mlp_test_f1), np.mean(mlp_test_auprc), np.mean(mlp_test_auroc))])[:-3])\n",
    "print('MLP Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(mlp_test_accuracies), np.std(mlp_test_mcc), np.std(mlp_test_f1), np.std(mlp_test_auprc), np.std(mlp_test_auroc))])[:-3])\n",
    "print('ConvNet Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convnet_test_accuracies), np.mean(convnet_test_mcc), np.mean(convnet_test_f1), np.mean(convnet_test_auprc), np.mean(convnet_test_auroc))])[:-3])\n",
    "print('ConvNet Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convnet_test_accuracies), np.std(convnet_test_mcc), np.std(convnet_test_f1), np.std(convnet_test_auprc), np.std(convnet_test_auroc))])[:-3])\n",
    "print('LSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(lstm_test_accuracies), np.mean(lstm_test_mcc), np.mean(lstm_test_f1), np.mean(lstm_test_auprc), np.mean(lstm_test_auroc))])[:-3])\n",
    "print('LSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(lstm_test_accuracies), np.std(lstm_test_mcc), np.std(lstm_test_f1), np.std(lstm_test_auprc), np.std(lstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(bilstm_test_accuracies), np.mean(bilstm_test_mcc), np.mean(bilstm_test_f1), np.mean(bilstm_test_auprc), np.mean(bilstm_test_auroc))])[:-3])\n",
    "print('BiLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(bilstm_test_accuracies), np.std(bilstm_test_mcc), np.std(bilstm_test_f1), np.std(bilstm_test_auprc), np.std(bilstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Metrics: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.mean(convlstm_test_accuracies), np.mean(convlstm_test_mcc), np.mean(convlstm_test_f1), np.mean(convlstm_test_auprc), np.mean(convlstm_test_auroc))])[:-3])\n",
    "print('ConvLSTM Deviations: ', ''.join([str(round(x, 6)) + ' & ' for x in (np.std(convlstm_test_accuracies), np.std(convlstm_test_mcc), np.std(convlstm_test_f1), np.std(convlstm_test_auprc), np.std(convlstm_test_auroc))])[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "e3RU21d6F9Vi",
    "TO6KZIF_tQa8",
    "BfKIDldPA3o1"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
